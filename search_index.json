[
["index.html", "R para Ciencia de Datos Bienvenida 0.1 Sobre la traducción 0.2 Sobre la versión original en inglés", " R para Ciencia de Datos Garrett Grolemund Hadley Wickham Bienvenida Este es el sitio web de la versión en español de “R for Data Science”, de Hadley Wickham y Garrett Grolemund. Este texto te enseñará cómo hacer ciencia de datos con R: aprenderás a importar datos, llevarlos a la estructura más conveniente, transformarlos, visualizarlos y modelarlos. Con él podrás poner en pŕactica las habilidades necesarias para hacer ciencia de datos. Tal como los químicos aprenden a limpiar tubos de ensayo y ordenar un laboratorio, aprenderás a limpiar datos y crear gráficos— junto a muchas otras habilidades que permiten que la ciencia de datos tenga lugar. En este libro encontrarás las mejores prácticas para desarrollar dichas tareas usando R. También aprenderás a usar la gramática de gráficos, programación letrada e investigación reproducible para ahorrar tiempo. Además, aprenderás a manejar recursos cognitivos para facilitar el hacer descubrimientos al momento de manipular, visualizar y explorar datos. 0.1 Sobre la traducción La traducción de “R para Ciencia de Datos” es un proyecto colaborativo de la comunidad de R de Latinoamérica, que tiene por objetivo hacer R más accesible en la región. El proceso se encuentra actualmente en curso, por lo que progresivamente irán apareciendo en este sitio las versiones en español de los capítulos. En la traducción del libro están participando las siguientes personas (en orden alfabético): Marcela Alfaro, Mónica Alonso, Fernando Álvarez, Zulemma Bazurto, Yanina Bellini, Juliana Benítez, María Paula Caldas, Elio Campitelli, Florencia D’Andrea, Rocío Espada, Joshua Kunst, Patricia Loto, Pamela Matías, Lina Moreno, Paola Prieto, Riva Quiroga, Lucía Rodríguez, Mauricio Vargas, Daniela Vázquez, Melina Vidoni, Roxana N. Villafañe. ¡Muchas gracias por su trabajo! Agradecemos también a Laura Ación y Edgar Ruiz, que hicieron la convocatoria inicial a participar. La administración del repositorio con la traducción ha estado cargo de Mauricio Vargas. La coordinación general y la edición, a cargo de Riva Quiroga. Este proyecto no solo implica la traducción del texto, sino también de los sets de datos que se utilizan a lo largo de él. Para ello, se creó el paquete datos, que contiene las versiones traducidas de estos. El paquete ha estado a cargo de Edgar Ruiz, Riva Quiroga y Mauricio Vargas. Para su creación, se utilizó el paquete datalang de Edgar Ruiz. Si quieres conocer más sobre los principios que han orientado nuestro trabajo y saber cómo participar en el proceso de revisión de las traducciones, puedes leer la documentación del proyecto aquí. 0.2 Sobre la versión original en inglés Puedes consultar la versión original del libro en r4ds.had.co.nz/. Existe una edición impresa, que fue publicada por O’Reilly en enero de 2017. Puedes adquirir una copia en Amazon. (El libro “R for Data Science” primero se llamó “Data Science with R” en “Hands-On Programming with R”) Esta obra se distribuye bajo los términos y condiciones de la licencia Creative Commons Atribución-No Comercial-Sin Derivados 3.0 vigente en los Estados Unidos de América. "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Running R code 1.6 Getting help and learning more 1.7 Acknowledgements 1.8 Colophon", " 1 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of R. 1.1 What you will learn Data science is a huge field, and there’s no way you can master it by reading a single book. The goal of this book is to give you a solid foundation in the most important tools. Our model of the tools needed in a typical data science project looks something like this: First you must import your data into R. This typically means that you take data stored in a file, database, or web API, and load it into a data frame in R. If you can’t get your data into R, you can’t do data science on it! Once you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing velocity from speed and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight! Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times. Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease. You’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book we’ll point you to resources where you can learn more. 1.2 How this book is organised The previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them: Starting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth it. Some topics are best explained with other tools. For example, we believe that it’s easier to understand how models work if you already know about visualisation, tidy data, and programming. Programming tools are not necessarily interesting in their own right, but do allow you to tackle considerably more challenging problems. We’ll give you a selection of programming tools in the middle of the book, and then you’ll see how they can combine with the data science tools to tackle interesting modelling problems. Within each chapter, we try and stick to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. While it’s tempting to skip the exercises, there’s no better way to learn than practicing on real problems. 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic. 1.3.1 Big data This book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you’re working with large data, the performance payoff is worth the extra effort required to learn it. If your data is bigger than this, carefully consider if your big data problem might actually be a small data problem in disguise. While the complete data might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration. Another possibility is that your big data problem is actually a large number of small data problems. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. That would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer the question for a single subset using the tools described in this book, you learn new tools like sparklyr, rhipe, and ddr to solve it for the full dataset. 1.3.2 Python, Julia, and friends In this book, you won’t learn anything about Python, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing. We think R is a great place to start your data science journey because it is an environment designed from the ground up to support data science. R is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, R is a much more flexible language than many of its peers. This flexibility comes with its downsides, but the big upside is how easy it is to evolve tailored grammars for specific parts of the data science process. These mini languages help you think about problems as a data scientist, while supporting fluent interaction between your brain and the computer. 1.3.3 Non-rectangular data This book focuses exclusively on rectangular data: collections of values that are each associated with a variable and an observation. There are lots of datasets that do not naturally fit in this paradigm: including images, sounds, trees, and text. But rectangular data frames are extremely common in science and industry, and we believe that they are a great place to start your data science journey. 1.3.4 Hypothesis confirmation It’s possible to divide data analysis into two camps: hypothesis generation and hypothesis confirmation (sometimes called confirmatory analysis). The focus of this book is unabashedly on hypothesis generation, or data exploration. Here you’ll look deeply at the data and, in combination with your subject knowledge, generate many interesting hypotheses to help explain why the data behaves the way it does. You evaluate the hypotheses informally, using your scepticism to challenge the data in multiple ways. The complement of hypothesis generation is hypothesis confirmation. Hypothesis confirmation is hard for two reasons: You need a precise mathematical model in order to generate falsifiable predictions. This often requires considerable statistical sophistication. You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you’re back to doing exploratory analysis. This means to do hypothesis confirmation you need to “preregister” (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. We’ll talk a little about some strategies you can use to make this easier in modelling. It’s common to think about modelling as a tool for hypothesis confirmation, and visualisation as a tool for hypothesis generation. But that’s a false dichotomy: models are often used for exploration, and with a little care you can use visualisation for confirmation. The key difference is how often do you look at each observation: if you look only once, it’s confirmation; if you look more than once, it’s exploration. 1.4 Prerequisites We’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. If you’ve never programmed before, you might find Hands on Programming with R by Garrett to be a useful adjunct to this book. There are four things you need to run the code in this book: R, RStudio, a collection of R packages called the tidyverse, and a handful of other packages. Packages are the fundamental units of reproducible R code. They include reusable functions, the documentation that describes how to use them, and sample data. 1.4.1 R To download R, go to CRAN, the comprehensive R archive network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to reinstall all your packages, but putting it off only makes it worse. 1.4.2 RStudio RStudio is an integrated development environment, or IDE, for R programming. Download and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features. For this book, make sure you have RStudio 1.0.0. When you start RStudio, you’ll see two key regions in the interface: For now, all you need to know is that you type R code in the console pane, and press enter to run it. You’ll learn more as we go along! 1.4.3 The tidyverse You’ll also need to install some R packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this book are part of the so-called tidyverse. The packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally. You can install the complete tidyverse with a single line of code: install.packages(&quot;tidyverse&quot;) On your own computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on to your computer. If you have problems installing, make sure that you are connected to the internet, and that https://cloud.r-project.org/ isn’t blocked by your firewall or proxy. You will not be able to use the functions, objects, and help files in a package until you load it with library(). Once you have installed a package, you can load it with the library() function: library(tidyverse) #&gt; ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.0.9000 ✔ purrr 0.2.5 #&gt; ✔ tibble 2.0.1 ✔ dplyr 0.7.8 #&gt; ✔ tidyr 0.8.2 ✔ stringr 1.3.1 #&gt; ✔ readr 1.3.1 ✔ forcats 0.3.0 #&gt; ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() This tells you that tidyverse is loading the ggplot2, tibble, tidyr, readr, purrr, and dplyr packages. These are considered to be the core of the tidyverse because you’ll use them in almost every analysis. Packages in the tidyverse change fairly frequently. You can see if updates are available, and optionally install them, by running tidyverse_update(). 1.4.4 Other packages There are many other excellent packages that are not part of the tidyverse, because they solve problems in a different domain, or are designed with a different set of underlying principles. This doesn’t make them better or worse, just different. In other words, the complement to the tidyverse is not the messyverse, but many other universes of interrelated packages. As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data. In this book we’ll use three data packages from outside the tidyverse: install.packages(c(&quot;nycflights13&quot;, &quot;gapminder&quot;, &quot;Lahman&quot;)) These packages provide data on airline flights, world development, and baseball that we’ll use to illustrate key data science ideas. 1.5 Running R code The previous section showed you a couple of examples of running R code. Code in the book looks like this: 1 + 2 #&gt; [1] 3 #&gt; [1] 3 If you run the same code in your local console, it will look like this: &gt; 1 + 2 [1] 3 There are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, output is commented out with #&gt;; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the console. Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like flights or x. If we want to make it clear what package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate(), or nycflights13::flights. This is also valid R code. 1.6 Getting help and learning more This book is not an island; there is no single resource that will allow you to master R. As you start to apply the techniques described in this book to your own data you will soon find questions that I do not answer. This section describes a few tips on how to get help, and to help you keep learning. If you get stuck, start with Google. Typically adding “R” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any R-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = &quot;en&quot;) and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer, including [R] to restrict your search to questions and answers that use R. If you don’t find anything useful, prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are three things you need to include to make your example reproducible: required packages, data, and code. Packages should be loaded at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed the package. For packages in the tidyverse, the easiest way to check is to run tidyverse_update(). The easiest way to include data in a question is to use dput() to generate the R code to recreate it. For example, to recreate the mtcars dataset in R, I’d perform the following steps: Run dput(mtcars) in R Copy the output In my reproducible script, type mtcars &lt;- then paste. Try and find the smallest subset of your data that still reveals the problem. Spend a little bit of time ensuring that your code is easy for others to read: Make sure you’ve used spaces and your variable names are concise, yet informative. Use comments to indicate where your problem lies. Do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand, and the easier it is to fix. Finish by checking that you have actually made a reproducible example by starting a fresh R session and copying and pasting your script in. You should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning R each day will pay off handsomely in the long run. One way is to follow what Hadley, Garrett, and everyone else at RStudio are doing on the RStudio blog. This is where we post announcements about new packages, new IDE features, and in-person courses. You might also want to follow Hadley (@hadleywickham) or Garrett (@statgarrett) on Twitter, or follow @rstudiotips to keep up with new features in the IDE. To keep up with the R community more broadly, we recommend reading http://www.r-bloggers.com: it aggregates over 500 blogs about R from around the world. If you’re an active Twitter user, follow the #rstats hashtag. Twitter is one of the key tools that Hadley uses to keep up with new developments in the community. 1.7 Acknowledgements This book isn’t just the product of Hadley and Garrett, but is the result of many conversations (in person and online) that we’ve had with the many people in the R community. There are a few people we’d like to thank in particular, because they have spent many hours answering our dumb questions and helping us to better think about data science: Jenny Bryan and Lionel Henry for many helpful discussions around working with lists and list-columns. The three chapters on workflow were adapted (with permission), from http://stat545.com/block002_hello-r-workspace-wd-project.html by Jenny Bryan. Genevera Allen for discussions about models, modelling, the statistical learning perspective, and the difference between hypothesis generation and hypothesis confirmation. Yihui Xie for his work on the bookdown package, and for tirelessly responding to my feature requests. Bill Behrman for his thoughtful reading of the entire book, and for trying it out with his data science class at Stanford. The #rstats twitter community who reviewed all of the draft chapters and provided tons of useful feedback. Tal Galili for augmenting his dendextend package to support a section on clustering that did not make it into the final draft. This book was written in the open, and many people contributed pull requests to fix minor problems. Special thanks goes to everyone who contributed via GitHub: Thanks go to all contributers in alphabetical order: adi pradhan, Ahmed ElGabbas, Ajay Deonarine, @Alex, Andrew Landgraf, bahadir cankardes, @batpigandme, @behrman, Ben Marwick, Bill Behrman, Brandon Greenwell, Brett Klamer, Christian G. Warden, Christian Mongeau, Colin Gillespie, Cooper Morris, Curtis Alexander, Daniel Gromer, David Clark, Derwin McGeary, Devin Pastoor, Dylan Cashman, Earl Brown, Eric Watt, Etienne B. Racine, Flemming Villalona, Gregory Jefferis, @harrismcgehee, Hengni Cai, Ian Lyttle, Ian Sealy, Jakub Nowosad, Jennifer (Jenny) Bryan, @jennybc, Jeroen Janssens, Jim Hester, @jjchern, Joanne Jang, John Sears, Jon Calder, Jonathan Page, @jonathanflint, Jose Roberto Ayala Solares, Julia Stewart Lowndes, Julian During, Justinas Petuchovas, Kara Woo, @kdpsingh, Kenny Darrell, Kirill Sevastyanenko, @koalabearski, @KyleHumphrey, Lawrence Wu, Matthew Sedaghatfar, Mine Cetinkaya-Rundel, @MJMarshall, Mustafa Ascha, @nate-d-olson, Nelson Areal, Nick Clark, @nickelas, Nirmal Patel, @nwaff, @OaCantona, Patrick Kennedy, @Paul, Peter Hurford, Rademeyer Vermaak, Radu Grosu, @rlzijdeman, Robert Schuessler, @robinlovelace, @robinsones, S’busiso Mkhondwane, @seamus-mckinsey, @seanpwilliams, Shannon Ellis, @shoili, @sibusiso16, @spirgel, Steve Mortimer, @svenski, Terence Teo, Thomas Klebel, TJ Mahr, Tom Prior, Will Beasley, @yahwes, Yihui Xie, @zeal626. 1.8 Colophon An online version of this book is available at http://r4ds.had.co.nz. It will continue to evolve in between reprints of the physical book. The source of the book is available at https://github.com/hadley/r4ds. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: devtools::session_info(c(&quot;tidyverse&quot;)) #&gt; ─ Session info ────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.5.2 (2017-01-27) #&gt; os Ubuntu 14.04.5 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz UTC #&gt; date 2019-01-19 #&gt; #&gt; ─ Packages ────────────────────────────────────────────────────────────── #&gt; package * version date lib #&gt; askpass 1.1 2019-01-13 [1] #&gt; assertthat 0.2.0 2017-04-11 [1] #&gt; backports 1.1.3 2018-12-14 [1] #&gt; base64enc 0.1-3 2015-07-28 [1] #&gt; BH 1.69.0-1 2019-01-07 [1] #&gt; bindr 0.1.1 2018-03-13 [1] #&gt; bindrcpp * 0.2.2 2018-03-29 [1] #&gt; broom 0.5.1 2018-12-05 [1] #&gt; callr 3.1.1 2018-12-21 [1] #&gt; cellranger 1.1.0 2016-07-27 [1] #&gt; cli 1.0.1 2018-09-25 [1] #&gt; clipr 0.5.0 2019-01-11 [1] #&gt; colorspace 1.4-0 2019-01-13 [1] #&gt; crayon 1.3.4 2017-09-16 [1] #&gt; curl 3.3 2019-01-10 [1] #&gt; DBI 1.0.0 2018-05-02 [1] #&gt; dbplyr 1.3.0 2019-01-09 [1] #&gt; digest 0.6.18 2018-10-10 [1] #&gt; dplyr * 0.7.8 2018-11-10 [1] #&gt; evaluate 0.12 2018-10-09 [1] #&gt; fansi 0.4.0 2018-10-05 [1] #&gt; forcats * 0.3.0 2018-02-19 [1] #&gt; fs 1.2.6 2018-08-23 [1] #&gt; generics 0.0.2 2018-11-29 [1] #&gt; ggplot2 * 3.1.0.9000 2019-01-19 [1] #&gt; glue 1.3.0 2018-07-17 [1] #&gt; gtable 0.2.0 2016-02-26 [1] #&gt; haven 2.0.0 2018-11-22 [1] #&gt; highr 0.7 2018-06-09 [1] #&gt; hms 0.4.2 2018-03-10 [1] #&gt; htmltools 0.3.6 2017-04-28 [1] #&gt; httr 1.4.0 2018-12-11 [1] #&gt; jsonlite 1.6 2018-12-07 [1] #&gt; knitr 1.21 2018-12-10 [1] #&gt; labeling 0.3 2014-08-23 [1] #&gt; lattice 0.20-38 2018-11-04 [3] #&gt; lazyeval 0.2.1 2017-10-29 [1] #&gt; lubridate 1.7.4 2018-04-11 [1] #&gt; magrittr 1.5 2014-11-22 [1] #&gt; markdown 0.9 2018-12-07 [1] #&gt; MASS 7.3-51.1 2018-11-01 [3] #&gt; Matrix 1.2-15 2018-11-01 [3] #&gt; mgcv 1.8-26 2018-11-21 [3] #&gt; mime 0.6 2018-10-05 [1] #&gt; modelr 0.1.2 2018-05-11 [1] #&gt; munsell 0.5.0 2018-06-12 [1] #&gt; nlme 3.1-137 2018-04-07 [3] #&gt; openssl 1.2.1 2019-01-17 [1] #&gt; pillar 1.3.1 2018-12-15 [1] #&gt; pkgconfig 2.0.2 2018-08-16 [1] #&gt; plogr 0.2.0 2018-03-25 [1] #&gt; plyr 1.8.4 2016-06-08 [1] #&gt; prettyunits 1.0.2 2015-07-13 [1] #&gt; processx 3.2.1 2018-12-05 [1] #&gt; progress 1.2.0 2018-06-14 [1] #&gt; ps 1.3.0 2018-12-21 [1] #&gt; purrr * 0.2.5 2018-05-29 [1] #&gt; R6 2.3.0 2018-10-04 [1] #&gt; RColorBrewer 1.1-2 2014-12-07 [1] #&gt; Rcpp 1.0.0 2018-11-07 [1] #&gt; readr * 1.3.1 2018-12-21 [1] #&gt; readxl 1.2.0 2018-12-19 [1] #&gt; rematch 1.0.1 2016-04-21 [1] #&gt; reprex 0.2.1 2018-09-16 [1] #&gt; reshape2 1.4.3 2017-12-11 [1] #&gt; rlang 0.3.1 2019-01-08 [1] #&gt; rmarkdown 1.11.3 2019-01-19 [1] #&gt; rstudioapi 0.9.0 2019-01-09 [1] #&gt; rvest 0.3.2 2016-06-17 [1] #&gt; scales 1.0.0 2018-08-09 [1] #&gt; selectr 0.4-1 2018-04-06 [1] #&gt; stringi 1.2.4 2018-07-20 [1] #&gt; stringr * 1.3.1 2018-05-10 [1] #&gt; sys 2.1 2018-11-13 [1] #&gt; tibble * 2.0.1 2019-01-12 [1] #&gt; tidyr * 0.8.2 2018-10-28 [1] #&gt; tidyselect 0.2.5 2018-10-11 [1] #&gt; tidyverse * 1.2.1 2017-11-14 [1] #&gt; tinytex 0.10 2019-01-10 [1] #&gt; utf8 1.1.4 2018-05-24 [1] #&gt; viridisLite 0.3.0 2018-02-01 [1] #&gt; whisker 0.3-2 2013-04-28 [1] #&gt; withr 2.1.2 2018-03-15 [1] #&gt; xfun 0.4 2018-10-23 [1] #&gt; xml2 1.2.0 2018-01-24 [1] #&gt; yaml 2.2.0 2018-07-25 [1] #&gt; source #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; Github (hadley/ggplot2@f0da3d0) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; Github (rstudio/rmarkdown@170d048) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; CRAN (R 3.5.2) #&gt; #&gt; [1] /home/travis/R/Library #&gt; [2] /usr/local/lib/R/site-library #&gt; [3] /home/travis/R-bin/lib/R/library "],
["explora-introduccion.html", "2 Introducción", " 2 Introducción El objetivo de la primera parte del libro es conducirte a las herramientas básicas de exploración de datos de la manera más veloz posible. La exploración de datos es el arte de mirar tus datos, generar hipótesis rápidamente, testearlas con celeridad y luego repetir el proceso iterativamente. El objetivo de la exploración de datos es generar muchos hallazgos prometedores que luego puedes retomar para explorarlos en mayor profundidad. En esta parte del libro aprenderás algunas herramientas que traen un beneficio inmediato: * Visualización es una buena arista para comenzar a programar en R, ya que el retorno es claro: puedes crear gráficos elegantes e informativos que te ayudan a entender los datos. En visualización de datos vas a profundizar en visualización y aprenderás la estructura básica de gráficos en ggplot2 junto con transformar datos en gráficos. Visualización por si sola a menudo no es suficiente, por lo que en [transformación de datos] aprenderás los verbos clave que te permitirán seleccionar variables importantes, filtrar observaciones, crear nuevas variables y sintetizar la información. Finalmente, en [análisis exploratorio de datos], vas a combinar visualización y transformación con tu curiosidad y escepticismo para formular y responder preguntas en torno a los datos. Modelar es un aspecto importante del proceso exploratorio, pero no tienes las habilidades para aprenderlo con efectividad o aplicarlo de momento. Volveremos a dicho tópico en [modelamiento], una vez que ya tengas las herramientas de manipulación de datos y programación. Entre estos tres capítulos que enseñan las herramientas de exploración de datos hay otros tres capítulos que se enfocan en el flujo de trabajo en R. En [flujo de trabajo: básico], [flujo de trabajo: scripts] y [flujo de trabajo: proyectos] aprenderás buenas prácticas para escribir y organizar código R. Tales prácticas te preparan para el éxito en el largo plazo, en cuanto te entregan las herramientas para organizarse y abordar proyectos reales. "],
["visualizacion-de-datos.html", "3 Visualización de datos 3.1 Introducción 3.2 Primeros pasos 3.3 Mapeos estéticos 3.4 Problemas comúnes 3.5 Separar en facetas 3.6 Objetos geométricos 3.7 Transformaciones estadísticas 3.8 Ajustes de posición 3.9 Sistemas de coordenadas 3.10 La gramática de gráficos en capas", " 3 Visualización de datos 3.1 Introducción “Un simple gráfico ha brindado más información a la mente del analista de datos que cualquier otro dispositivo”. — John Tukey En este capítulo aprenderás cómo visualizar datos usando el paquete ggplot2. De los muchos sistemas que posee R para hacer gráficos, ggplot2 es uno de los más elegantes y versátiles. Esto se debe a que implementa un sistema coherente para describir y construir gráficos, conocido como la gramática de gráficos. Con ggplot2 puedes hacer más cosas en menor tiempo, aprendiendo un único sistema y aplicándolo en muchos lugares Si deseas obtener más información sobre los fundamentos teóricos de ggplot2 antes de comenzar, te recomendamos leer “La gramática de gráficos en capas”, http://vita.had.co.nz/papers/layered-grammar.pdf. 3.1.1 Prerrequisitos Este capítulo se centra en ggplot2, uno de los paquetes principales de tidyverse. Para acceder a los conjuntos de datos, las páginas de ayuda y las funciones que utilizaremos en este capítulo, debes cargar tidyverse ejecutando este código: library(tidyverse) #&gt; ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── #&gt; ✔ ggplot2 3.1.0.9000 ✔ purrr 0.2.5 #&gt; ✔ tibble 2.0.1 ✔ dplyr 0.7.8 #&gt; ✔ tidyr 0.8.2 ✔ stringr 1.3.1 #&gt; ✔ readr 1.3.1 ✔ forcats 0.3.0 #&gt; ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── #&gt; ✖ dplyr::filter() masks stats::filter() #&gt; ✖ dplyr::lag() masks stats::lag() Esa única línea de código carga el núcleo de tidyverse; compuesto por los paquetes que usarás en casi todos tus análisis de datos. Al correr esta línea verás cuáles funciones de tidyverse pueden tener conflicto con funciones de R base (o de otros paquetes que puedas haber cargado previamente). Si ejecutas este código y recibes el mensaje de error “no hay ningún paquete llamado ‘tidyverse’”, primero deberás instalarlo y luego ejecutar library () una vez más. install.packages(&quot;tidyverse&quot;) library(tidyverse) Solo necesitas instalar el paquete una única vez, pero debes volver a cargarlo siempre que inicies una nueva sesión. Si necesitamos ser explícitos acerca de dónde viene una función (o un conjunto de datos), usaremos el formato especial package::function(). Por ejemplo, ggplot2::ggplot() dice explícitamente que estamos usando la función ggplot() del paquete ggplot2. 3.2 Primeros pasos Usemos nuestro primer gráfico para responder una pregunta: ¿Los automóviles con motores grandes consumen más combustible que los automóviles con motores pequeños? Probablemente ya tengas una respuesta, pero trata de responder de forma precisa. ¿Cómo es la relación entre el tamaño del motor y la eficiencia del combustible? ¿Es positiva? ¿Es negativa? ¿Es lineal o no lineal? 3.2.1 El conjunto de datos millas Puedes poner a prueba tu respuesta empleando el conjunto de datos millas que se encuentra en el paquete datos (datos::millas). Un conjunto de datos es una colección rectangular de variables (en las columnas) y observaciones (en las filas). millas contiene observaciones para 38 modelos de automóviles recopiladas por la Agencia de Protección Ambiental de los EE. UU. library(datos) millas #&gt; # A tibble: 234 x 11 #&gt; fabricante modelo motor anio cilindros transmision traccion ciudad #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 audi a4 1.8 1999 4 auto(l5) d 18 #&gt; 2 audi a4 1.8 1999 4 manual(m5) d 21 #&gt; 3 audi a4 2 2008 4 manual(m6) d 20 #&gt; 4 audi a4 2 2008 4 auto(av) d 21 #&gt; 5 audi a4 2.8 1999 6 auto(l5) d 16 #&gt; 6 audi a4 2.8 1999 6 manual(m5) d 18 #&gt; # … with 228 more rows, and 3 more variables: autopista &lt;int&gt;, #&gt; # combustible &lt;fct&gt;, clase &lt;fct&gt; Entre las variables en millas encontramos: 1-motor. Tamaño del motor de un automóvil, en litros. 2-autopista. La eficiencia del uso de combustible de un automóvil en la carretera, en millas por galón. Sobre la misma distancia, un automóvil de baja eficiencia consume más combustible que un automóvil de alta eficiencia. Para obtener más información sobre el conjunto de datos millas, puedes abrir la página de ayuda ejecutando ?millas. 3.2.2 Creando un gráfico con ggplot Para graficar millas, corre este código usando motor en el eje x y autopista en el eje y. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) El gráfico muestra una relación negativa entre el tamaño del motor (motor) y la eficiencia del combustible (autopista). En otras palabras, los autos con motores grandes usan más combustible. Este resultado, ¿confirma o refuta tu hipótesis acerca de la relación entre la eficiencia del combustible y el tamaño del motor? Para graficar con ggplot2, comienzas un gráfico con la función ggplot(). ggplot() crea un sistema de coordenadas al cual puedes agregar capas. El primer argumento de ggplot() es el conjunto de datos para usar en el gráfico. Si corres ggplot(data = millas), obtendrás un gráfico vacío. Como no es muy interesante, no vamos a mostrarlo aquí. Para completar tu gráfico debes agregar una o más capas a ggplot(). La función geom_point() agrega una capa de puntos al gráfico, que crea un diagrama de dispersión (scatterplot). ggplot2 incluye muchas funciones llamadas geom, cada una de las cuales agrega un tipo de capa diferente a un gráfico. Aprenderás sobre muchas de ellas a lo largo de este capítulo. Cada función geom en ggplot2 tiene un argumento de mapping. Este define cómo se asignan o se “mapean” las variables del conjunto de datos a propiedades visuales. El argumento de mapping siempre aparece emparejado con aes(), y los argumentos x e y dentro de aes() especifican qué variables asignar a los ejes x e y. ggplot2 busca la variable asignada en el argumento data, en este caso, millas. 3.2.3 Una plantilla de gráficos Convirtamos ahora este código en una plantilla reutilizable para hacer gráficos con ggplot2. Para hacer un gráfico, reemplaza las secciones entre corchetes en el siguiente código con un conjunto de datos, una función geom o una colección de mapeos. ggplot(data = &lt;DATOS&gt;) + &lt;GEOM_FUNCION&gt;(mapping = aes(&lt;MAPEOS&gt;)) El resto de este capítulo te mostrará cómo utilizar y adaptar esta planilla para crear diferentes tipos de gráficos. Comenzaremos por el componente &lt;MAPEOS&gt; 3.2.4 Ejercicios Corre ggplot(data = millas). ¿Qué observas? ¿Cuántas filas hay en millas? ¿Cuántas columnas? ¿Qué describe la variable traccion? Lee la ayuda de ?millas para encontrar la respuesta. Realiza un gráfico de dispersión de las variables autopista y cilindros. ¿Qué sucede cuando haces un gráfico de dispersión de clase versus traccion? ¿Por qué no es útil este gráfico? 3.3 Mapeos estéticos “El mayor valor de una imagen es cuando nos obliga a observar lo que no esperabamos ver”. — John Tukey En el siguiente gráfico, un grupo de puntos resaltados en rojo parece quedar fuera de la tendencia lineal. Estos autos tienen un kilometraje mayor de lo que esperaríamos. ¿Cómo puedes explicar estos autos? Supongamos que estos automóviles son híbridos. Una forma de probar esta hipótesis es observando la variable que indica la clase de cada automóvil. La variable clase del conjunto de datos de millas clasifica los autos en grupos como compacto, mediano y SUV. Si los puntos periféricos corresponden a automóviles híbridos, deberían estar clasificados como compactos o, tal vez, subcompactos (ten en cuenta que estos datos se recopilaron antes de que los camiones híbridos y SUV se hicieran populares). Puedes agregar una tercera variable, como clase, a un diagrama de dispersión bidimensional asignándolo a una estética. Una estética es una propiedad visual de los objetos de un gráfico. La estética incluye cosas como el tamaño, la forma o el color de tus puntos. Puedes mostrar un punto (como el siguiente) de diferentes maneras cambiando los valores de sus propiedades estéticas. Como ya usamos la palabra “valor” para describir los datos, usemos la palabra “nivel” para describir las propiedades estéticas. Aquí cambiamos los niveles del tamaño, la forma y el color de un punto para que el punto sea pequeño, triangular o azul: El mapeo de las propiedades estéticas en tus gráficos a las variables en tu conjunto de datos te permite comunicar información de los mismos. Por ejemplo, puedes asignar los colores de tus puntos a la variable clase para revelar la clase de cada automóvil. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, color = clase)) (Si prefieres el inglés británico, como Hadley, puedes usar colour en lugar de color). (Si prefieres el inglés británico, como Hadley, puedes usar colour en lugar de color). Para asignar una estética a una variable, debes asociar el nombre de la estética al de la variable dentro de aes(). ggplot2 asignará automáticamente un nivel único de la estética (en este ejemplo, un color ) a cada valor único de la variable. Este proceso es conocido como escalamiento (scaling). ggplot2 acompañará el gráfico con una leyenda que explica qué niveles corresponden a qué valores. Los colores revelan que muchos de los puntos inusuales son los coches de dos asientos. ¡Estos automóviles no parecen híbridos, y son, de hecho, autos deportivos! Los autos deportivos tienen motores grandes, como camionetas todo terreno o pickups, a diferencia de los vehículos pequeños como los autos medianos y compactos, lo que mejora su consumo de gasolina. En retrospectiva, es poco probable que estos autos sean híbridos ya que tienen motores grandes. En el ejemplo anterior, asignamos la variable clase a la estética del color , pero podríamos haber asignado a la estética del tamaño de la misma manera. En este caso, el tamaño exacto de cada punto revelaría clase. Recibimos aquí una advertencia (warning), porque mapear una variable desordenada (clase) a una estética ordenada (size) no es una buena idea. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, size = clase)) #&gt; Warning: Using size for a discrete variable is not advised. También podríamos haber asignado la clase a la estética alfa, que controla la transparencia de los puntos o a la estética shape que controla la forma (shape) de los puntos. # Left ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, alpha = clase)) # Right ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, shape = clase)) ¿Qué pasó con los SUV? ggplot2 solo puede usar seis formas a la vez. De forma predeterminada, los grupos adicionales no se grafican cuando se emplea la estética de la forma. Para cada estética, se usa aes() para asociar el nombre de la estética con la variable seleccionada para graficar. La función aes() reúne cada una de las asignaciones estéticas utilizadas por una capa y las pasa al argumento de mapeo de la capa. La sintaxis resalta una visión útil sobre x e y: las ubicaciones de x e y de un punto son en sí mismas también estéticas, es decir propiedades visuales que se puede asignar a las variables para mostrar información sobre los datos. Una vez que asignas una estética, ggplot2 se ocupa del resto. El paquete selecciona una escala razonable para usar con la estética elegida y construye una leyenda que explica la relación entre niveles y valores. Para la estética x e y, ggplot2 no crea una leyenda, pero crea una línea que delimita el eje con sus marcas de graduación y una etiqueta. La línea del eje actúa como una leyenda; explica el mapeo entre ubicaciones y valores. También puedes fijar las propiedades estéticas de tu geom manualmente. Por ejemplo, podemos hacer que todos los puntos del gráfico sean azules: ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista), color = &quot;blue&quot;) Aquí, el color no transmite información sobre una variable, sino que cambia la apariencia del gráfico. Para establecer una estética de forma manual, debes usar el nombre de la estética como un argumento de la función geom; es decir, va fuera de aes(). Tendrás que elegir un nivel que tenga sentido para esa estética: El nombre de un color como una cadena de caracteres. El tamaño de un punto en mm. La forma de un punto como un número, como se muestra en la Figura 3.1.Hay algunas que parecen duplicados: por ejemplo 0, 15 y 22 son todos cuadrados. La diferencia viene de la interacción entre las estéticas color y fill (relleno). Las formas vacías (0–14) tienen un borde determinado por color; las formas sólidas (15–18) están rellenas con color; las formas rellenas (21–24) tienen un borde de color y están rellenas por fill Figure 3.1: R tiene 25 formas de default que están identificadas por números. . 3.3.1 Ejercicios ¿Qué no va bien en este código? ¿Por qué hay puntos que no son azules? ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista, color = &quot;blue&quot;)) ¿Qué variables en millas son categóricas? ¿Qué variables son continuas? (Sugerencia: escribe ? millas para leer la documentación de ayuda para este conjunto de datos). ¿Cómo puedes ver esta información cuando ejecutas millas? Asigna una variable continua a color, size, y shape. ¿Cómo se comportan estas estéticas de manera diferente para variables categóricas y variables continuas? ¿Qué ocurre si asignas o mapeas la misma variable a múltiples estéticas? ¿Qué hace la estética stroke? ¿Con qué formas trabaja? (Sugerencia: consultar ?geom_point) ¿Qué ocurre si se asigna o mapea una estética a algo diferente del nombre de una variable, como ser aes (color = motor &lt;5)? 3.4 Problemas comúnes Es probable que encuentres problemas con los primeros códigos que ejecutes en R, e. No te preocupes, es lo más común . He estado escribiendo código en R durante años, ¡y todos los días sigo escribiendo código que no funciona! Comienza comparando cuidadosamente el código que estás ejecutando con el código en este libro. R es extremadamente exigente, y un carácter fuera de lugar puede marcar la diferencia. Asegúrate de que cada ( coincida con un ) y cada &quot; esté emparejado con otro&quot;. Algunas veces ejecutarás el código y no pasará nada. Comprueba la parte izquierda de tu consola: si es un +, significa que R no cree que hayas escrito una expresión completa y está esperando que la termines. En este caso, normalmente es fácil comenzar nuevamente desde cero presionando ESCAPE para cancelar el procesamiento del comando actual. Un problema común al crear gráficos con ggplot2 es colocar el + en el lugar equivocado: debe encontrarse al final de la línea, no al inicio. En otras palabras, asegúrate de no haber escrito accidentalmente un código como este: ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) Si esto no resuelve el problema, prueba la ayuda. Puedes obtener ayuda sobre cualquier función R ejecutando ?nombre_de_la_funcion en la consola, o seleccionando el nombre de la función y presionando F1 en RStudio. No te preocupes si la ayuda no te parece tan útil, trata entonces de saltar a los ejemplos y buscar un pedazo de código que coincida con lo que intentas hacer. Si eso no ayuda, lee cuidadosamente el mensaje de error. ¡A veces la respuesta estará oculta allí! Cuando eres nuevo en R, la respuesta puede estar en el mensaje de error, pero aún no sabes cómo entenderlo. Otra gran herramienta es Google: intenta buscar allí el mensaje de error, ya que es probable que otra persona haya tenido el mismo problema y haya obtenido ayuda en línea. 3.5 Separar en facetas Una forma de agregar variables adicionales es con las estéticas. Otra forma particularmente útil para las variables categóricas consiste en dividir el gráfico en facetas, sub-gráficos que muestran cada uno un subconjunto de los datos. Para separar en facetas un gráfico según una sola variable, usa facet_wrap() - del inglés envolver una faceta. El primer argumento de facet_wrap() debería ser una fórmula creada con ~ seguido por el nombre de una de las variable (aquí “fórmula” es el nombre de un tipo de estructura en R, no un sinónimo de “ecuación”). La variable que uses en facet_wrap() debe ser discreta. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_wrap(~ clase, nrow = 2) Para separar en facetas un gráfico según las combinaciones de dos variables, agregua facet_grid() a tu código del gráfico. El primer argumento de facet_grid() también corresponde a una fórmula. Esta vez, la fórmula debe contener dos nombres de variables separados por un ~. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(traccion ~ cilindros) Si prefieres no separar en facetas las filas o columnas, remplaza por un . el nombre de alguna de las variables, por ejemplo + facet_grid (. ~ cyl). 3.5.1 Ejercicios Qué ocurre si intentas separar en facetas a una variable continua? ¿Qué significan las celdas vacías que aparecen en el gráfico generado usando facet_grid (traccion ~ cilindros)? ¿Cómo se relacionan con este gráfico? ggplot(data = millas) + geom_point(mapping = aes(x = traccion, y = cilindros)) ¿Qué gráfica el siguiente código? ¿Qué hace . ? ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(traccion ~ .) ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + facet_grid(. ~ cilindros) Mira de nuevo el primer gráfico en facetas presentado en esta sección: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~ class, nrow = 2) ¿Cuáles son las ventajas de separar en facetas en lugar de aplicar una estética de color? ¿Cuáles son las desventajas? ¿Cómo cambiaría este balance si tuvieras un conjunto de datos más grande? Lee ?facet_wrap. ¿Qué hace nrow? ¿Qué hace ncol? ¿Qué otras opciones controlan el diseño de los paneles individuales? ¿Por qué facet_grid() no tiene argumentos nrow y ncol? Cuando usas facet_grid(), generalmente deberías poner la variable con un mayor número de niveles únicos en las columnas. ¿Por qué? 3.6 Objetos geométricos ¿Cómo son estos dos gráficos similares? Ambos gráficos contienen las mismas variables x e y, y describen los mismos datos. Pero los gráficos no son idénticos. Cada gráfico usa un objeto visual diferente para representar los datos. En la sintaxis ggplot2, decimos que usan diferentes geoms. Un geom es el objeto geométrico usado para representar datos de forma gráfica. La gente a menudo llama los gráficos por el tipo de geom que utiliza. Por ejemplo, los diagramas de barras usan geoms de barra (bar), los diagramas de líneas usan geoms de línea (line), los diagramas de caja usan geoms de diagrama de caja (boxplot), y así sucesivamente. En inglés, los diagramas de puntos (llamados scatterplots) rompen la tendencia; ellos usan geom de punto (o point). Como vemos arriba, puedes usar diferentes geoms para graficar los mismos datos. La gráfica de la izquierda usa el geom de punto (geom_point()), y la gráfica de la derecha usa el geom liso (geom_smooth()), una línea suave ajustada a los datos. Para cambiar el geom de tu gráfico, modifica la función geom que acompaña a ggplot (). Por ejemplo, para hacer los gráficos que se muestran arriba, puedes usar este código: # left ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) # right ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) Cada función geom en ggplot2 toma un argumento de mapping. Sin embargo, no todas las estéticas funcionan con todos los geom. Podrías establecer la forma para un punto, pero no podrías establecer la “forma” de una línea. Por otro lado, para una línea es posible elegir el tipo de línea (linetype). geom_smooth() dibujará una línea diferente, con un tipo de línea diferente, para cada valor único de la variable que asignes al tipo de línea. ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, linetype=traccion)) Aquí geom_smooth() separa los automóviles en tres líneas en función de su valor de traccion, que describe el tipo de transmisión de un automóvil. Una línea describe todos los puntos con un valor de 4, otra línea los de valor d, y una tercera línea describe los puntos con un valor t. Aquí, 4 significa tracción en las cuatro ruedas, d tracción delantera y t tracción trasera. Si esto suena extraño, podemos hacerlo más claro al superponer las líneas sobre los datos brutos y luego colorear todo según traccion. ¡Observa que generamos un gráfico que contiene dos geoms! Si esto te emociona, abróchate el cinturón. En la siguiente sección aprenderemos cómo colocar múltiples geoms en el mismo gráfico. ggplot2 proporciona más de 30 geoms, y los paquetes de extensión proporcionan aún más (consulta https://www.ggplot2-exts.org para obtener una muestra). La mejor forma de obtener un panorama completo sobre las posibilidades que brinda ggplot2 es consultando la hoja de referencia (cheatsheet), que puedes encontrar en http://rstudio.com/cheatsheets. Para obtener más información sobre un tipo dado de geoms, usa la ayuda: ?geom_smooth. Muchos geoms, tal como geom_smooth(), usan un único objeto geométrico para mostrar múltiples filas de datos. Para estos geoms, puedes asignar la estética de group a una variable categórica para graficar múltiples objetos. ggplot2 representará un objeto distinto por cada valor único de la variable de agrupamiento. En la práctica, ggplot2 agrupará automáticamente los datos para estos geoms siempre que se asigne una estética a una variable discreta (como en el ejemplo del tipo de línea o linetype). Es conveniente confiar en esta característica porque la estética del grupo en sí misma no agrega una leyenda o características distintivas a los geoms. ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista)) ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, group = traccion)) ggplot(data = millas) + geom_smooth(mapping = aes(x = motor, y = autopista, color = traccion), show.legend = FALSE) Para mostrar múltiples geoms en el mismo gráfico, agrega varias funciones geom a ggplot(): ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista)) + geom_smooth(mapping = aes(x = motor, y = autopista)) Esto introduce sin embargo cierta duplicación en nuestro código. Imagina que deseas cambiar el eje y para mostrar ciudad en lugar de autopista. Necesitarías cambiar la variable en dos lugares, y podrías olvidarte de actualizar uno. Puedes evitar este tipo de repetición pasando un conjunto de mapeos a ggplot(). ggplot2 tratará estos mapeos como mapeos globales que se aplican a cada geom en el gráfico. En otras palabras, este código producirá la misma gráfica que el código anterior: ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point() + geom_smooth() Si colocas mapeos en una función geom, ggplot2 los tratará como mapeos locales para la capa. Estas asignaciones serán usadas para extender o sobrescribir los mapeos globales de solo esa capa. Esto permite mostrar diferentes estéticas en diferentes capas. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth() La misma idea se puede emplear para especificar distintos conjuntos de datos (data) para cada capa. Aquí, nuestra línea suave muestra solo un subconjunto del conjunto de datos de millas, los autos subcompactos. El argumento de datos locales en geom_smooth() anula el argumento de datos globales en ggplot() solo para esa capa. ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point(mapping = aes(color = clase)) + geom_smooth(data = filter(millas, clase == &quot;subcompacto&quot;), se = FALSE) (Aprenderás cómo funciona filter() en el próximo capítulo: por ahora, solo recuerda que este comando selecciona los automóviles subcompactos). 3.6.1 Ejercicios ¿Qué geom usarías para generar un gráfico de líneas? ¿Un diagrama de caja? ¿Un histograma? ¿Un gráfico de área ? Ejecuta este código en tu mente y predice cómo se verá el output. Luego, ejecuta el código en R y verifica tus predicciones. ggplot(data = millas, mapping = aes(x = motor, y = autopista, color = traccion)) + geom_point() + geom_smooth(se = FALSE) ¿Qué muestra show.legend = FALSE? ¿Qué pasa si lo quitas? ¿Por qué crees que lo usé antes en el capítulo? ¿Qué hace el argumento se en geom_smooth()? Will these two graphs look different? Why/why not? ggplot(data = millas, mapping = aes(x = motor, y = autopista)) + geom_point() + geom_smooth() ggplot() + geom_point(data = millas, mapping = aes(x = motor, y = autopista)) + geom_smooth(data = millas, mapping = aes(x = motor, y = autopista)) Recrea el código R necesario para generar los siguientes gráficos: 3.7 Transformaciones estadísticas A continuación, echemos un vistazo a un gráfico de barras. Los gráficos de barras parecen simples, pero son interesantes porque revelan algo sutil sobre los gráficos. Considera un gráfico de barras básico, como se realizó con geom_bar(). El siguiente cuadro muestra la cantidad total de diamantes en el conjunto de datos de diamantes, agrupados por la variable corte. El conjunto de datos de diamantes se encuentra en el paquete datos y contiene información sobre ~ 54000 diamantes, incluido el precio, el quilate, el color, la claridad y el corte de cada diamante. El gráfico muestra que hay más diamantes disponibles con cortes de alta calidad que con cortes de baja calidad. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte)) En el eje x, el gráfico muestra corte, una variable de diamantes. En el eje y muestra recuento, ¡pero el recuento no es una variable en diamantes! ¿De dónde viene el recuento? Muchos gráficos, como los diagramas de dispersión, grafican los valores brutos de su conjunto de datos. Otros gráficos, como los gráficos de barras, calculan nuevos valores para presentar: los gráficos de barras, los histogramas y los polígonos de frecuencia almacenan los datos y luego grafican los conteos de contenedores, sea el número de puntos que caen en cada contenedor. los suavizadores ajustan un modelo a los datos y luego grafican las predicciones del modelo. los diagramas de caja calculan un sólido resumen de la distribución y luego muestran un cuadro con formato especial. El algoritmo utilizado para calcular nuevos valores para un gráfico se llama stat, abreviatura en inglés de transformación estadística. La siguiente figura describe cómo funciona este proceso con geom_bar (). Puedes aprender qué stat usa cada geom inspeccionando el valor predeterminado para el argumento stat. Por ejemplo, ?geom_bar muestra que el valor predeterminado para stat es “count”, lo que significa que geom_bar() usa stat_count(). stat_count() está documentado en la misma página que geom_bar(), y si te desplazas hacia abajo puedes encontrar una sección llamada “Variables calculadas” (Computed variables). Eso describe cómo calcula dos nuevas variables: count y prop. Por lo general puedes usar geoms y estadísticas de forma intercambiable. Por ejemplo, puedes volver a crear la gráfica anterior usando stat_count() en lugar de geom_bar(): ggplot(data = diamantes) + stat_count(mapping = aes(x = corte)) Esto funciona porque cada geom tiene una estadística predeterminada; y cada estadística tiene un geom predeterminado. Esto significa que generalmente puedes usar geoms sin preocuparte por la transformación estadística subyacente. Hay tres razones por las que podrías necesitar usar una estadística explícitamente: Es posible que desee anular la estadística predeterminada. En el siguiente código, cambio la estadística de geom_bar() de recuento (el valor predeterminado) a identidad. Esto me permite asignar la altura de las barras a los valores brutos de una variable \\(y\\) . Desafortunadamente, cuando la gente habla de gráficos de barras casualmente, podría estar refiriéndose a este tipo de gráfico de barras, donde la altura de la barra ya está presente en los datos, o al gráfico de barras anterior, donde la altura de la barra se determina contando filas. demo &lt;- tribble( ~corte, ~freq, &quot;Regular&quot;, 1610, &quot;Bueno&quot;, 4906, &quot;Muy Bueno&quot;, 12082, &quot;Premium&quot;, 13791, &quot;Ideal&quot;, 21551 ) ggplot(data = demo) + geom_bar(mapping = aes(x = corte, y = freq), stat = &quot;identity&quot;) (No te preocupes si nunca has visto &lt;- o tribble(). Puede que seas capaz de adivinar su significado por el contexto. ¡Aprenderás lo que hacen exactamente pronto!) Es posible que desees anular el mapeo predeterminado de las variables transformadas a la estética. Por ejemplo, es posible que desees mostrar un gráfico de barras de proporciones, en lugar de un recuento: ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, y = ..prop.., group = 1)) Para encontrar las variables calculadas por la estadística, busca la sección de ayuda titulada “Variables calculadas”. Es posible que desees resaltar la transformación estadística en tu código. Por ejemplo, puedes usar stat_summary(), que resume los valores de y para cada valor único de x, para resaltar el resumen que se está computando: ggplot(data = diamantes) + stat_summary( mapping = aes(x = corte, y = profundidad), fun.ymin = min, fun.ymax = max, fun.y = median) ggplot2 proporciona más de 20 estadísticas para que uses. Cada estadística es una función, por lo que puedes obtener ayuda de la manera habitual, por ejemplo: ?stat_bin. Para ver una lista completa de estadísticas disponibles para ggplot2, consulta la hoja de referencia. 3.7.1 Ejercicios ¿Cuál es el geom predeterminado asociado con stat_summary()? ¿Cómo podrías reescribir el gráfico anterior para usar esa función geom en lugar de la función stat? ¿Qué hace geom_col()? ¿Cómo es diferente a geom_bar()? La mayoría de los geoms y las estadísticas vienen en pares que casi siempre se usan en conjunto. Lee la documentación y has una lista de todos los pares. ¿Qué tienen en común? ¿Qué variables calcula stat_smooth()? ¿Qué parámetros controlan su comportamiento? En nuestro gráfico de barras de proporción , necesitamos establecer group = 1. ¿Por qué? En otras palabras, ¿cuál es el problema con estos dos gráficos? ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, y = ..prop..)) ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = color, y = ..prop..)) 3.8 Ajustes de posición Hay una pieza más de magia asociada con los gráficos de barras. Puede colorear un gráfico de barras usando la estética de color o, tal vez con el más útil fill: ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, colour = corte)) ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = corte)) Mira lo que sucede si asigna la estética de relleno a otra variable, como claridad: las barras se apilan automáticamente. Cada rectángulo de color representa una combinación de corte y claridad. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad)) El apilamiento se realiza automáticamente mediante el ajuste de posición especificado por el argumento position. Si no deseas un gráfico de barras apiladas , puedes usar una de las otras tres opciones: &quot;identity&quot;, &quot;dodge&quot; o &quot;fill&quot;, del inglés identidad, esquivar y llenar respectivamente. position = &quot;identity&quot; colocará cada objeto exactamente donde cae en el contexto del gráfico. Esto no es muy útil al momento de graficar barras, porque las superpone. Para ver esa superposición, debemos hacer que las barras sean ligeramente transparentes al configurar alfa a un valor pequeño, o completamente transparente al establecer fill = NA. ggplot(data = diamantes, mapping = aes(x = corte, fill = claridad)) + geom_bar(alpha = 1/5, position = &quot;identity&quot;) ggplot(data = diamantes, mapping = aes(x = corte, colour = claridad)) + geom_bar(fill = NA, position = &quot;identity&quot;) El ajuste de posición = identity es más útil para geoms 2-D, como puntos, donde es la opción predeterminada. position = &quot;fill&quot; funciona como el apilamiento, pero hace que cada conjunto de barras apiladas tenga la misma altura. Esto hace que sea más fácil comparar proporciones entre grupos. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad), position = &quot;dodge&quot;) position = &quot;dodge&quot; coloca objetos superpuestos directamente uno al lado del otro. Esto hace que sea más fácil comparar valores individuales. ggplot(data = diamantes) + geom_bar(mapping = aes(x = corte, fill = claridad), position = &quot;dodge&quot;) Hay otro tipo de ajuste que no es útil para gráficos de barras, pero puede ser muy útil para diagramas de dispersión. Recuerda nuestro primer diagrama de dispersión. ¿Notaste que la trama muestra solo 126 puntos, a pesar de que hay 234 observaciones en el conjunto de datos? Los valores de las variables autopista y motor se redondean de modo que los puntos aparecen en una cuadrícula y muchos se superponen entre sí. Este problema se conoce como sobregraficado (overplotting). Esta disposición hace que sea difícil ver dónde está la masa de datos. ¿Los puntos de datos se distribuyen equitativamente a lo largo de la gráfica, o hay una combinación especial de autopista y motor que contiene 109 valores? Puedes evitar esta grilla estableciendo el ajuste de posición en “jitter”. position = &quot;jitter&quot; agrega una pequeña cantidad de ruido aleatorio a cada punto. Esto dispersa los puntos ya que no es probable que dos puntos reciban la misma cantidad de ruido aleatorio. ggplot(data = millas) + geom_point(mapping = aes(x = motor, y = autopista), position = &quot;jitter&quot;) Si bien agregar aleatoriedad a los puntos puede parecer una forma extraña de mejorar tu gráfico ya que hace que sea menos preciso a escalas pequeñas, lo hace ser más revelador a gran escala. Como esta es una operación tan útil, ggplot2 viene con una abreviatura de geom_point (position = “jitter”): geom_jitter (). Para obtener más información sobre ajustes de posición, busca la página de ayuda asociada con cada ajuste: ?position_dodge, ?position_fill, ?position_identity, ?position_jitter y ?position_stack. 3.8.1 Ejercicios ¿Cuál es el problema con este gráfico? ¿Cómo podrías mejorarlo? ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() ¿Qué parámetros de geom_jitter() controlan la cantidad de ruido? Compara y contrasta geom_jitter() con geom_count () ¿Cuál es el ajuste de posición predeterminado de geom_boxplot()? Crea una visualización del conjunto de datos de millas que lo demuestre. 3.9 Sistemas de coordenadas Los sistemas de coordenadas son probablemente la parte más complicada de ggplot2. El sistema predeterminado es el sistema de coordenadas cartesianas, donde las posiciones x e y actúan independientemente para determinar la ubicación de cada punto. Hay varios otros sistemas de coordenadas que ocasionalmente son útiles. coord_flip() cambia los ejes x e y. Esto es útil (por ejemplo), si quieres diagramas de caja horizontales. También es útil para etiquetas largas: es difícil ajustarlas sin superposición en el eje x. ggplot(data = millas, mapping = aes(x = clase, y = autopista)) + geom_boxplot() ggplot(data = millas, mapping = aes(x = clase, y = autopista)) + geom_boxplot() + coord_flip() coord_quickmap() establece la relación de aspecto correctamente para los mapas. Esto es muy importante si graficas datos espaciales con ggplot2 (tema que desafortunadamente no contamos con espacio para desarrollar en este libro). nz &lt;- map_data(&quot;nz&quot;) ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) + coord_quickmap() coord_polar() usa coordenadas polares. Las coordenadas polares revelan una conexión interesante entre un gráfico de barras y un gráfico de Coxcomb. bar &lt;- ggplot(data = diamantes) + geom_bar( mapping = aes(x = corte, fill = corte), show.legend = FALSE, width = 1) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar + coord_flip() bar + coord_polar() 3.9.1 Ejercicios Convierte un gráfico de barras apiladas en un gráfico circular usando coord_polar (). ¿Qué hace labs()? Lee la documentación. ¿Cuál es la diferencia entre coord_quickmap() y coord_map()? ¿Qué te dice la gráfica siguiente sobre la relación entre la ciudad y la autopista? ¿Por qué es coord_fixed() importante? ¿Qué hace geom_abline()? ggplot(data = millas, mapping = aes(x = ciudad, y = autopista)) + geom_point() + geom_abline() + coord_fixed() 3.10 La gramática de gráficos en capas En las secciones anteriores, aprendiste mucho más que cómo hacer diagramas de dispersión, gráficos de barras y diagramas de caja. Aprendiste una base que se puede usar para hacer cualquier tipo de gráfico con ggplot2. Para ver esto, agreguemos ajustes de posición, estadísticas, sistemas de coordenadas y facetas a nuestra plantilla de código: ggplot(data = &lt;DATOS&gt;) + &lt;GEOM_FUNCION&gt;( mapping = aes(&lt;MAPEOS&gt;), stat = &lt;ESTADISTICA&gt;, position = &lt;POSICION&gt; ) + &lt;FUNCION_COORDENADAS&gt; + &lt;FUNCION_FACETAS&gt; Nuestra nueva plantilla tiene siete parámetros que se corresponde con las palabras entre corchetes que aparecen en la plantilla. En la práctica, rara vez necesitas proporcionar los siete parámetros para hacer un gráfico porque ggplot2 proporcionará valores predeterminados útiles para todos excepto para los datos, las asignaciones y la función geom. Los siete parámetros en la plantilla componen la gramática de los gráficos, un sistema formal de construcción de gráficos. La gramática de los gráficos se basa en la idea de que puedes describir de manera única cualquier gráfico como una combinación de un conjunto de datos, un geom, un conjunto de asignaciones, una estadística, un ajuste de posición, un sistema de coordenadas y un esquema de facetado. To see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat). Para ver cómo funciona esto, considera cómo podrías construir un gráfico básico desde cero: podrías comenzar con un conjunto de datos y luego transformarlo en la información que deseas mostrar (con una estadística). A continuación, podrías elegir un objeto geométrico para representar cada observación en los datos transformados. Luego podrías usar las propiedades estéticas de los geoms para representar variables de los datos. Asignarías los valores de cada variable a los niveles de una estética. Posteriormente, seleccionarías un sistema de coordenadas para colocar los geoms. Podrías utilizar la ubicación de los objetos (que es en sí misma una propiedad estética) para mostrar los valores de las variables x e y. Ya en este punto podrías tener un gráfico completo, pero también podrías ajustar aún más las posiciones de los geoms dentro del sistema de coordenadas (un ajuste de posición) o dividir el gráfico en subtramas (facetas). También podrías extender el gráfico agregando una o más capas adicionales, donde cada capa adicional usaría un conjunto de datos, un geom, un conjunto de asignaciones, una estadística y un ajuste de posición. Puedes usar este método para construir cualquier gráfico que imagines. En otras palabras, puedes usar la plantilla de código aprendiste en este capítulo para construir cientos de miles de gráficos únicos. "],
["workflow-basics.html", "4 Workflow: basics 4.1 Coding basics 4.2 What’s in a name? 4.3 Calling functions 4.4 Practice", " 4 Workflow: basics You now have some experience running R code. I didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in R, because it is such a stickler for punctuation, and even one character out of place will cause it to complain. But while you should expect to be a little frustrated, take comfort in that it’s both typical and temporary: it happens to everyone, and the only way to get over it is to keep trying. Before we go any further, let’s make sure you’ve got a solid foundation in running R code, and that you know about some of the most helpful RStudio features. 4.1 Coding basics Let’s review some basics we’ve so far omitted in the interests of getting you plotting as quickly as possible. You can use R as a calculator: 1 / 200 * 30 #&gt; [1] 0.15 (59 + 73 + 2) / 3 #&gt; [1] 44.7 sin(pi / 2) #&gt; [1] 1 You can create new objects with &lt;-: x &lt;- 3 * 4 All R statements where you create objects, assignment statements, have the same form: object_name &lt;- value When reading that code say “object name gets value” in your head. You will make lots of assignments and &lt;- is a pain to type. Don’t be lazy and use =: it will work, but it will cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which is a good code formatting practice. Code is miserable to read on a good day, so giveyoureyesabreak and use spaces. 4.2 What’s in a name? Object names must start with a letter, and can only contain letters, numbers, _ and .. You want your object names to be descriptive, so you’ll need a convention for multiple words. I recommend snake_case where you separate lowercase words with _. i_use_snake_case otherPeopleUseCamelCase some.people.use.periods And_aFew.People_RENOUNCEconvention We’ll come back to code style later, in functions. You can inspect an object by typing its name: x #&gt; [1] 12 Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this object, try out RStudio’s completion facility: type “this”, press TAB, add characters until you have a unique prefix, then press return. Ooops, you made a mistake! this_is_a_really_long_name should have value 3.5 not 2.5. Use another keyboard shortcut to help you fix it. Type “this” then press Cmd/Ctrl + ↑. That will list all the commands you’ve typed that start those letters. Use the arrow keys to navigate, then press enter to retype the command. Change 2.5 to 3.5 and rerun. Make yet another assignment: r_rocks &lt;- 2 ^ 3 Let’s try to inspect it: r_rock #&gt; Error: object &#39;r_rock&#39; not found R_rocks #&gt; Error: object &#39;R_rocks&#39; not found There’s an implied contract between you and R: it will do the tedious computation for you, but in return, you must be completely precise in your instructions. Typos matter. Case matters. 4.3 Calling functions R has a large collection of built-in functions that are called like this: function_name(arg1 = val1, arg2 = val2, ...) Let’s try using seq() which makes regular sequences of numbers and, while we’re at it, learn more helpful features of RStudio. Type se and hit TAB. A popup shows you possible completions. Specify seq() by typing more (a “q”) to disambiguate, or by using ↑/↓ arrows to select. Notice the floating tooltip that pops up, reminding you of the function’s arguments and purpose. If you want more help, press F1 to get all the details in the help tab in the lower right pane. Press TAB once more when you’ve selected the function you want. RStudio will add matching opening (() and closing ()) parentheses for you. Type the arguments 1, 10 and hit return. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Type this code and notice you get similar assistance with the paired quotation marks: x &lt;- &quot;hello world&quot; Quotation marks and parentheses must always come in a pair. RStudio does its best to help you, but it’s still possible to mess up and end up with a mismatch. If this happens, R will show you the continuation character “+”: &gt; x &lt;- &quot;hello + The + tells you that R is waiting for more input; it doesn’t think you’re done yet. Usually that means you’ve forgotten either a &quot; or a ). Either add the missing pair, or press ESCAPE to abort the expression and try again. If you make an assignment, you don’t get to see the value. You’re then tempted to immediately double-check the result: y &lt;- seq(1, 10, length.out = 5) y #&gt; [1] 1.00 3.25 5.50 7.75 10.00 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10, length.out = 5)) #&gt; [1] 1.00 3.25 5.50 7.75 10.00 Now look at your environment in the upper right pane: Here you can see all of the objects that you’ve created. 4.4 Practice Why does this code not work? my_variable &lt;- 10 my_varıable #&gt; Error in eval(expr, envir, enclos): object &#39;my_varıable&#39; not found Look carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.) Tweak each of the following R commands so that they run correctly: library(tidyverse) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) fliter(mpg, cyl = 8) filter(diamond, carat &gt; 3) Press Alt + Shift + K. What happens? How can you get to the same place using the menus? "],
["transform.html", "5 Data transformation 5.1 Introduction 5.2 Filter rows with filter() 5.3 Arrange rows with arrange() 5.4 Select columns with select() 5.5 Add new variables with mutate() 5.6 Grouped summaries with summarise() 5.7 Grouped mutates (and filters)", " 5 Data transformation 5.1 Introduction Visualisation is an important tool for insight generation, but it is rare that you get the data in exactly the right form you need. Often you’ll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this chapter, which will teach you how to transform your data using the dplyr package and a new dataset on flights departing New York City in 2013. 5.1.1 Prerequisites In this chapter we’re going to focus on how to use the dplyr package, another core member of the tidyverse. We’ll illustrate the key ideas using data from the nycflights13 package, and use ggplot2 to help us understand the data. library(nycflights13) library(tidyverse) Take careful note of the conflicts message that’s printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R. If you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag(). 5.1.2 nycflights13 To explore the basic data manipulation verbs of dplyr, we’ll use nycflights13::flights. This data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented in ?flights. flights #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # … with 3.368e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; You might notice that this data frame prints a little differently from other data frames you might have used in the past: it only shows the first few rows and all the columns that fit on one screen. (To see the whole dataset, you can run View(flights) which will open the dataset in the RStudio viewer). It prints differently because it’s a tibble. Tibbles are data frames, but slightly tweaked to work better in the tidyverse. For now, you don’t need to worry about the differences; we’ll come back to tibbles in more detail in wrangle. You might also have noticed the row of three (or four) letter abbreviations under the column names. These describe the type of each variable: int stands for integers. dbl stands for doubles, or real numbers. chr stands for character vectors, or strings. dttm stands for date-times (a date + a time). There are three other common types of variables that aren’t used in this dataset but you’ll encounter later in the book: lgl stands for logical, vectors that contain only TRUE or FALSE. fctr stands for factors, which R uses to represent categorical variables with fixed possible values. date stands for dates. 5.1.3 dplyr basics In this chapter you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values (filter()). Reorder the rows (arrange()). Pick variables by their names (select()). Create new variables with functions of existing variables (mutate()). Collapse many values down to a single summary (summarise()). These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a data frame. The subsequent arguments describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work. 5.2 Filter rows with filter() filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all flights on January 1st with: filter(flights, month == 1, day == 1) #&gt; # A tibble: 842 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # … with 836 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, &lt;-: jan1 &lt;- filter(flights, month == 1, day == 1) R either prints out the results, or saves them to a variable. If you want to do both, you can wrap the assignment in parentheses: (dec25 &lt;- filter(flights, month == 12, day == 25)) #&gt; # A tibble: 719 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 12 25 456 500 -4 649 #&gt; 2 2013 12 25 524 515 9 805 #&gt; 3 2013 12 25 542 540 2 832 #&gt; 4 2013 12 25 546 550 -4 1022 #&gt; 5 2013 12 25 556 600 -4 730 #&gt; 6 2013 12 25 557 600 -3 743 #&gt; # … with 713 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 5.2.1 Comparisons To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). When you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality. When this happens you’ll get an informative error: filter(flights, month = 1) #&gt; Error: `month` (`month = 1`) must not be named, do you need `==`? There’s another common problem you might encounter when using ==: floating point numbers. These results might surprise you! sqrt(2)^2 == 2 #&gt; [1] FALSE 1 / 49 * 49 == 1 #&gt; [1] FALSE Computers use finite precision arithmetic (they obviously can’t store an infinite number of digits!) so remember that every number you see is an approximation. Instead of relying on ==, use near(): near(sqrt(2)^2, 2) #&gt; [1] TRUE near(1 / 49 * 49, 1) #&gt; [1] TRUE 5.2.2 Logical operators Multiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: &amp; is “and”, | is “or”, and ! is “not”. Figure 5.1 shows the complete set of Boolean operations. Figure 5.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. The following code finds all flights that departed in November or December: filter(flights, month == 11 | month == 12) The order of operations doesn’t work like English. You can’t write filter(flights, month == 11 | 12), which you might literally translate into “finds all flights that departed in November or December”. Instead it finds all months that equal 11 | 12, an expression that evaluates to TRUE. In a numeric context (like here), TRUE becomes one, so this finds all flights in January, not November or December. This is quite confusing! A useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the code above: nov_dec &lt;- filter(flights, month %in% c(11, 12)) Sometimes you can simplify complicated subsetting by remembering De Morgan’s law: !(x &amp; y) is the same as !x | !y, and !(x | y) is the same as !x &amp; !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters: filter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120)) filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120) As well as &amp; and |, R also has &amp;&amp; and ||. Don’t use them here! You’ll learn when you should use them in conditional execution. Whenever you start using complicated, multipart expressions in filter(), consider making them explicit variables instead. That makes it much easier to check your work. You’ll learn how to create new variables shortly. 5.2.3 Missing values One important feature of R that can make comparison tricky are missing values, or NAs (“not availables”). NA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown. NA &gt; 5 #&gt; [1] NA 10 == NA #&gt; [1] NA NA + 10 #&gt; [1] NA NA / 2 #&gt; [1] NA The most confusing result is this one: NA == NA #&gt; [1] NA It’s easiest to understand why this is true with a bit more context: # Let x be Mary&#39;s age. We don&#39;t know how old she is. x &lt;- NA # Let y be John&#39;s age. We don&#39;t know how old he is. y &lt;- NA # Are John and Mary the same age? x == y #&gt; [1] NA # We don&#39;t know! If you want to determine if a value is missing, use is.na(): is.na(x) #&gt; [1] TRUE filter() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. If you want to preserve missing values, ask for them explicitly: df &lt;- tibble(x = c(1, NA, 3)) filter(df, x &gt; 1) #&gt; # A tibble: 1 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 3 filter(df, is.na(x) | x &gt; 1) #&gt; # A tibble: 2 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 NA #&gt; 2 3 5.2.4 Exercises Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive) Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? How many flights have a missing dep_time? What other variables are missing? What might these rows represent? Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE &amp; NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!) 5.3 Arrange rows with arrange() arrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns: arrange(flights, year, month, day) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # … with 3.368e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Use desc() to re-order by a column in descending order: arrange(flights, desc(dep_delay)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 #&gt; 2 2013 6 15 1432 1935 1137 1607 #&gt; 3 2013 1 10 1121 1635 1126 1239 #&gt; 4 2013 9 20 1139 1845 1014 1457 #&gt; 5 2013 7 22 845 1600 1005 1044 #&gt; 6 2013 4 10 1100 1900 960 1342 #&gt; # … with 3.368e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Missing values are always sorted at the end: df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 2 #&gt; 2 5 #&gt; 3 NA arrange(df, desc(x)) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 5 #&gt; 2 2 #&gt; 3 NA 5.3.1 Exercises How could you use arrange() to sort all missing values to the start? (Hint: use is.na()). Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest flights. Which flights travelled the longest? Which travelled the shortest? 5.4 Select columns with select() It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: # Select columns by name select(flights, year, month, day) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 3.368e+05 more rows # Select all columns between year and day (inclusive) select(flights, year:day) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # … with 3.368e+05 more rows # Select all columns except those from year to day (inclusive) select(flights, -(year:day)) #&gt; # A tibble: 336,776 x 16 #&gt; dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 515 2 830 819 11 #&gt; 2 533 529 4 850 830 20 #&gt; 3 542 540 2 923 850 33 #&gt; 4 544 545 -1 1004 1022 -18 #&gt; 5 554 600 -6 812 837 -25 #&gt; 6 554 558 -4 740 728 12 #&gt; # … with 3.368e+05 more rows, and 10 more variables: carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; There are a number of helper functions you can use within select(): starts_with(&quot;abc&quot;): matches names that begin with “abc”. ends_with(&quot;xyz&quot;): matches names that end with “xyz”. contains(&quot;ijk&quot;): matches names that contain “ijk”. matches(&quot;(.)\\\\1&quot;): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings. num_range(&quot;x&quot;, 1:3): matches x1, x2 and x3. See ?select for more details. select() can be used to rename variables, but it’s rarely useful because it drops all of the variables not explicitly mentioned. Instead, use rename(), which is a variant of select() that keeps all the variables that aren’t explicitly mentioned: rename(flights, tail_num = tailnum) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # … with 3.368e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tail_num &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Another option is to use select() in conjunction with the everything() helper. This is useful if you have a handful of variables you’d like to move to the start of the data frame. select(flights, time_hour, air_time, everything()) #&gt; # A tibble: 336,776 x 19 #&gt; time_hour air_time year month day dep_time sched_dep_time #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013-01-01 05:00:00 227 2013 1 1 517 515 #&gt; 2 2013-01-01 05:00:00 227 2013 1 1 533 529 #&gt; 3 2013-01-01 05:00:00 160 2013 1 1 542 540 #&gt; 4 2013-01-01 05:00:00 183 2013 1 1 544 545 #&gt; 5 2013-01-01 06:00:00 116 2013 1 1 554 600 #&gt; 6 2013-01-01 05:00:00 150 2013 1 1 554 558 #&gt; # … with 3.368e+05 more rows, and 12 more variables: dep_delay &lt;dbl&gt;, #&gt; # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt; 5.4.1 Exercises Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. What happens if you include the name of a variable multiple times in a select() call? What does the one_of() function do? Why might it be helpful in conjunction with this vector? vars &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;dep_delay&quot;, &quot;arr_delay&quot;) Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? select(flights, contains(&quot;TIME&quot;)) 5.5 Add new variables with mutate() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of mutate(). mutate() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. Remember that when you’re in RStudio, the easiest way to see all the columns is View(). flights_sml &lt;- select( flights, year:day, ends_with(&quot;delay&quot;), distance, air_time ) mutate(flights_sml, gain = dep_delay - arr_delay, speed = distance / air_time * 60 ) #&gt; # A tibble: 336,776 x 9 #&gt; year month day dep_delay arr_delay distance air_time gain speed #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 -9 370. #&gt; 2 2013 1 1 4 20 1416 227 -16 374. #&gt; 3 2013 1 1 2 33 1089 160 -31 408. #&gt; 4 2013 1 1 -1 -18 1576 183 17 517. #&gt; 5 2013 1 1 -6 -25 762 116 19 394. #&gt; 6 2013 1 1 -4 12 719 150 -16 288. #&gt; # … with 3.368e+05 more rows Note that you can refer to columns that you’ve just created: mutate(flights_sml, gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) #&gt; # A tibble: 336,776 x 10 #&gt; year month day dep_delay arr_delay distance air_time gain hours #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 -9 3.78 #&gt; 2 2013 1 1 4 20 1416 227 -16 3.78 #&gt; 3 2013 1 1 2 33 1089 160 -31 2.67 #&gt; 4 2013 1 1 -1 -18 1576 183 17 3.05 #&gt; 5 2013 1 1 -6 -25 762 116 19 1.93 #&gt; 6 2013 1 1 -4 12 719 150 -16 2.5 #&gt; # … with 3.368e+05 more rows, and 1 more variable: gain_per_hour &lt;dbl&gt; If you only want to keep the new variables, use transmute(): transmute(flights, gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) #&gt; # A tibble: 336,776 x 3 #&gt; gain hours gain_per_hour #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -9 3.78 -2.38 #&gt; 2 -16 3.78 -4.23 #&gt; 3 -31 2.67 -11.6 #&gt; 4 17 3.05 5.57 #&gt; 5 19 1.93 9.83 #&gt; 6 -16 2.5 -6.4 #&gt; # … with 3.368e+05 more rows 5.5.1 Useful creation functions There are many functions for creating new variables that you can use with mutate(). The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Arithmetic operators: +, -, *, /, ^. These are all vectorised, using the so called “recycling rules”. If one parameter is shorter than the other, it will be automatically extended to be the same length. This is most useful when one of the arguments is a single number: air_time / 60, hours * 60 + minute, etc. Arithmetic operators are also useful in conjunction with the aggregate functions you’ll learn about later. For example, x / sum(x) calculates the proportion of a total, and y - mean(y) computes the difference from the mean. Modular arithmetic: %/% (integer division) and %% (remainder), where x == y * (x %/% y) + (x %% y). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the flights dataset, you can compute hour and minute from dep_time with: transmute(flights, dep_time, hour = dep_time %/% 100, minute = dep_time %% 100 ) #&gt; # A tibble: 336,776 x 3 #&gt; dep_time hour minute #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 517 5 17 #&gt; 2 533 5 33 #&gt; 3 542 5 42 #&gt; 4 544 5 44 #&gt; 5 554 5 54 #&gt; 6 554 5 54 #&gt; # … with 3.368e+05 more rows Logs: log(), log2(), log10(). Logarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude. They also convert multiplicative relationships to additive, a feature we’ll come back to in modelling. All else being equal, I recommend using log2() because it’s easy to interpret: a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving. Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)). They are most useful in conjunction with group_by(), which you’ll learn about shortly. (x &lt;- 1:10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 lag(x) #&gt; [1] NA 1 2 3 4 5 6 7 8 9 lead(x) #&gt; [1] 2 3 4 5 6 7 8 9 10 NA Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); and dplyr provides cummean() for cumulative means. If you need rolling aggregates (i.e. a sum computed over a rolling window), try the RcppRoll package. x #&gt; [1] 1 2 3 4 5 6 7 8 9 10 cumsum(x) #&gt; [1] 1 3 6 10 15 21 28 36 45 55 cummean(x) #&gt; [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, which you learned about earlier. If you’re doing a complex sequence of logical operations it’s often a good idea to store the interim values in new variables so you can check that each step is working as expected. Ranking: there are a number of ranking functions, but you should start with min_rank(). It does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks. y &lt;- c(1, 2, 2, NA, 3, 4) min_rank(y) #&gt; [1] 1 2 2 NA 4 5 min_rank(desc(y)) #&gt; [1] 5 3 3 NA 2 1 If min_rank() doesn’t do what you need, look at the variants row_number(), dense_rank(), percent_rank(), cume_dist(), ntile(). See their help pages for more details. row_number(y) #&gt; [1] 1 2 3 NA 4 5 dense_rank(y) #&gt; [1] 1 2 2 NA 3 4 percent_rank(y) #&gt; [1] 0.00 0.25 0.25 NA 0.75 1.00 cume_dist(y) #&gt; [1] 0.2 0.6 0.6 NA 0.8 1.0 5.5.2 Exercises Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank(). What does 1:3 + 1:10 return? Why? What trigonometric functions does R provide? 5.6 Grouped summaries with summarise() The last key verb is summarise(). It collapses a data frame to a single row: summarise(flights, delay = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 1 x 1 #&gt; delay #&gt; &lt;dbl&gt; #&gt; 1 12.6 (We’ll come back to what that na.rm = TRUE means very shortly.) summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date: by_day &lt;- group_by(flights, year, month, day) summarise(by_day, delay = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows Together group_by() and summarise() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe. 5.6.1 Combining multiple operations with the pipe Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about dplyr, you might write code like this: by_dest &lt;- group_by(flights, dest) delay &lt;- summarise(by_dest, count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) delay &lt;- filter(delay, count &gt; 20, dest != &quot;HNL&quot;) # It looks like delays increase with distance up to ~750 miles # and then decrease. Maybe as flights get longer there&#39;s more # ability to make up delays in the air? ggplot(data = delay, mapping = aes(x = dist, y = delay)) + geom_point(aes(size = count), alpha = 1 / 3) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; There are three steps to prepare this data: Group flights by destination. Summarise to compute distance, average delay, and number of flights. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport. This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis. There’s another way to tackle the same problem with the pipe, %&gt;%: delays &lt;- flights %&gt;% group_by(dest) %&gt;% summarise( count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% filter(count &gt; 20, dest != &quot;HNL&quot;) This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce %&gt;% when reading code is “then”. Behind the scenes, x %&gt;% f(y) turns into f(x, y), and x %&gt;% f(y) %&gt;% g(z) turns into g(f(x, y), z) and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use piping frequently from now on because it considerably improves the readability of code, and we’ll come back to it in more detail in pipes. Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered. Unfortunately, the next iteration of ggplot2, ggvis, which does use the pipe, isn’t quite ready for prime time yet. 5.6.2 Missing values You may have wondered about the na.rm argument we used above. What happens if we don’t set it? flights %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 NA #&gt; 2 2013 1 2 NA #&gt; 3 2013 1 3 NA #&gt; 4 2013 1 4 NA #&gt; 5 2013 1 5 NA #&gt; 6 2013 1 6 NA #&gt; # … with 359 more rows We get a lot of missing values! That’s because aggregation functions obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. Fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation: flights %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows In this case, where missing values represent cancelled flights, we could also tackle the problem by first removing the cancelled flights. We’ll save this dataset so we can reuse in the next few examples. not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day mean #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.4 #&gt; 2 2013 1 2 13.7 #&gt; 3 2013 1 3 10.9 #&gt; 4 2013 1 4 8.97 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows 5.6.3 Counts Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. For example, let’s look at the planes (identified by their tail number) that have the highest average delays: delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise( delay = mean(arr_delay) ) ggplot(data = delays, mapping = aes(x = delay)) + geom_freqpoly(binwidth = 10) Wow, there are some planes that have an average delay of 5 hours (300 minutes)! The story is actually a little more nuanced. We can get more insight if we draw a scatterplot of number of flights vs. average delay: delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarise( delay = mean(arr_delay, na.rm = TRUE), n = n() ) ggplot(data = delays, mapping = aes(x = n, y = delay)) + geom_point(alpha = 1 / 10) Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you’ll see that the variation decreases as the sample size increases. When looking at this sort of plot, it’s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups. This is what the following code does, as well as showing you a handy pattern for integrating ggplot2 into dplyr flows. It’s a bit painful that you have to switch from %&gt;% to +, but once you get the hang of it, it’s quite convenient. delays %&gt;% filter(n &gt; 25) %&gt;% ggplot(mapping = aes(x = n, y = delay)) + geom_point(alpha = 1 / 10) RStudio tip: a useful keyboard shortcut is Cmd/Ctrl + Shift + P. This resends the previously sent chunk from the editor to the console. This is very convenient when you’re (e.g.) exploring the value of n in the example above. You send the whole block once with Cmd/Ctrl + Enter, then you modify the value of n and press Cmd/Ctrl + Shift + P to resend the complete block. There’s another common variation of this type of pattern. Let’s look at how the average performance of batters in baseball is related to the number of times they’re at bat. Here I use data from the Lahman package to compute the batting average (number of hits / number of attempts) of every major league baseball player. When I plot the skill of the batter (measured by the batting average, ba) against the number of opportunities to hit the ball (measured by at bat, ab), you see two patterns: As above, the variation in our aggregate decreases as we get more data points. There’s a positive correlation between skill (ba) and opportunities to hit the ball (ab). This is because teams control who gets to play, and obviously they’ll pick their best players. # Convert to a tibble so it prints nicely batting &lt;- as_tibble(Lahman::Batting) batters &lt;- batting %&gt;% group_by(playerID) %&gt;% summarise( ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE), ab = sum(AB, na.rm = TRUE) ) batters %&gt;% filter(ab &gt; 100) %&gt;% ggplot(mapping = aes(x = ab, y = ba)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; This also has important implications for ranking. If you naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled: batters %&gt;% arrange(desc(ba)) #&gt; # A tibble: 18,915 x 3 #&gt; playerID ba ab #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 abramge01 1 1 #&gt; 2 banisje01 1 1 #&gt; 3 bartocl01 1 1 #&gt; 4 bassdo01 1 1 #&gt; 5 berrijo01 1 1 #&gt; 6 birasst01 1 2 #&gt; # … with 1.891e+04 more rows You can find a good explanation of this problem at http://varianceexplained.org/r/empirical_bayes_baseball/ and http://www.evanmiller.org/how-not-to-sort-by-average-rating.html. 5.6.4 Useful summary functions Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions: Measures of location: we’ve used mean(x), but median(x) is also useful. The mean is the sum divided by the length; the median is a value where 50% of x is above it, and 50% is below it. It’s sometimes useful to combine aggregation with logical subsetting. We haven’t talked about this sort of subsetting yet, but you’ll learn more about it in [subsetting]. not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise( avg_delay1 = mean(arr_delay), avg_delay2 = mean(arr_delay[arr_delay &gt; 0]) # the average positive delay ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: year, month [?] #&gt; year month day avg_delay1 avg_delay2 #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 12.7 32.5 #&gt; 2 2013 1 2 12.7 32.0 #&gt; 3 2013 1 3 5.73 27.7 #&gt; 4 2013 1 4 -1.93 28.3 #&gt; 5 2013 1 5 -1.53 22.6 #&gt; 6 2013 1 6 4.24 24.4 #&gt; # … with 359 more rows Measures of spread: sd(x), IQR(x), mad(x). The root mean squared deviation, or standard deviation or sd for short, is the standard measure of spread. The interquartile range IQR() and median absolute deviation mad(x) are robust equivalents that may be more useful if you have outliers. # Why is distance to some destinations more variable than to others? not_cancelled %&gt;% group_by(dest) %&gt;% summarise(distance_sd = sd(distance)) %&gt;% arrange(desc(distance_sd)) #&gt; # A tibble: 104 x 2 #&gt; dest distance_sd #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 EGE 10.5 #&gt; 2 SAN 10.4 #&gt; 3 SFO 10.2 #&gt; 4 HNL 10.0 #&gt; 5 SEA 9.98 #&gt; 6 LAS 9.91 #&gt; # … with 98 more rows Measures of rank: min(x), quantile(x, 0.25), max(x). Quantiles are a generalisation of the median. For example, quantile(x, 0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%. # When do the first and last flights leave each day? not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise( first = min(dep_time), last = max(dep_time) ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: year, month [?] #&gt; year month day first last #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 517 2356 #&gt; 2 2013 1 2 42 2354 #&gt; 3 2013 1 3 32 2349 #&gt; 4 2013 1 4 25 2358 #&gt; 5 2013 1 5 14 2357 #&gt; 6 2013 1 6 16 2355 #&gt; # … with 359 more rows Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)] but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements). For example, we can find the first and last departure for each day: not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise( first_dep = first(dep_time), last_dep = last(dep_time) ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: year, month [?] #&gt; year month day first_dep last_dep #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 2356 #&gt; 2 2013 1 2 42 2354 #&gt; 3 2013 1 3 32 2349 #&gt; 4 2013 1 4 25 2358 #&gt; 5 2013 1 5 14 2357 #&gt; 6 2013 1 6 16 2355 #&gt; # … with 359 more rows These functions are complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row: not_cancelled %&gt;% group_by(year, month, day) %&gt;% mutate(r = min_rank(desc(dep_time))) %&gt;% filter(r %in% range(r)) #&gt; # A tibble: 770 x 20 #&gt; # Groups: year, month, day [365] #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 2356 2359 -3 425 #&gt; 3 2013 1 2 42 2359 43 518 #&gt; 4 2013 1 2 2354 2359 -5 413 #&gt; 5 2013 1 3 32 2359 33 504 #&gt; 6 2013 1 3 2349 2359 -10 434 #&gt; # … with 764 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, r &lt;int&gt; Counts: You’ve seen n(), which takes no arguments, and returns the size of the current group. To count the number of non-missing values, use sum(!is.na(x)). To count the number of distinct (unique) values, use n_distinct(x). # Which destinations have the most carriers? not_cancelled %&gt;% group_by(dest) %&gt;% summarise(carriers = n_distinct(carrier)) %&gt;% arrange(desc(carriers)) #&gt; # A tibble: 104 x 2 #&gt; dest carriers #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 7 #&gt; 2 BOS 7 #&gt; 3 CLT 7 #&gt; 4 ORD 7 #&gt; 5 TPA 7 #&gt; 6 AUS 6 #&gt; # … with 98 more rows Counts are so useful that dplyr provides a simple helper if all you want is a count: not_cancelled %&gt;% count(dest) #&gt; # A tibble: 104 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ABQ 254 #&gt; 2 ACK 264 #&gt; 3 ALB 418 #&gt; 4 ANC 8 #&gt; 5 ATL 16837 #&gt; 6 AUS 2411 #&gt; # … with 98 more rows You can optionally provide a weight variable. For example, you could use this to “count” (sum) the total number of miles a plane flew: not_cancelled %&gt;% count(tailnum, wt = distance) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # … with 4,031 more rows Counts and proportions of logical values: sum(x &gt; 10), mean(y == 0). When used with numeric functions, TRUE is converted to 1 and FALSE to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and mean(x) gives the proportion. # How many flights left before 5am? (these usually indicate delayed # flights from the previous day) not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(n_early = sum(dep_time &lt; 500)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day n_early #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 0 #&gt; 2 2013 1 2 3 #&gt; 3 2013 1 3 4 #&gt; 4 2013 1 4 3 #&gt; 5 2013 1 5 3 #&gt; 6 2013 1 6 2 #&gt; # … with 359 more rows # What proportion of flights are delayed by more than an hour? not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(hour_perc = mean(arr_delay &gt; 60)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day hour_perc #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 0.0722 #&gt; 2 2013 1 2 0.0851 #&gt; 3 2013 1 3 0.0567 #&gt; 4 2013 1 4 0.0396 #&gt; 5 2013 1 5 0.0349 #&gt; 6 2013 1 6 0.0470 #&gt; # … with 359 more rows 5.6.5 Grouping by multiple variables When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset: daily &lt;- group_by(flights, year, month, day) (per_day &lt;- summarise(daily, flights = n())) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [?] #&gt; year month day flights #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 842 #&gt; 2 2013 1 2 943 #&gt; 3 2013 1 3 914 #&gt; 4 2013 1 4 915 #&gt; 5 2013 1 5 720 #&gt; 6 2013 1 6 832 #&gt; # … with 359 more rows (per_month &lt;- summarise(per_day, flights = sum(flights))) #&gt; # A tibble: 12 x 3 #&gt; # Groups: year [?] #&gt; year month flights #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 27004 #&gt; 2 2013 2 24951 #&gt; 3 2013 3 28834 #&gt; 4 2013 4 28330 #&gt; 5 2013 5 28796 #&gt; 6 2013 6 28243 #&gt; # … with 6 more rows (per_year &lt;- summarise(per_month, flights = sum(flights))) #&gt; # A tibble: 1 x 2 #&gt; year flights #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 336776 Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median. 5.6.6 Ungrouping If you need to remove grouping, and return to operations on ungrouped data, use ungroup(). daily %&gt;% ungroup() %&gt;% # no longer grouped by date summarise(flights = n()) # all flights #&gt; # A tibble: 1 x 1 #&gt; flights #&gt; &lt;int&gt; #&gt; 1 336776 5.6.7 Exercises Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. A flight is always 10 minutes late. A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay? Come up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()). Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column? Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n())) What does the sort argument to count() do. When might you use it? 5.7 Grouped mutates (and filters) Grouping is most useful in conjunction with summarise(), but you can also do convenient operations with mutate() and filter(): Find the worst members of each group: flights_sml %&gt;% group_by(year, month, day) %&gt;% filter(rank(desc(arr_delay)) &lt; 10) #&gt; # A tibble: 3,306 x 7 #&gt; # Groups: year, month, day [365] #&gt; year month day dep_delay arr_delay distance air_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 853 851 184 41 #&gt; 2 2013 1 1 290 338 1134 213 #&gt; 3 2013 1 1 260 263 266 46 #&gt; 4 2013 1 1 157 174 213 60 #&gt; 5 2013 1 1 216 222 708 121 #&gt; 6 2013 1 1 255 250 589 115 #&gt; # … with 3,300 more rows Find all groups bigger than a threshold: popular_dests &lt;- flights %&gt;% group_by(dest) %&gt;% filter(n() &gt; 365) popular_dests #&gt; # A tibble: 332,577 x 19 #&gt; # Groups: dest [77] #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # … with 3.326e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Standardise to compute per group metrics: popular_dests %&gt;% filter(arr_delay &gt; 0) %&gt;% mutate(prop_delay = arr_delay / sum(arr_delay)) %&gt;% select(year:day, dest, arr_delay, prop_delay) #&gt; # A tibble: 131,106 x 6 #&gt; # Groups: dest [77] #&gt; year month day dest arr_delay prop_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 IAH 11 0.000111 #&gt; 2 2013 1 1 IAH 20 0.000201 #&gt; 3 2013 1 1 MIA 33 0.000235 #&gt; 4 2013 1 1 ORD 12 0.0000424 #&gt; 5 2013 1 1 FLL 19 0.0000938 #&gt; 6 2013 1 1 ORD 8 0.0000283 #&gt; # … with 1.311e+05 more rows A grouped filter is a grouped mutate followed by an ungrouped filter. I generally avoid them except for quick and dirty manipulations: otherwise it’s hard to check that you’ve done the manipulation correctly. Functions that work most naturally in grouped mutates and filters are known as window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: vignette(&quot;window-functions&quot;). 5.7.1 Exercises Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping. Which plane (tailnum) has the worst on-time record? What time of day should you fly if you want to avoid delays as much as possible? For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. For each plane, count the number of flights before the first delay of greater than 1 hour. "],
["workflow-scripts.html", "6 Workflow: scripts 6.1 Running code 6.2 RStudio diagnostics 6.3 Practice", " 6 Workflow: scripts So far you’ve been using the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File, then R script, or using the keyboard shortcut Cmd/Ctrl + Shift + N. Now you’ll see four panes: The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. 6.1 Running code The script editor is also a great place to build up complex ggplot2 plots or long sequences of dplyr manipulations. The key to using the script editor effectively is to memorise one of the most important keyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R expression in the console. For example, take the code below. If your cursor is at █, pressing Cmd/Ctrl + Enter will run the complete command that generates not_cancelled. It will also move the cursor to the next statement (beginning with not_cancelled %&gt;%). That makes it easy to run your complete script by repeatedly pressing Cmd/Ctrl + Enter. library(dplyr) library(nycflights13) not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% group_by(year, month, day) %&gt;% summarise(mean = mean(dep_delay)) Instead of running expression-by-expression, you can also execute the complete script in one step: Cmd/Ctrl + Shift + S. Doing this regularly is a great way to check that you’ve captured all the important parts of your code in the script. I recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see what packages they need to install. Note, however, that you should never include install.packages() or setwd() in a script that you share. It’s very antisocial to change settings on someone else’s computer! When working through future chapters, I highly recommend starting in the editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it. 6.2 RStudio diagnostics The script editor will also highlight syntax errors with a red squiggly line and a cross in the sidebar: Hover over the cross to see what the problem is: RStudio will also let you know about potential problems: 6.3 Practice Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it! What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out. "],
["exploratory-data-analysis.html", "7 Exploratory Data Analysis 7.1 Introduction 7.2 Questions 7.3 Variation 7.4 Missing values 7.5 Covariation 7.6 Patterns and models 7.7 ggplot2 calls 7.8 Learning more", " 7 Exploratory Data Analysis 7.1 Introduction This chapter will show you how to use visualisation and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. Use what you learn to refine your questions and/or generate new questions. EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others. EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling. 7.1.1 Prerequisites In this chapter we’ll combine what you’ve learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions. library(tidyverse) 7.2 Questions “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make. EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find. There is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as: What type of variation occurs within my variables? What type of covariation occurs between my variables? The rest of this chapter will look at these two questions. I’ll explain what variation and covariation are, and I’ll show you several ways to answer each question. To make the discussion easier, let’s define some terms: A variable is a quantity, quality, or property that you can measure. A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement. An observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. I’ll sometimes refer to an observation as a data point. Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row. So far, all of the data that you’ve seen has been tidy. In real-life, most data isn’t tidy, so we’ll come back to these ideas again in [tidy data]. 7.3 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. This is true even if you measure quantities that are constant, like the speed of light. Each of your measurements will include a small amount of error that varies from measurement to measurement. Categorical variables can also vary if you measure across different subjects (e.g. the eye colors of different people), or different times (e.g. the energy levels of an electron at different moments). Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable’s values. 7.3.1 Visualising distributions How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In R, categorical variables are usually saved as factors or character vectors. To examine the distribution of a categorical variable, use a bar chart: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) The height of the bars displays how many observations occurred with each x value. You can compute these values manually with dplyr::count(): diamonds %&gt;% count(cut) #&gt; # A tibble: 5 x 2 #&gt; cut n #&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 Fair 1610 #&gt; 2 Good 4906 #&gt; 3 Very Good 12082 #&gt; 4 Premium 13791 #&gt; 5 Ideal 21551 A variable is continuous if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram: ggplot(data = diamonds) + geom_histogram(mapping = aes(x = carat), binwidth = 0.5) You can compute this by hand by combining dplyr::count() and ggplot2::cut_width(): diamonds %&gt;% count(cut_width(carat, 0.5)) #&gt; # A tibble: 11 x 2 #&gt; `cut_width(carat, 0.5)` n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 [-0.25,0.25] 785 #&gt; 2 (0.25,0.75] 29498 #&gt; 3 (0.75,1.25] 15977 #&gt; 4 (1.25,1.75] 5313 #&gt; 5 (1.75,2.25] 2002 #&gt; 6 (2.25,2.75] 322 #&gt; # … with 5 more rows A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that almost 30,000 observations have a carat value between 0.25 and 0.75, which are the left and right edges of the bar. You can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth. smaller &lt;- diamonds %&gt;% filter(carat &lt; 3) ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.1) If you wish to overlay multiple histograms in the same plot, I recommend using geom_freqpoly() instead of geom_histogram(). geom_freqpoly() performs the same calculation as geom_histogram(), but instead of displaying the counts with bars, uses lines instead. It’s much easier to understand overlapping lines than bars. ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) + geom_freqpoly(binwidth = 0.1) There are a few challenges with this type of plot, which we will come back to in visualising a categorical and a continuous variable. Now that you can visualise variation, what should you look for in your plots? And what type of follow-up questions should you ask? I’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?). 7.3.2 Typical values In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. To turn this information into useful questions, look for anything unexpected: Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? As an example, the histogram below suggests several interesting questions: Why are there more diamonds at whole carats and common fractions of carats? Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak? Why are there no diamonds bigger than 3 carats? ggplot(data = smaller, mapping = aes(x = carat)) + geom_histogram(binwidth = 0.01) Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask: How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between. ggplot(data = faithful, mapping = aes(x = eruptions)) + geom_histogram(binwidth = 0.25) Many of the questions above will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable. We’ll get to that shortly. 7.3.3 Unusual values Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis. ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) There are so many observations in the common bins that the rare bins are so short that you can’t see them (although maybe if you stare intently at 0 you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian(): ggplot(diamonds) + geom_histogram(mapping = aes(x = y), binwidth = 0.5) + coord_cartesian(ylim = c(0, 50)) (coord_cartesian() also has an xlim() argument for when you need to zoom into the x-axis. ggplot2 also has xlim() and ylim() functions that work slightly differently: they throw away the data outside the limits.) This allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr: unusual &lt;- diamonds %&gt;% filter(y &lt; 3 | y &gt; 20) %&gt;% select(price, x, y, z) %&gt;% arrange(y) unusual #&gt; # A tibble: 9 x 4 #&gt; price x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 5139 0 0 0 #&gt; 2 6381 0 0 0 #&gt; 3 12800 0 0 0 #&gt; 4 15686 0 0 0 #&gt; 5 18034 0 0 0 #&gt; 6 2130 0 0 0 #&gt; 7 2130 0 0 0 #&gt; 8 2075 5.15 31.8 5.12 #&gt; 9 12210 8.09 58.9 8.06 The y variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars! It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up. 7.3.4 Exercises Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? 7.4 Missing values If you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options. Drop the entire row with the strange values: diamonds2 &lt;- diamonds %&gt;% filter(between(y, 3, 20)) I don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left! Instead, I recommend replacing the unusual values with missing values. The easiest way to do this is to use mutate() to replace the variable with a modified copy. You can use the ifelse() function to replace unusual values with NA: diamonds2 &lt;- diamonds %&gt;% mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y)) ifelse() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is false. Alternatively to ifelse, use dplyr::case_when(). case_when() is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed: ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + geom_point() #&gt; Warning: Removed 9 rows containing missing values (geom_point). To suppress that warning, set na.rm = TRUE: ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + geom_point(na.rm = TRUE) Other times you want to understand what makes observations with missing values different to observations with recorded values. For example, in nycflights13::flights, missing values in the dep_time variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable with is.na(). nycflights13::flights %&gt;% mutate( cancelled = is.na(dep_time), sched_hour = sched_dep_time %/% 100, sched_min = sched_dep_time %% 100, sched_dep_time = sched_hour + sched_min / 60 ) %&gt;% ggplot(mapping = aes(sched_dep_time)) + geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1 / 4) However this plot isn’t great because there are many more non-cancelled flights than cancelled flights. In the next section we’ll explore some techniques for improving this comparison. 7.4.1 Exercises What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? What does na.rm = TRUE do in mean() and sum()? 7.5 Covariation If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should again depend on the type of variables involved. 7.5.1 A categorical and continuous variable It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. For example, let’s explore how the price of a diamond varies with its quality: ggplot(data = diamonds, mapping = aes(x = price)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) It’s hard to see the difference in distribution because the overall counts differ so much: ggplot(diamonds) + geom_bar(mapping = aes(x = cut)) To make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display density, which is the count standardised so that the area under each frequency polygon is one. ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot. Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of: A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side. Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually. A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution. Let’s take a look at the distribution of price by cut using geom_boxplot(): ggplot(data = diamonds, mapping = aes(x = cut, y = price)) + geom_boxplot() We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counterintuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why. cut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is with the reorder() function. For example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes: ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() To make the trend easier to see, we can reorder class based on the median value of hwy: ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) If you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that with coord_flip(). ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip() 7.5.1.1 Exercises Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? Install the ggstance package, and create a horizontal boxplot. How does this compare to using coord_flip()? One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs cut. What do you learn? How do you interpret the plots? Compare and contrast geom_violin() with a facetted geom_histogram(), or a coloured geom_freqpoly(). What are the pros and cons of each method? If you have a small dataset, it’s sometimes useful to use geom_jitter() to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does. 7.5.2 Two categorical variables To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to rely on the built-in geom_count(): ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color)) The size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values. Another approach is to compute the count with dplyr: diamonds %&gt;% count(color, cut) #&gt; # A tibble: 35 x 3 #&gt; color cut n #&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 D Fair 163 #&gt; 2 D Good 662 #&gt; 3 D Very Good 1513 #&gt; 4 D Premium 1603 #&gt; 5 D Ideal 2834 #&gt; 6 E Fair 224 #&gt; # … with 29 more rows Then visualise with geom_tile() and the fill aesthetic: diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n)) If the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, you might want to try the d3heatmap or heatmaply packages, which create interactive plots. 7.5.2.1 Exercises How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Use geom_tile() together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? Why is it slightly better to use aes(x = color, y = cut) rather than aes(x = cut, y = color) in the example above? 7.5.3 Two continuous variables You’ve already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with geom_point(). You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the carat size and price of a diamond. ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above). You’ve already seen one way to fix the problem: using the alpha aesthetic to add transparency. ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100) But using transparency can be challenging for very large datasets. Another solution is to use bin. Previously you used geom_histogram() and geom_freqpoly() to bin in one dimension. Now you’ll learn how to use geom_bin2d() and geom_hex() to bin in two dimensions. geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex(). ggplot(data = smaller) + geom_bin2d(mapping = aes(x = carat, y = price)) # install.packages(&quot;hexbin&quot;) ggplot(data = smaller) + geom_hex(mapping = aes(x = carat, y = price)) Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot: ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) cut_width(x, width), as used above, divides x into bins of width width. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarises a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with varwidth = TRUE. Another approach is to display approximately the same number of points in each bin. That’s the job of cut_number(): ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_number(carat, 20))) 7.5.3.1 Exercises Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price? Visualise the distribution of carat, partitioned by price. How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you? Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. ggplot(data = diamonds) + geom_point(mapping = aes(x = x, y = y)) + coord_cartesian(xlim = c(4, 11), ylim = c(4, 11)) Why is a scatterplot a better display than a binned plot for this case? 7.6 Patterns and models Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself: Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above. ggplot(data = faithful) + geom_point(mapping = aes(x = eruptions, y = waiting)) Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second. Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. library(modelr) mod &lt;- lm(log(price) ~ log(carat), data = diamonds) diamonds2 &lt;- diamonds %&gt;% add_residuals(mod) %&gt;% mutate(resid = exp(resid)) ggplot(data = diamonds2) + geom_point(mapping = aes(x = carat, y = resid)) Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive. ggplot(data = diamonds2) + geom_boxplot(mapping = aes(x = cut, y = resid)) You’ll learn how models, and the modelr package, work in the final part of the book, model. We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand. 7.7 ggplot2 calls As we move on from these introductory chapters, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning: ggplot(data = faithful, mapping = aes(x = eruptions)) + geom_freqpoly(binwidth = 0.25) Typically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, and the first two arguments to aes() are x and y. In the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back in functions. Rewriting the previous plot more concisely yields: ggplot(faithful, aes(eruptions)) + geom_freqpoly(binwidth = 0.25) Sometimes we’ll turn the end of a pipeline of data transformation into a plot. Watch for the transition from %&gt;% to +. I wish this transition wasn’t necessary but unfortunately ggplot2 was created before the pipe was discovered. diamonds %&gt;% count(cut, clarity) %&gt;% ggplot(aes(clarity, cut, fill = n)) + geom_tile() 7.8 Learning more If you want to learn more about the mechanics of ggplot2, I’d highly recommend grabbing a copy of the ggplot2 book: https://amzn.com/331924275X. It’s been recently updated, so it includes dplyr and tidyr code, and has much more space to explore all the facets of visualisation. Unfortunately the book isn’t generally available for free, but if you have a connection to a university you can probably get an electronic version for free through SpringerLink. Another useful resource is the R Graphics Cookbook by Winston Chang. Much of the contents are available online at http://www.cookbook-r.com/Graphs/. I also recommend Graphical Data Analysis with R, by Antony Unwin. This is a book-length treatment similar to the material covered in this chapter, but has the space to go into much greater depth. "],
["workflow-projects.html", "8 Workflow: projects 8.1 What is real? 8.2 Where does your analysis live? 8.3 Paths and directories 8.4 RStudio projects 8.5 Summary", " 8 Workflow: projects One day you will need to quit R, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 8.1 What is real? As a beginning R user, it’s OK to consider your environment (i.e. the objects listed in the environment pane) “real”. However, in the long run, you’ll be much better off if you consider your R scripts as “real”. With your R scripts (and your data files), you can recreate the environment. It’s much harder to recreate your R scripts from your environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your R history. To foster this behaviour, I highly recommend that you instruct RStudio not to preserve your workspace between sessions: This will cause you some short-term pain, because now when you restart RStudio it will not remember the results of the code that you ran last time. But this short-term pain will save you long-term agony because it forces you to capture all important interactions in your code. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code. There is a great pair of keyboard shortcuts that will work together to make sure you’ve captured the important parts of your code in the editor: Press Cmd/Ctrl + Shift + F10 to restart RStudio. Press Cmd/Ctrl + Shift + S to rerun the current script. I use this pattern hundreds of times a week. 8.2 Where does your analysis live? R has a powerful notion of the working directory. This is where R looks for files that you ask it to load, and where it will put any files that you ask it to save. RStudio shows your current working directory at the top of the console: And you can print this out in R code by running getwd(): getwd() # &gt; [1] &quot;/Users/hadley/Documents/r4ds/r4ds&quot; As a beginning R user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be R’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organising your analytical projects into directories and, when working on a project, setting R’s working directory to the associated directory. I do not recommend it, but you can also set the working directory from within R: setwd(&quot;/path/to/my/CoolProject&quot;) But you should never do this because there’s a better way; a way that also puts you on the path to managing your R work like an expert. 8.3 Paths and directories Paths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ: The most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). R can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to R, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes. Absolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you. The last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory. 8.4 RStudio projects R experts keep all the files associated with a project together — input data, R scripts, analytical results, figures. This is such a wise and common practice that RStudio has built-in support for this via projects. Let’s make a project for you to use while you’re working through the rest of this book. Click File &gt; New Project, then: Call your project r4ds and think carefully about which subdirectory you put the project in. If you don’t store it somewhere sensible, it will be hard to find it in the future! Once this process is complete, you’ll get a new RStudio project just for this book. Check that the “home” directory of your project is the current working directory: getwd() # &gt; [1] /Users/hadley/Documents/r4ds/r4ds Whenever you refer to a file with a relative path it will look for it here. Now enter the following commands in the script editor, and save the file, calling it “diamonds.R”. Next, run the complete script which will save a PDF and CSV file into your project directory. Don’t worry about the details, you’ll learn them later in the book. library(tidyverse) ggplot(diamonds, aes(carat, price)) + geom_hex() ggsave(&quot;diamonds.pdf&quot;) write_csv(diamonds, &quot;diamonds.csv&quot;) Quit RStudio. Inspect the folder associated with your project — notice the .Rproj file. Double-click that file to re-open the project. Notice you get back to where you left off: it’s the same working directory and command history, and all the files you were working on are still open. Because you followed my instructions above, you will, however, have a completely fresh environment, guaranteeing that you’re starting with a clean slate. In your favorite OS-specific way, search your computer for diamonds.pdf and you will find the PDF (no surprise) but also the script that created it (diamonds.R). This is huge win! One day you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with R code and never with the mouse or the clipboard, you will be able to reproduce old work with ease! 8.5 Summary In summary, RStudio projects give you a solid workflow that will serve you well in the future: Create an RStudio project for each data analysis project. Keep data files there; we’ll talk about loading them into R in data import. Keep scripts there; edit them, run them in bits or as a whole. Save your outputs (plots and cleaned data) there. Only ever use relative paths, not absolute paths. Everything you need is in one place, and cleanly separated from all the other projects that you are working on. "],
["parte-domar-datos.html", "(PARTE) Domar datos", " (PARTE) Domar datos "],
["introduccion-domar-datos-intro.html", "9 Introducción {#domar datos -intro}", " 9 Introducción {#domar datos -intro} En esta parte del libro, aprenderás cómo domar datos: el arte de tener tus datos en R de una forma conveniente para su visualización y modelado. La doma de datos es muy importante: ¡sin ella no puedes trabajar con tus propios datos! Existen tres partes principales para la doma de datos: Esta parte del libro continúa de la siguiente forma: En tibbles, aprenderás sobre la variante de data frame que usamos en este libro: el tibble. Conocerás qué los hace diferentes de los data frames comunes, y cómo puedes construirlos “a mano”. En [importar datos], aprenderás cómo traer tus datos del disco y dejarlos en R. Nos enfocaremos en los formatos rectangulares de texto plano, pero daremos referencias a paquetes que ayudan con otros tipos de datos. En datos ordenados, aprenderás una manera consistente de almacenar tus datos que facilita la transformación, la visualización y el modelado de los mismos. Aprenderás los principios subyacentes, y cómo poner tus datos en una forma ordenada. La doma de datos también abarca la transformación de los mismos, sobre lo cual ya has aprendido un poco. Ahora nos enfocaremos en nuevas habilidades para tres tipos de datos específicos que encontrarás frecuentemente en la práctica: Los [datos relacionales] te darán herramientas para trabajar con múltiples conjuntos de datos interrelacionados. Las [cadenas de caracteres] te introducirán en las expresiones regulares (regular expressions), las cuales son una herramienta poderosa para manipular cadenas de caracteres. Los Factores indican cómo R almacena los datos categóricos. Se usan cuando una variable tiene un conjunto fijo de posibles valores, o cuando quieres usar una cadena de caracteres en un orden distinto al alfabético. Las [Fechas y horas] te darán herramientas clave para trabajar con fechas y fecha-horas. "],
["tibbles.html", "10 Tibbles 10.1 Introducción 10.2 Creando tibbles 10.3 Tibbles vs. data.frame 10.4 Interactuando con código previo 10.5 Ejercicios", " 10 Tibbles 10.1 Introducción A lo largo de este libro, trabajaremos con “tibbles” (pronunciado /tibl/) en lugar de los tradicionales data.frames de R. Los tibbles son data frames, que modifican algunas características antiguas para hacernos la vida más fácil. R es un lenguaje viejo y algunas cosas que eran útiles hace 10 o 20 años, actualmente pueden resultar inconvenientes. Es difícil modificar R base sin alterar el código existente, así que la mayor parte de la innovación ocurre en paquetes. Aquí describiremos el paquete tibble, que provee data frames prácticos que facilitan el trabajo con el tidyverse. La mayoría de las veces voy a usar el término tibble y data frame de manera indistinta; para referirme al data frame de R lo voy a llamar data.frame. Si luego de leer este capítulo te quedas con ganas de aprender más sobre tibbles, quizás disfrutes vignette(&quot;tibble&quot;). 10.1.1 Requisitos En este capítulo exploraré el paquete tibble, parte de los paquetes principales del tidyverse. library(tidyverse) 10.2 Creando tibbles La mayoría de las funciones que usarás en este libro producen tibbles, ya que éstos son una de las características principales de tidyverse. La mayoría de los paquetes de R suelen usar data frames clásicos, así que quizás quieras convertir un data frame en un tibble. Esto lo puedes hacer con as_tibble(): as_tibble(iris) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; # … with 144 more rows Puedes crear un nuevo tibble a partir de vectores individuales con tibble(). Esta función recicla vectores de longitud 1 automáticamente y te permite usar variables creadas dentro de la propia función, como se muestra abajo. tibble( x = 1:5, y = 1, z = x^2 + y ) #&gt; # A tibble: 5 x 3 #&gt; x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2 #&gt; 2 2 1 5 #&gt; 3 3 1 10 #&gt; 4 4 1 17 #&gt; 5 5 1 26 Si ya estás familiarizado con data.frame(), es importante que tomes en cuenta que tibble() hace menos cosas: nunca cambia el tipo de los inputs (p.e. ¡nunca convierte caracteres en factores!), nunca cambia el nombre de las variables, y nunca asigna nombres a las filas. Un tibble puede usar nombres de columnas que no son nombres de variables válidos en R; es decir nombres no sintácticos. Por ejemplo, pueden empezar con un caracter diferente a una letra, o contener caracteres poco comunes, como espacios. Para referirse a estas variables, tienes que rodearlos de acentos graves, `: tb &lt;- tibble( `:)` = &quot;sonrisa&quot;, ` ` = &quot;espacio&quot;, `2000` = &quot;número&quot; ) tb #&gt; # A tibble: 1 x 3 #&gt; `:)` ` ` `2000` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 sonrisa espacio número También necesitarás los espacios abiertos al trabajar con estas variables en otros paquetes, como ggplot2, dplyr y tidyr. Otra forma de crear un tibble es con tribble(), que es una abreviación de tibble transpuesto. Esta función está pensada para realizar la entrada de datos en el código: los nombres de las columnas se definen con fórmulas (comienzan con ~), y cada entrada está separada por comas. Esto permite escribir pocos datos de manera legible. tribble( ~ x, ~ y, ~ z, #--|--|---- &quot;a&quot;, 2, 3.6, &quot;b&quot;, 1, 8.5 ) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 2 3.6 #&gt; 2 b 1 8.5 Usualmente agrego un comentario para dejar en claro cuál es el encabezado (esta línea debe empezar con #). 10.3 Tibbles vs. data.frame Existen dos diferencias principales entre el uso de un tibble y un data.frame clásico: la impresión en la consola y la selección de los subconjuntos. 10.3.1 Impresión en la consola Los tibbles tienen un método de impresión en la consola refinado: sólo muestran las primeras 10 filas, y sólo aquellas columnas que entran en el ancho de la pantalla. Esto simplifica y facilita trabajar con bases de datos grandes. Además del nombre, cada columna muestra su tipo. Esto último es una característica útil tomada de str(). tibble( a = lubridate::now() + runif(1e3) * 86400, b = lubridate::today() + runif(1e3) * 30, c = 1:1e3, d = runif(1e3), e = sample(letters, 1e3, replace = TRUE) ) #&gt; # A tibble: 1,000 x 5 #&gt; a b c d e #&gt; &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2019-01-19 20:43:09 2019-01-26 1 0.368 h #&gt; 2 2019-01-20 14:48:19 2019-01-31 2 0.612 n #&gt; 3 2019-01-20 09:11:58 2019-02-10 3 0.415 l #&gt; 4 2019-01-19 22:33:15 2019-02-09 4 0.212 x #&gt; 5 2019-01-19 18:57:32 2019-02-06 5 0.733 a #&gt; 6 2019-01-20 05:58:29 2019-02-02 6 0.460 v #&gt; # … with 994 more rows Los tibbles están diseñados para no inundar tu consola accidentalmente al mirar data frames muy grandes. Sin embargo, a veces es necesario un output mayor que el que se obtiene por default. Existen algunas opciones que pueden ayudar. Primero, puedes usar print() en el data frame y controlar el número de filas (n) y el ancho (width) mostrado. Por otro lado, width = Inf muestra todas las columnas: vuelos %&gt;% print(n = 10, width = Inf) También puedes controlar las características de impresión, modificando las opciones que estan determinadas por default. options(tibble.print_max = n, tibble.print_min = m): si hay más de n filas, mostrar solo m filas. Usa options(tibble.print_min = Inf) para mostrar siempre todas las filas. Usa options(tibble.width = Inf) para mostrar siempre todas las columnas sin importar el ancho de la pantalla. Puedes ver una lista completa de opciones en la ayuda del paquete con package?tibble. La opción final es usar el visualizador de datos de RStudio para obtener una versión interactiva del data frame completo. Esto también es útil luego de realizar una larga cadena de manipulaciones. vuelos %&gt;% View() 10.3.2 Selección de subconjuntos Hasta ahora, todas las herramientas que aprendiste funcionan con el data frame completo. Si quieres recuperar una variable individual, necesitas algunas herramientas nuevas: $ y [[. Mientras que [[ permite extraer variables usando tanto su nombre como su suposición, con $ sólo se puede extraer mediante el nombre. La única diferencia es que $ permite escribir un poco menos. df &lt;- tibble( x = runif(5), y = rnorm(5) ) # Extraer usando el nombre df$x #&gt; [1] 0.434 0.395 0.548 0.762 0.254 df[[&quot;x&quot;]] #&gt; [1] 0.434 0.395 0.548 0.762 0.254 # Extraer indicando la posición df[[1]] #&gt; [1] 0.434 0.395 0.548 0.762 0.254 Para usarlos con un pipe, necesitarás usar el marcador de posición .: df %&gt;% .$x #&gt; [1] 0.434 0.395 0.548 0.762 0.254 df %&gt;% .[[&quot;x&quot;]] #&gt; [1] 0.434 0.395 0.548 0.762 0.254 En comparación a un data.frame, los tibbles son más estrictos: nunca funcionan con coincidencias parciales, y generan una advertencia si la columna a la que intentas de acceder no existe. 10.4 Interactuando con código previo Algunas funciones previas no funcionan con tibbles. Si te encuentras uno de esos casos, usa as.data.frame() para convertir un tibble de nuevo en un data.frame: class(as.data.frame(tb)) #&gt; [1] &quot;data.frame&quot; La principal razón de que algunas funciones previas no funcionen con tibbles es la función [. En este libro no usamos mucho [ porque dplyr::filter() y dplyr::select() resuelven los mismos problemas con un código más claro (aprenderás un poco sobre ello en vector subsetting XXX). Con los data frames de R base, [ a veces devuelve un data frame y a veces devuelve un vector. Con tibbles, [ siempre devuelve otro tibble. 10.5 Ejercicios ¿Cómo puedes saber si un objeto es un tibble? (Sugerencia: imprime mtautos en consola, que es un data frame clásico). Compara y contrasta las siguientes operaciones aplicadas a un data.frame y a un tibble equivalente. ¿Qué es diferente? ¿Por qué podría causarte problemas el comportamiento por defecto del data frame? df &lt;- data.frame(abc = 1, xyz = &quot;a&quot;) df$x df[, &quot;xyz&quot;] df[, c(&quot;abc&quot;, &quot;xyz&quot;)] Si tienes el nombre de una variable guardada en un objeto, p.e., var &lt;- &quot;mpg&quot;, ¿cómo puedes extraer esta variable de un tibble? Practica referenciar nombres no sintácticos en el siguiente data frame: Extrayendo la variable llamada 1. Generando un gráfico de dispersión de 1 vs 2. Creando una nueva columna llamada 3 que sea el resultado de la división de 2 por 1. Renombrando las columnas como uno, dos y tres. molesto &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) ¿Qué hace tibble::enframe()? ¿Cuándo lo usarías? ¿Qué opción controla cuántos nombres de columnas adicionales se muestran al pie de un tibble? "],
["data-import.html", "11 Data import 11.1 Introduction 11.2 Getting started 11.3 Parsing a vector 11.4 Parsing a file 11.5 Writing to a file 11.6 Other types of data", " 11 Data import 11.1 Introduction Working with data provided by R packages is a great way to learn the tools of data science, but at some point you want to stop learning and start working with your own data. In this chapter, you’ll learn how to read plain-text rectangular files into R. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data. We’ll finish with a few pointers to packages that are useful for other types of data. 11.1.1 Prerequisites In this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse. library(tidyverse) 11.2 Getting started Most of readr’s functions are concerned with turning flat files into data frames: read_csv() reads comma delimited files, read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place), read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter. read_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space. read_log() reads Apache style log files. (But also check out webreadr which is built on top of read_log() and provides many more helpful tools.) These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand read_csv(), you can easily apply your knowledge to all the other functions in readr. The first argument to read_csv() is the most important: it’s the path to the file to read. heights &lt;- read_csv(&quot;data/heights.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; earn = col_double(), #&gt; height = col_double(), #&gt; sex = col_character(), #&gt; ed = col_double(), #&gt; age = col_double(), #&gt; race = col_character() #&gt; ) When you run read_csv() it prints out a column specification that gives the name and type of each column. That’s an important part of readr, which we’ll come back to in parsing a file. You can also supply an inline csv file. This is useful for experimenting with readr and for creating reproducible examples to share with others: read_csv(&quot;a,b,c 1,2,3 4,5,6&quot;) #&gt; # A tibble: 2 x 3 #&gt; a b c #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 In both cases read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behaviour: Sometimes there are a few lines of metadata at the top of the file. You can use skip = n to skip the first n lines; or use comment = &quot;#&quot; to drop all lines that start with (e.g.) #. read_csv(&quot;The first line of metadata The second line of metadata x,y,z 1,2,3&quot;, skip = 2) #&gt; # A tibble: 1 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 read_csv(&quot;# A comment I want to skip x,y,z 1,2,3&quot;, comment = &quot;#&quot;) #&gt; # A tibble: 1 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 The data might not have column names. You can use col_names = FALSE to tell read_csv() not to treat the first row as headings, and instead label them sequentially from X1 to Xn: read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = FALSE) #&gt; # A tibble: 2 x 3 #&gt; X1 X2 X3 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 (&quot;\\n&quot; is a convenient shortcut for adding a new line. You’ll learn more about it and other types of string escape in string basics.) Alternatively you can pass col_names a character vector which will be used as the column names: read_csv(&quot;1,2,3\\n4,5,6&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;)) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 3 #&gt; 2 4 5 6 Another option that commonly needs tweaking is na: this specifies the value (or values) that are used to represent missing values in your file: read_csv(&quot;a,b,c\\n1,2,.&quot;, na = &quot;.&quot;) #&gt; # A tibble: 1 x 3 #&gt; a b c #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 1 2 NA This is all you need to know to read ~75% of CSV files that you’ll encounter in practice. You can also easily adapt what you’ve learned to read tab separated files with read_tsv() and fixed width files with read_fwf(). To read in more challenging files, you’ll need to learn more about how readr parses each column, turning them into R vectors. 11.2.1 Compared to base R If you’ve used R before, you might wonder why we’re not using read.csv(). There are a few good reasons to favour readr functions over the base equivalents: They are typically much faster (~10x) than their base equivalents. Long running jobs have a progress bar, so you can see what’s happening. If you’re looking for raw speed, try data.table::fread(). It doesn’t fit quite so well into the tidyverse, but it can be quite a bit faster. They produce tibbles, they don’t convert character vectors to factors, use row names, or munge the column names. These are common sources of frustration with the base R functions. They are more reproducible. Base R functions inherit some behaviour from your operating system and environment variables, so import code that works on your computer might not work on someone else’s. 11.2.2 Exercises What function would you use to read a file where fields were separated with “|”? Apart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common? What are the most important arguments to read_fwf()? Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like &quot; or '. By convention, read_csv() assumes that the quoting character will be &quot;, and if you want to change it you’ll need to use read_delim() instead. What arguments do you need to specify to read the following text into a data frame? &quot;x,y\\n1,&#39;a,b&#39;&quot; Identify what is wrong with each of the following inline CSV files. What happens when you run the code? read_csv(&quot;a,b\\n1,2,3\\n4,5,6&quot;) read_csv(&quot;a,b,c\\n1,2\\n1,2,3,4&quot;) read_csv(&quot;a,b\\n\\&quot;1&quot;) read_csv(&quot;a,b\\n1,2\\na,b&quot;) read_csv(&quot;a;b\\n1;3&quot;) 11.3 Parsing a vector Before we get into the details of how readr reads files from disk, we need to take a little detour to talk about the parse_*() functions. These functions take a character vector and return a more specialised vector like a logical, integer, or date: str(parse_logical(c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;NA&quot;))) #&gt; logi [1:3] TRUE FALSE NA str(parse_integer(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) #&gt; int [1:3] 1 2 3 str(parse_date(c(&quot;2010-01-01&quot;, &quot;1979-10-14&quot;))) #&gt; Date[1:2], format: &quot;2010-01-01&quot; &quot;1979-10-14&quot; These functions are useful in their own right, but are also an important building block for readr. Once you’ve learned how the individual parsers work in this section, we’ll circle back and see how they fit together to parse a complete file in the next section. Like all functions in the tidyverse, the parse_*() functions are uniform: the first argument is a character vector to parse, and the na argument specifies which strings should be treated as missing: parse_integer(c(&quot;1&quot;, &quot;231&quot;, &quot;.&quot;, &quot;456&quot;), na = &quot;.&quot;) #&gt; [1] 1 231 NA 456 If parsing fails, you’ll get a warning: x &lt;- parse_integer(c(&quot;123&quot;, &quot;345&quot;, &quot;abc&quot;, &quot;123.45&quot;)) #&gt; Warning: 2 parsing failures. #&gt; row col expected actual #&gt; 3 -- an integer abc #&gt; 4 -- no trailing characters .45 And the failures will be missing in the output: x #&gt; [1] 123 345 NA NA #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 If there are many parsing failures, you’ll need to use problems() to get the complete set. This returns a tibble, which you can then manipulate with dplyr. problems(x) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 Using parsers is mostly a matter of understanding what’s available and how they deal with different types of input. There are eight particularly important parsers: parse_logical() and parse_integer() parse logicals and integers respectively. There’s basically nothing that can go wrong with these parsers so I won’t describe them here further. parse_double() is a strict numeric parser, and parse_number() is a flexible numeric parser. These are more complicated than you might expect because different parts of the world write numbers in different ways. parse_character() seems so simple that it shouldn’t be necessary. But one complication makes it quite important: character encodings. parse_factor() create factors, the data structure that R uses to represent categorical variables with fixed and known values. parse_datetime(), parse_date(), and parse_time() allow you to parse various date &amp; time specifications. These are the most complicated because there are so many different ways of writing dates. The following sections describe these parsers in more detail. 11.3.1 Numbers It seems like it should be straightforward to parse a number, but three problems make it tricky: People write numbers differently in different parts of the world. For example, some countries use . in between the integer and fractional parts of a real number, while others use ,. Numbers are often surrounded by other characters that provide some context, like “$1000” or “10%”. Numbers often contain “grouping” characters to make them easier to read, like “1,000,000”, and these grouping characters vary around the world. To address the first problem, readr has the notion of a “locale”, an object that specifies parsing options that differ from place to place. When parsing numbers, the most important option is the character you use for the decimal mark. You can override the default value of . by creating a new locale and setting the decimal_mark argument: parse_double(&quot;1.23&quot;) #&gt; [1] 1.23 parse_double(&quot;1,23&quot;, locale = locale(decimal_mark = &quot;,&quot;)) #&gt; [1] 1.23 readr’s default locale is US-centric, because generally R is US-centric (i.e. the documentation of base R is written in American English). An alternative approach would be to try and guess the defaults from your operating system. This is hard to do well, and, more importantly, makes your code fragile: even if it works on your computer, it might fail when you email it to a colleague in another country. parse_number() addresses the second problem: it ignores non-numeric characters before and after the number. This is particularly useful for currencies and percentages, but also works to extract numbers embedded in text. parse_number(&quot;$100&quot;) #&gt; [1] 100 parse_number(&quot;20%&quot;) #&gt; [1] 20 parse_number(&quot;It cost $123.45&quot;) #&gt; [1] 123 The final problem is addressed by the combination of parse_number() and the locale as parse_number() will ignore the “grouping mark”: # Used in America parse_number(&quot;$123,456,789&quot;) #&gt; [1] 1.23e+08 # Used in many parts of Europe parse_number(&quot;123.456.789&quot;, locale = locale(grouping_mark = &quot;.&quot;)) #&gt; [1] 1.23e+08 # Used in Switzerland parse_number(&quot;123&#39;456&#39;789&quot;, locale = locale(grouping_mark = &quot;&#39;&quot;)) #&gt; [1] 1.23e+08 11.3.2 Strings It seems like parse_character() should be really simple — it could just return its input. Unfortunately life isn’t so simple, as there are multiple ways to represent the same string. To understand what’s going on, we need to dive into the details of how computers represent strings. In R, we can get at the underlying representation of a string using charToRaw(): charToRaw(&quot;Hadley&quot;) #&gt; [1] 48 61 64 6c 65 79 Each hexadecimal number represents a byte of information: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case the encoding is called ASCII. ASCII does a great job of representing English characters, because it’s the American Standard Code for Information Interchange. Things get more complicated for languages other than English. In the early days of computing there were many competing standards for encoding non-English characters, and to correctly interpret a string you needed to know both the values and the encoding. For example, two common encodings are Latin1 (aka ISO-8859-1, used for Western European languages) and Latin2 (aka ISO-8859-2, used for Eastern European languages). In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today, as well as many extra symbols (like emoji!). readr uses UTF-8 everywhere: it assumes your data is UTF-8 encoded when you read it, and always uses it when writing. This is a good default, but will fail for data produced by older systems that don’t understand UTF-8. If this happens to you, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times you’ll get complete gibberish. For example: x1 &lt;- &quot;El Ni\\xf1o was particularly bad this year&quot; x2 &lt;- &quot;\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd&quot; x1 #&gt; [1] &quot;El Ni\\xf1o was particularly bad this year&quot; x2 #&gt; [1] &quot;\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd&quot; To fix the problem you need to specify the encoding in parse_character(): parse_character(x1, locale = locale(encoding = &quot;Latin1&quot;)) #&gt; [1] &quot;El Niño was particularly bad this year&quot; parse_character(x2, locale = locale(encoding = &quot;Shift-JIS&quot;)) #&gt; [1] &quot;こんにちは&quot; How do you find the correct encoding? If you’re lucky, it’ll be included somewhere in the data documentation. Unfortunately, that’s rarely the case, so readr provides guess_encoding() to help you figure it out. It’s not foolproof, and it works better when you have lots of text (unlike here), but it’s a reasonable place to start. Expect to try a few different encodings before you find the right one. guess_encoding(charToRaw(x1)) #&gt; # A tibble: 2 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ISO-8859-1 0.46 #&gt; 2 ISO-8859-9 0.23 guess_encoding(charToRaw(x2)) #&gt; # A tibble: 1 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 KOI8-R 0.42 The first argument to guess_encoding() can either be a path to a file, or, as in this case, a raw vector (useful if the strings are already in R). Encodings are a rich and complex topic, and I’ve only scratched the surface here. If you’d like to learn more I’d recommend reading the detailed explanation at http://kunststube.net/encoding/. 11.3.3 Factors R uses factors to represent categorical variables that have a known set of possible values. Give parse_factor() a vector of known levels to generate a warning whenever an unexpected value is present: fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;) parse_factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;bananana&quot;), levels = fruit) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 3 -- value in level set bananana #&gt; [1] apple banana &lt;NA&gt; #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 1 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA value in level set bananana #&gt; Levels: apple banana But if you have many problematic entries, it’s often easier to leave as character vectors and then use the tools you’ll learn about in strings and factors to clean them up. 11.3.4 Dates, date-times, and times You pick between three parsers depending on whether you want a date (the number of days since 1970-01-01), a date-time (the number of seconds since midnight 1970-01-01), or a time (the number of seconds since midnight). When called without any additional arguments: parse_datetime() expects an ISO8601 date-time. ISO8601 is an international standard in which the components of a date are organised from biggest to smallest: year, month, day, hour, minute, second. parse_datetime(&quot;2010-10-01T2010&quot;) #&gt; [1] &quot;2010-10-01 20:10:00 UTC&quot; # If time is omitted, it will be set to midnight parse_datetime(&quot;20101010&quot;) #&gt; [1] &quot;2010-10-10 UTC&quot; This is the most important date/time standard, and if you work with dates and times frequently, I recommend reading https://en.wikipedia.org/wiki/ISO_8601 parse_date() expects a four digit year, a - or /, the month, a - or /, then the day: parse_date(&quot;2010-10-01&quot;) #&gt; [1] &quot;2010-10-01&quot; parse_time() expects the hour, :, minutes, optionally : and seconds, and an optional am/pm specifier: library(hms) parse_time(&quot;01:10 am&quot;) #&gt; 01:10:00 parse_time(&quot;20:10:01&quot;) #&gt; 20:10:01 Base R doesn’t have a great built in class for time data, so we use the one provided in the hms package. If these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces: Year %Y (4 digits). %y (2 digits); 00-69 -&gt; 2000-2069, 70-99 -&gt; 1970-1999. Month %m (2 digits). %b (abbreviated name, like “Jan”). %B (full name, “January”). Day %d (2 digits). %e (optional leading space). Time %H 0-23 hour. %I 0-12, must be used with %p. %p AM/PM indicator. %M minutes. %S integer seconds. %OS real seconds. %Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this [time zones]. %z (as offset from UTC, e.g. +0800). Non-digits %. skips one non-digit character. %* skips any number of non-digits. The best way to figure out the correct format is to create a few examples in a character vector, and test with one of the parsing functions. For example: parse_date(&quot;01/02/15&quot;, &quot;%m/%d/%y&quot;) #&gt; [1] &quot;2015-01-02&quot; parse_date(&quot;01/02/15&quot;, &quot;%d/%m/%y&quot;) #&gt; [1] &quot;2015-02-01&quot; parse_date(&quot;01/02/15&quot;, &quot;%y/%m/%d&quot;) #&gt; [1] &quot;2001-02-15&quot; If you’re using %b or %B with non-English month names, you’ll need to set the lang argument to locale(). See the list of built-in languages in date_names_langs(), or if your language is not already included, create your own with date_names(). parse_date(&quot;1 janvier 2015&quot;, &quot;%d %B %Y&quot;, locale = locale(&quot;fr&quot;)) #&gt; [1] &quot;2015-01-01&quot; 11.3.5 Exercises What are the most important arguments to locale()? What happens if you try and set decimal_mark and grouping_mark to the same character? What happens to the default value of grouping_mark when you set decimal_mark to “,”? What happens to the default value of decimal_mark when you set the grouping_mark to “.”? I didn’t discuss the date_format and time_format options to locale(). What do they do? Construct an example that shows when they might be useful. If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly. What’s the difference between read_csv() and read_csv2()? What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. Generate the correct format string to parse each of the following dates and times: d1 &lt;- &quot;January 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 t1 &lt;- &quot;1705&quot; t2 &lt;- &quot;11:15:10.12 PM&quot; 11.4 Parsing a file Now that you’ve learned how to parse an individual vector, it’s time to return to the beginning and explore how readr parses a file. There are two new things that you’ll learn about in this section: How readr automatically guesses the type of each column. How to override the default specification. 11.4.1 Strategy readr uses a heuristic to figure out the type of each column: it reads the first 1000 rows and uses some (moderately conservative) heuristics to figure out the type of each column. You can emulate this process with a character vector using guess_parser(), which returns readr’s best guess, and parse_guess() which uses that guess to parse the column: guess_parser(&quot;2010-10-01&quot;) #&gt; [1] &quot;date&quot; guess_parser(&quot;15:01&quot;) #&gt; [1] &quot;time&quot; guess_parser(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) #&gt; [1] &quot;logical&quot; guess_parser(c(&quot;1&quot;, &quot;5&quot;, &quot;9&quot;)) #&gt; [1] &quot;double&quot; guess_parser(c(&quot;12,352,561&quot;)) #&gt; [1] &quot;number&quot; str(parse_guess(&quot;2010-10-10&quot;)) #&gt; Date[1:1], format: &quot;2010-10-10&quot; The heuristic tries each of the following types, stopping when it finds a match: logical: contains only “F”, “T”, “FALSE”, or “TRUE”. integer: contains only numeric characters (and -). double: contains only valid doubles (including numbers like 4.5e-5). number: contains valid doubles with the grouping mark inside. time: matches the default time_format. date: matches the default date_format. date-time: any ISO8601 date. If none of these rules apply, then the column will stay as a vector of strings. 11.4.2 Problems These defaults don’t always work for larger files. There are two basic problems: The first thousand rows might be a special case, and readr guesses a type that is not sufficiently general. For example, you might have a column of doubles that only contains integers in the first 1000 rows. The column might contain a lot of missing values. If the first 1000 rows contain only NAs, readr will guess that it’s a character vector, whereas you probably want to parse it as something more specific. readr contains a challenging CSV that illustrates both of these problems: challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) #&gt; Warning: 1000 parsing failures. #&gt; row col expected actual file #&gt; 1001 y 1/0/T/F/TRUE/FALSE 2015-01-16 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1002 y 1/0/T/F/TRUE/FALSE 2018-05-18 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1003 y 1/0/T/F/TRUE/FALSE 2015-09-05 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1004 y 1/0/T/F/TRUE/FALSE 2012-11-28 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; 1005 y 1/0/T/F/TRUE/FALSE 2020-01-13 &#39;/home/travis/R/Library/readr/extdata/challenge.csv&#39; #&gt; .... ... .................. .......... .................................................... #&gt; See problems(...) for more details. (Note the use of readr_example() which finds the path to one of the files included with the package) There are two printed outputs: the column specification generated by looking at the first 1000 rows, and the first five parsing failures. It’s always a good idea to explicitly pull out the problems(), so you can explore them in more depth: problems(challenge) #&gt; # A tibble: 1,000 x 5 #&gt; row col expected actual file #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1001 y 1/0/T/F/TRUE/F… 2015-01-… &#39;/home/travis/R/Library/readr/extd… #&gt; 2 1002 y 1/0/T/F/TRUE/F… 2018-05-… &#39;/home/travis/R/Library/readr/extd… #&gt; 3 1003 y 1/0/T/F/TRUE/F… 2015-09-… &#39;/home/travis/R/Library/readr/extd… #&gt; 4 1004 y 1/0/T/F/TRUE/F… 2012-11-… &#39;/home/travis/R/Library/readr/extd… #&gt; 5 1005 y 1/0/T/F/TRUE/F… 2020-01-… &#39;/home/travis/R/Library/readr/extd… #&gt; 6 1006 y 1/0/T/F/TRUE/F… 2016-04-… &#39;/home/travis/R/Library/readr/extd… #&gt; # … with 994 more rows A good strategy is to work column by column until there are no problems remaining. Here we can see that there are a lot of parsing problems with the x column - there are trailing characters after the integer value. That suggests we need to use a double parser instead. To fix the call, start by copying and pasting the column specification into your original call: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_integer(), y = col_character() ) ) Then you can tweak the type of the x column: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_character() ) ) That fixes the first problem, but if we look at the last few rows, you’ll see that they’re dates stored in a character vector: tail(challenge) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.805 2019-11-21 #&gt; 2 0.164 2018-03-29 #&gt; 3 0.472 2014-08-04 #&gt; 4 0.718 2015-08-16 #&gt; 5 0.270 2020-02-04 #&gt; 6 0.608 2019-01-06 You can fix that by specifying that y is a date column: challenge &lt;- read_csv( readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_date() ) ) tail(challenge) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 0.805 2019-11-21 #&gt; 2 0.164 2018-03-29 #&gt; 3 0.472 2014-08-04 #&gt; 4 0.718 2015-08-16 #&gt; 5 0.270 2020-02-04 #&gt; 6 0.608 2019-01-06 Every parse_xyz() function has a corresponding col_xyz() function. You use parse_xyz() when the data is in a character vector in R already; you use col_xyz() when you want to tell readr how to load the data. I highly recommend always supplying col_types, building up from the print-out provided by readr. This ensures that you have a consistent and reproducible data import script. If you rely on the default guesses and your data changes, readr will continue to read it in. If you want to be really strict, use stop_for_problems(): that will throw an error and stop your script if there are any parsing problems. 11.4.3 Other strategies There are a few other general strategies to help you parse files: In the previous example, we just got unlucky: if we look at just one more row than the default, we can correctly parse in one shot: challenge2 &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), guess_max = 1001) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_date(format = &quot;&quot;) #&gt; ) challenge2 #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors: challenge2 &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types = cols(.default = col_character()) ) This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame. df &lt;- tribble( ~ x, ~ y, &quot;1&quot;, &quot;1.21&quot;, &quot;2&quot;, &quot;2.32&quot;, &quot;3&quot;, &quot;4.56&quot; ) df #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 1.21 #&gt; 2 2 2.32 #&gt; 3 3 4.56 # Note the column types type_convert(df) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_double() #&gt; ) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.21 #&gt; 2 2 2.32 #&gt; 3 3 4.56 If you’re reading a very large file, you might want to set n_max to a smallish number like 10,000 or 100,000. That will accelerate your iterations while you eliminate common problems. If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats. 11.5 Writing to a file readr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). Both functions increase the chances of the output file being read back in correctly by: Always encoding strings in UTF-8. Saving dates and date-times in ISO8601 format so they are easily parsed elsewhere. If you want to export a csv file to Excel, use write_excel_csv() — this writes a special character (a “byte order mark”) at the start of the file which tells Excel that you’re using the UTF-8 encoding. The most important arguments are x (the data frame to save), and path (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file. write_csv(challenge, &quot;challenge.csv&quot;) Note that the type information is lost when you save to csv: challenge #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows write_csv(challenge, &quot;challenge-2.csv&quot;) read_csv(&quot;challenge-2.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows This makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two alternatives: write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS: write_rds(challenge, &quot;challenge.rds&quot;) read_rds(&quot;challenge.rds&quot;) #&gt; # A tibble: 2,000 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 404 NA #&gt; 2 4172 NA #&gt; 3 3004 NA #&gt; 4 787 NA #&gt; 5 37 NA #&gt; 6 2332 NA #&gt; # … with 1,994 more rows The feather package implements a fast binary file format that can be shared across programming languages: library(feather) write_feather(challenge, &quot;challenge.feather&quot;) read_feather(&quot;challenge.feather&quot;) # &gt; # A tibble: 2,000 x 2 # &gt; x y # &gt; &lt;dbl&gt; &lt;date&gt; # &gt; 1 404 &lt;NA&gt; # &gt; 2 4172 &lt;NA&gt; # &gt; 3 3004 &lt;NA&gt; # &gt; 4 787 &lt;NA&gt; # &gt; 5 37 &lt;NA&gt; # &gt; 6 2332 &lt;NA&gt; # &gt; # ... with 1,994 more rows Feather tends to be faster than RDS and is usable outside of R. RDS supports list-columns (which you’ll learn about in [many models]); feather currently does not. 11.6 Other types of data To get other types of data into R, we recommend starting with the tidyverse packages listed below. They’re certainly not perfect, but they are a good place to start. For rectangular data: haven reads SPSS, Stata, and SAS files. readxl reads excel files (both .xls and .xlsx). DBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame. For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. Jenny Bryan has some excellent worked examples at https://jennybc.github.io/purrr-tutorial/. For other file types, try the R data import/export manual and the rio package. "],
["datos-ordenados.html", "12 Datos ordenados 12.1 Introducción 12.2 Datos ordenados 12.3 Gather y Spread 12.4 Separar y unir 12.5 Missing values 12.6 Case Study 12.7 Non-tidy data", " 12 Datos ordenados 12.1 Introducción “Todas las familias felices se parecen unas a otras, pero cada familia infeliz lo es a su manera.” –– Leo Tolstoy “Todos los datos ordenados se parecen unos a otros, pero cada dato desordenado lo es a su manera” — Hadley Wickham En este capítulo aprenderás una metodología consistente para organizar datos en R, a esta metodología le llamaremos tidy data (datos ordenados). Llevar tus datos a este formato requiere algo de trabajo previo, sin embargo dicho trabajo tiene retorno positivo en el largo plazo. Una vez que tengas tus datos ordenados y las herramientas para ordenar datos que provee el tidyverse, vas a gastar mucho menos tiempo pasando de una forma de representar datos a otra, permietiéndote destinar más tiempo a las preguntas analíticas. Este capítulo te dará una introducción práctica a tidy data y las herramientas que provee el paquete tidyr. Si desear saber más acerca de la teoría subyacente, puede que te guste el artículo Tidy Data publicado en la revista Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 12.1.1 Prerequisitos En este capítulo nos enfocaremos en tidyr, un paquete que provee un conjunto de herramientas que te ayudarán a ordenar datos desordenados. tidyr es parte del núcleo del tidyverse. library(tidyverse) library(datos) 12.2 Datos ordenados Puedes representar la misma información de múltiples formas. El ejemplo a continuación muestra los mismos datos ordenados de cuatro manera distintas. Cada dataset muestra los mismos valores de cuatro variables pais, anio, poblacion y casos, pero cada dataset organiza los valores de forma distinta. tabla1 #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 tabla2 #&gt; # A tibble: 12 x 4 #&gt; pais anio tipo cuenta #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 casos 745 #&gt; 2 Afganistán 1999 población 19987071 #&gt; 3 Afganistán 2000 casos 2666 #&gt; 4 Afganistán 2000 población 20595360 #&gt; 5 Brasil 1999 casos 37737 #&gt; 6 Brasil 1999 población 172006362 #&gt; # … with 6 more rows tabla3 #&gt; # A tibble: 6 x 3 #&gt; pais anio tasa #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 # Spread across two tibbles tabla4a # casos #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 745 2666 #&gt; 2 Brasil 37737 80488 #&gt; 3 China 212258 213766 tabla4b # poblacion #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 19987071 20595360 #&gt; 2 Brasil 172006362 174504898 #&gt; 3 China 1272915272 1280428583 Todo lo anterior representa los mismos datos subyacentes, pero no es igualmente fácil de usar. Un dataset, el dataset ordenado, es mucho más fácil de trabajar en el tidyverse. Existen tres reglas interrelacionadas que hacen que un dataset sea ordenado: Cada variable tiene su propia columna. Cada observación tiene su propia fila. Cada valor tiene su propia celda. La figura 12.1 muestra estas reglas visualmente. Figure 12.1: Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. Estas reglas están interrelacionadas ya que es imposible cumplir dos de las tres. Esta interrelación lleva a un conjunto práctico de instrucciones mucho más simple: Coloca cada dataset en un tibble. Coloca cada variable en una columna. En este ejemplo, solo tabla1 está ordenado. Es la única representación en que cada columna es una variable. ¿Por qué asegurarse de que los datos están ordenados? Existen dos principales ventajas: Existe una ventaja general de elegir una forma consistente de almacenar datos. Si tienes una estructura de datos consistente, es más fácil aprender las herramientas que sirven con aquello ya que presenta una uniformidad subyacente. Existe una ventaja específica al situar las variables en las columnas ya que permite que la naturaleza vectorizada de R brille. Como habrás aprendido en mutate y summary functions, muchas de las funciones que vienen con R trabajan con vectores de valores. Esto hace que transformar datos ordenados sea casi natural. dplyr, ggplot2 y el resto de los paquetes del tidyverse están diseñados para trabajar con datos ordenados. Aquí hay algunos ejemplos de cómo se podría trabajar con tabla1. # Calcular tasa por cada 10,000 habitantes tabla1 %&gt;% mutate(tasa = casos / poblacion * 10000) #&gt; # A tibble: 6 x 5 #&gt; pais anio casos poblacion tasa #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afganistán 1999 745 19987071 0.373 #&gt; 2 Afganistán 2000 2666 20595360 1.29 #&gt; 3 Brasil 1999 37737 172006362 2.19 #&gt; 4 Brasil 2000 80488 174504898 4.61 #&gt; 5 China 1999 212258 1272915272 1.67 #&gt; 6 China 2000 213766 1280428583 1.67 # Compute casos per anio tabla1 %&gt;% count(anio, wt = casos) #&gt; # A tibble: 2 x 2 #&gt; anio n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1999 250740 #&gt; 2 2000 296920 # Visualizar cambios en el tiempo library(ggplot2) ggplot(tabla1, aes(anio, casos)) + geom_line(aes(group = pais), colour = &quot;grey50&quot;) + geom_point(aes(colour = pais)) 12.2.1 Ejercicios Usando prosa, describe como las variables y observaciones se organizan en las tablas de ejemplo. Calcula la tasa en las tablas tabla2 y tabla4a + tabla4b. Necesitarás las siguientes operaciones: Extrae el número de casos de tuberculosis por país y año. Extrae la población por país y año. Divide los casos por la población y multiplica por 10000. Inserta los datos en el lugar adecuado. ¿Cuál representación es más fácil de trabajar? ¿Cuál es la más difícil? ¿Por qué? Recrea el gráfico que muestra el cambio en el número de casos usando la tabla2 en lugar de la tabla1. ¿Qué debes hacer en primera lugar? 12.3 Gather y Spread Los principios de tidy data parecen tan obvios que te preguntarás si acaso vas a encontrar un dataset que no está ordenado. Desafortunadamente, sin embargo, gran parte de los datos que vas a encontrar están desordenados. Existen dos principales razones para esto: La mayoría de las personas no están familirizadas con los principios de datos ordenados y es difícil derivarlos por cuenta propia a menos que pases mucho tiempo trabajando con datos. Los datos a menudo están organizados para facilitar tareas distintas del análisis. Por ejemplo, los datos se organizan para que su registro sea lo más sencillo posible. Esto significa que para la mayoría de los análisis, necesitarás ordenar los datos. El primer paso siempre es entender el significado de las variables y observaciones. Esto a veces es fácil, otras veces deberás consultar con quienes crearon el dataset. El segundo paso es resolver uno de los siguientes problemas frecuentes: Una variable se esparce entre varias columnas Una observación se esparce entre múltiples filas. Típicamente un dataset tiene uno de los problemas, ¡si contiene ambos significa que tienes muy mala suerte! Para solucionar estos problemas necesitarás las dos funciones más importantes de tidyr: gather() (reunir) y spread() (esparcir). 12.3.1 Gather Un problema común se tiene cuando en un dataset los nombres de las columnas no representan nombres de variables, sino que representan los valores de una variable. Tomando el caso de la tabla4a: los nombres de las columnas 1999 y 2000 representan los valores de la variable anio y cada fila representa dos observaciones en lugar de una. tabla4a #&gt; # A tibble: 3 x 3 #&gt; pais `1999` `2000` #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 745 2666 #&gt; 2 Brasil 37737 80488 #&gt; 3 China 212258 213766 Para ordenar un dataset como este necesitamos contraer (gather) tales columnas en un nuevo par de variables. Para describir dicha operación necesitamos tres parámetros: El conjunto de columnas que representan valores y no variables. En este ejemplo son las columnas 1999 y 2000. El nombre de la variable cuyos valores forman los nombres de las columnas. Llamaremos a esto key (llave) y en este caso corresponde a anio. El nombre de la variable cuyos valores se esparcen por las celdas. Llamaremos a esto value (valor) y en este caso corresponde al número de casos. Juntando estos parámetros se puede realizar una llamada a gather(): tabla4a %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;casos&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais anio casos #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 #&gt; 2 Brasil 1999 37737 #&gt; 3 China 1999 212258 #&gt; 4 Afganistán 2000 2666 #&gt; 5 Brasil 2000 80488 #&gt; 6 China 2000 213766 Las columnas a contraer quedan seleccionadas siguiendo el estilo de notación de dplyr::select(). En este caso hay dos columnas, por lo que las listamos individualmente. Nota que “1999” y “2000” son nombres no-sintáxicos (debido a que no comienzan con una letra) por lo que los escribimos con backtick. Para refrescar tu memoria respecto de la selección de columnas, consulta select. Figure 12.2: Gathering tabla4 into a tidy form. En el resultado final, las columnas reunidas se eliminan y obtenemos la nuevas variables key y value. De otro modo, la relacién entre las variables originales se mantiene. Visualmente, esto se observa en la Figura 12.2. Podemos usar gather() para ordenar tabla4b de modo similar. La única diferencia es la variable almacenada en los valores de las celdas: tabla4b %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;poblacion&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais anio poblacion #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 19987071 #&gt; 2 Brasil 1999 172006362 #&gt; 3 China 1999 1272915272 #&gt; 4 Afganistán 2000 20595360 #&gt; 5 Brasil 2000 174504898 #&gt; 6 China 2000 1280428583 Para combinar las versiones ordenadas de tabla4a y tabla4b en un único tibble, necesitamos usar dplyr::left_join(), función que aprenderás en [datos relacionales]. tidy4a &lt;- tabla4a %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;casos&quot;) tidy4b &lt;- tabla4b %&gt;% gather(`1999`, `2000`, key = &quot;anio&quot;, value = &quot;poblacion&quot;) left_join(tidy4a, tidy4b) #&gt; Joining, by = c(&quot;pais&quot;, &quot;anio&quot;) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Brasil 1999 37737 172006362 #&gt; 3 China 1999 212258 1272915272 #&gt; 4 Afganistán 2000 2666 20595360 #&gt; 5 Brasil 2000 80488 174504898 #&gt; 6 China 2000 213766 1280428583 12.3.2 Spread Extender (spread) es lo opuesto de gather. Lo usas cuando una observación aparece en múltiples filas. Por ejemplo, toma la tabla tabla2: una observación You use it when an observation is scattered across multiple rows. For example, take tabla2: una observación es un país en un año, pero cada observación se reparte entre dos filas. tabla2 #&gt; # A tibble: 12 x 4 #&gt; pais anio tipo cuenta #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 casos 745 #&gt; 2 Afganistán 1999 población 19987071 #&gt; 3 Afganistán 2000 casos 2666 #&gt; 4 Afganistán 2000 población 20595360 #&gt; 5 Brasil 1999 casos 37737 #&gt; 6 Brasil 1999 población 172006362 #&gt; # … with 6 more rows Para ordenar esto, primero analiza la representación de un modo similar a cómo se haría con gather(). Esta vez, sin embargo, necesitamos únicamente dos parámetros: La columna que contiene los nombres de las variables, la columna key. En este caso corresponde a tipo. La columna que contiene valores de múltiples variables, la columna value. En este caso corresponde a cuenta. Una vez resuelto esto, podemos usar spread(), como se muestra programáticamente abajo y visualmente en la Figura 12.3. tabla2 %&gt;% spread(key = tipo, value = cuenta) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos población #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 Figure 12.3: Spreading tabla2 makes it tidy Como te habrás dado cuenta a partir de los argumentos comunes key y value, gather() y spread() son complementarios. gather() genera tablas estrechas y largas, spread() genera tablas anchas y cortas. 12.3.3 Ejercicios ¿Por qué gather() y spread() no son perfectamente simétricas? Observa cuidadosamente el siguiente ejemplo: stocks &lt;- tibble( anio = c(2015, 2015, 2016, 2016), semestre = c(1, 2, 1, 2), retorno = c(1.88, 0.59, 0.92, 0.17) ) stocks %&gt;% spread(anio, retorno) %&gt;% gather(&quot;anio&quot;, &quot;retorno&quot;, `2015`:`2016`) (Hint: observa los tipos de variables y piensa en los nombres de las columnas) Tanto `spread()` como `gather()` tienen el argumento `convert` (convertir). ¿Qué hace dich argumento? ¿Por qué falla el siguiente código? tabla4a %&gt;% gather(1999, 2000, key = &quot;anio&quot;, value = &quot;casos&quot;) #&gt; Error in inds_combine(.vars, ind_list): Position must be between 0 and n ¿Por qué no se puede extender la siguiente tabla? ¿Cómo agregarias una nueva columna para resolver el problema? personas &lt;- tribble( ~ nombre, ~ llave, ~ valor, #-----------------|--------|------ &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 ) Ordena la siguiente tabla. ¿Necesitas extenderla o contraerla? ¿Cuáles son las variables? embarazo &lt;- tribble( ~ embarazo, ~ hombre, ~ mujer, &quot;yes&quot;, NA, 10, &quot;no&quot;, 20, 12 ) 12.4 Separar y unir Hasta ahora has aprendido a ordenar las tablas tabla2 y tabla4, pero no la tabla3 que tiene un problema diferente: contiene una columna (tasa) la cual contiene dos variables (casos y poblacion). Para solucionar este problema, necesitamos la función separate() (separar), También aprenderás acerca del complemento de separate(): unite() (unir), la cual se usa en los casos en que una única variable se reparte en varias columnas. 12.4.1 Separar separate()divide una columna en varias columnas, dividiendo de acuerdo a la posición de un caracter separador. Tomando la tabla3: tabla3 #&gt; # A tibble: 6 x 3 #&gt; pais anio tasa #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 La columna tasa contiene las variables casos y poblacion,necesitamos dividir esto en dos variables. separate() toma el nombre de la columna a separar y el nombre de las columnas a donde irá el resultado, tal como se muestra en la Figura 12.4 y el código a continuación. tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;)) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 Figure 12.4: Separating tabla3 makes it tidy Por defecto, separate() dividirá una columna donde aparezca un caracter no no alfanumérico (i.e. un caracter que no es un número o letra). Por ejemplo, en el siguiente código, separate() divide los valores de tasa donde aparece el caracter slash. Si deseas usar un caracter específico para separar una columna, puedes especificarlo en el argumento sep de separate(). Por ejemplo, lo anterior se puede re-escribir del siguiente modo: tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;), sep = &quot;/&quot;) (Formalmente, sep es una expresión regular y aprenderás más sobre esto en strings.) Look carefully at the column tipos: you’ll notice that casos and poblacion are character columns. This is the default behaviour in separate(): it leaves the tipo of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better tipos using convert = TRUE: tabla3 %&gt;% separate(tasa, into = c(&quot;casos&quot;, &quot;poblacion&quot;), convert = TRUE) #&gt; # A tibble: 6 x 4 #&gt; pais anio casos poblacion #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afganistán 1999 745 19987071 #&gt; 2 Afganistán 2000 2666 20595360 #&gt; 3 Brasil 1999 37737 172006362 #&gt; 4 Brasil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 You can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into. You can use this arrangement to separate the last two digits of each anio. This make this data less tidy, but is useful in other casos, as you’ll see in a little bit. tabla3 %&gt;% separate(anio, into = c(&quot;siglo&quot;, &quot;anio&quot;), sep = 2) #&gt; # A tibble: 6 x 4 #&gt; pais siglo anio tasa #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; #&gt; 1 Afganistán 19 99 745/19987071 #&gt; 2 Afganistán 20 00 2666/20595360 #&gt; 3 Brasil 19 99 37737/172006362 #&gt; 4 Brasil 20 00 80488/174504898 #&gt; 5 China 19 99 212258/1272915272 #&gt; 6 China 20 00 213766/1280428583 12.4.2 Unir unite() es el inverso de separate(): combina múltiples columnas en una única columna. Necesitarás esta función con mucha menos frecuencia que separate(), pero aún así es una buena herramienta a tener en el bolsillo trasero. Figure 12.5: Uniting tabla5 makes it tidy Podemos usar unite() para unir las columnas siglo y anio creadas en el ejemplo anterior. Los datos están guardados en tidyr::tabla5. unite() toma un data frame, el nombre de la nueva variable a crear, y un conjunto de columnas a combinar, las que se especifican siguiendo el estilo de la función dplyr::select(): tabla5 %&gt;% unite(new, siglo, anio) #&gt; # A tibble: 6 x 3 #&gt; pais new tasa #&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; #&gt; 1 Afganistán 19_99 745/19987071 #&gt; 2 Afganistán 20_00 2666/20595360 #&gt; 3 Brasil 19_99 37737/172006362 #&gt; 4 Brasil 20_00 80488/174504898 #&gt; 5 China 19_99 212258/1272915272 #&gt; 6 China 20_00 213766/1280428583 En este caso también necesitamos el arguento sep. El separador por defecto es el guión bajo (_) entre los valores de las distintas columnas. Si no queremos una separación usamos &quot;&quot;: tabla5 %&gt;% unite(new, siglo, anio, sep = &quot;&quot;) #&gt; # A tibble: 6 x 3 #&gt; pais new tasa #&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; #&gt; 1 Afganistán 1999 745/19987071 #&gt; 2 Afganistán 2000 2666/20595360 #&gt; 3 Brasil 1999 37737/172006362 #&gt; 4 Brasil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 12.4.3 Exercises What do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets. tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE? Compare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite? 12.5 Missing values Changing the representation of a dataset brings up an important subtlety of missing values. Surprisingly, a value can be missing in one of two possible ways: Explicitly, i.e. flagged with NA. Implicitly, i.e. simply not present in the data. Let’s illustrate this idea with a very simple data set: stocks &lt;- tibble( anio = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c(1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) There are two missing values in this dataset: The return for the fourth quarter of 2015 is explicitly missing, because the cell where its value should be instead contains NA. The return for the first quarter of 2016 is implicitly missing, because it simply does not appear in the dataset. One way to think about the difference is with this Zen-like koan: An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. The way that a dataset is represented can make implicit values explicit. For example, we can make the implicit missing value explicit by putting anios in the columns: stocks %&gt;% spread(anio, return) #&gt; # A tibble: 4 x 3 #&gt; qtr `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 NA #&gt; 2 2 0.59 0.92 #&gt; 3 3 0.35 0.17 #&gt; 4 4 NA 2.66 Because these explicit missing values may not be important in other representations of the data, you can set na.rm = TRUE in gather() to turn explicit missing values implicit: stocks %&gt;% spread(anio, return) %&gt;% gather(anio, return, `2015`:`2016`, na.rm = TRUE) #&gt; # A tibble: 6 x 3 #&gt; qtr anio return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 2 2015 0.59 #&gt; 3 3 2015 0.35 #&gt; 4 2 2016 0.92 #&gt; 5 3 2016 0.17 #&gt; 6 4 2016 2.66 Another important tool for making missing values explicit in tidy data is complete(): stocks %&gt;% complete(anio, qtr) #&gt; # A tibble: 8 x 3 #&gt; anio qtr return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015 1 1.88 #&gt; 2 2015 2 0.59 #&gt; 3 2015 3 0.35 #&gt; 4 2015 4 NA #&gt; 5 2016 1 NA #&gt; 6 2016 2 0.92 #&gt; # … with 2 more rows complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary. There’s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward: treatment &lt;- tribble( ~ person, ~ treatment, ~ response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward). treatment %&gt;% fill(person) #&gt; # A tibble: 4 x 3 #&gt; person treatment response #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Derrick Whitmore 1 7 #&gt; 2 Derrick Whitmore 2 10 #&gt; 3 Derrick Whitmore 3 9 #&gt; 4 Katherine Burke 1 4 12.5.1 Exercises Compare and contrast the fill arguments to spread() and complete(). What does the direction argument to fill() do? 12.6 Case Study To finish off the chapter, let’s pull together everything you’ve learned to tackle a realistic data tidying problem. The tidyr::who dataset contains tuberculosis (TB) casos broken down by anio, pais, age, gender, and diagnosis method. The data comes from the 2014 World Health Organization Global Tuberculosis Report, available at http://www.who.int/tb/pais/data/download/en/. There’s a wealth of epidemiological information in this dataset, but it’s challenging to work with the data in the form that it’s provided: who #&gt; # A tibble: 7,240 x 60 #&gt; country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghan… AF AFG 1980 NA NA NA #&gt; 2 Afghan… AF AFG 1981 NA NA NA #&gt; 3 Afghan… AF AFG 1982 NA NA NA #&gt; 4 Afghan… AF AFG 1983 NA NA NA #&gt; 5 Afghan… AF AFG 1984 NA NA NA #&gt; 6 Afghan… AF AFG 1985 NA NA NA #&gt; # … with 7,234 more rows, and 53 more variables: new_sp_m3544 &lt;int&gt;, #&gt; # new_sp_m4554 &lt;int&gt;, new_sp_m5564 &lt;int&gt;, new_sp_m65 &lt;int&gt;, #&gt; # new_sp_f014 &lt;int&gt;, new_sp_f1524 &lt;int&gt;, new_sp_f2534 &lt;int&gt;, #&gt; # new_sp_f3544 &lt;int&gt;, new_sp_f4554 &lt;int&gt;, new_sp_f5564 &lt;int&gt;, #&gt; # new_sp_f65 &lt;int&gt;, new_sn_m014 &lt;int&gt;, new_sn_m1524 &lt;int&gt;, #&gt; # new_sn_m2534 &lt;int&gt;, new_sn_m3544 &lt;int&gt;, new_sn_m4554 &lt;int&gt;, #&gt; # new_sn_m5564 &lt;int&gt;, new_sn_m65 &lt;int&gt;, new_sn_f014 &lt;int&gt;, #&gt; # new_sn_f1524 &lt;int&gt;, new_sn_f2534 &lt;int&gt;, new_sn_f3544 &lt;int&gt;, #&gt; # new_sn_f4554 &lt;int&gt;, new_sn_f5564 &lt;int&gt;, new_sn_f65 &lt;int&gt;, #&gt; # new_ep_m014 &lt;int&gt;, new_ep_m1524 &lt;int&gt;, new_ep_m2534 &lt;int&gt;, #&gt; # new_ep_m3544 &lt;int&gt;, new_ep_m4554 &lt;int&gt;, new_ep_m5564 &lt;int&gt;, #&gt; # new_ep_m65 &lt;int&gt;, new_ep_f014 &lt;int&gt;, new_ep_f1524 &lt;int&gt;, #&gt; # new_ep_f2534 &lt;int&gt;, new_ep_f3544 &lt;int&gt;, new_ep_f4554 &lt;int&gt;, #&gt; # new_ep_f5564 &lt;int&gt;, new_ep_f65 &lt;int&gt;, newrel_m014 &lt;int&gt;, #&gt; # newrel_m1524 &lt;int&gt;, newrel_m2534 &lt;int&gt;, newrel_m3544 &lt;int&gt;, #&gt; # newrel_m4554 &lt;int&gt;, newrel_m5564 &lt;int&gt;, newrel_m65 &lt;int&gt;, #&gt; # newrel_f014 &lt;int&gt;, newrel_f1524 &lt;int&gt;, newrel_f2534 &lt;int&gt;, #&gt; # newrel_f3544 &lt;int&gt;, newrel_f4554 &lt;int&gt;, newrel_f5564 &lt;int&gt;, #&gt; # newrel_f65 &lt;int&gt; This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values. In short, who is messy, and we’ll need multiple steps to tidy it. Like dplyr, tidyr is designed so that each function does one thing well. That means in real-life situations you’ll usually need to string together multiple verbs into a pipeline. The best place to start is almost always to gather together the columns that are not variables. Let’s have a look at what we’ve got: It looks like pais, iso2, and iso3 are three variables that redundantly specify the pais. anio is clearly also a variable. We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. new_sp_m014, new_ep_m014, new_ep_f014) these are likely to be values, not variables. So we need to gather together all the columns from new_sp_m014 to newrel_f65. We don’t know what those values represent yet, so we’ll give them the generic name &quot;key&quot;. We know the cells represent the count of casos, so we’ll use the variable casos. There are a lot of missing values in the current representation, so for now we’ll use na.rm just so we can focus on the values that are present. who1 &lt;- who %&gt;% gather(new_sp_m014:newrel_f65, key = &quot;key&quot;, value = &quot;casos&quot;, na.rm = TRUE) who1 #&gt; # A tibble: 76,046 x 6 #&gt; country iso2 iso3 year key casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 #&gt; 2 Afghanistan AF AFG 1998 new_sp_m014 30 #&gt; 3 Afghanistan AF AFG 1999 new_sp_m014 8 #&gt; 4 Afghanistan AF AFG 2000 new_sp_m014 52 #&gt; 5 Afghanistan AF AFG 2001 new_sp_m014 129 #&gt; 6 Afghanistan AF AFG 2002 new_sp_m014 90 #&gt; # … with 7.604e+04 more rows We can get some hint of the structure of the values in the new key column by counting them: who1 %&gt;% count(key) #&gt; # A tibble: 56 x 2 #&gt; key n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 new_ep_f014 1032 #&gt; 2 new_ep_f1524 1021 #&gt; 3 new_ep_f2534 1021 #&gt; 4 new_ep_f3544 1021 #&gt; 5 new_ep_f4554 1017 #&gt; 6 new_ep_f5564 1017 #&gt; # … with 50 more rows You might be able to parse this out by yourself with a little thought and some experimentation, but luckily we have the data dictionary handy. It tells us: The first three letters of each column denote whether the column contains new or old casos of TB. In this dataset, each column contains new casos. The next two letters describe the tipo of TB: rel stands for casos of relapse ep stands for casos of extrapulmonary TB sn stands for casos of pulmonary TB that could not be diagnosed by a pulmonary smear (smear negative) sp stands for casos of pulmonary TB that could be diagnosed be a pulmonary smear (smear positive) The sixth letter gives the sex of TB patients. The dataset groups casos by males (m) and females (f). The remaining numbers gives the age group. The dataset groups casos into seven age groups: 014 = 0 – 14 anios old 1524 = 15 – 24 anios old 2534 = 25 – 34 anios old 3544 = 35 – 44 anios old 4554 = 45 – 54 anios old 5564 = 55 – 64 anios old 65 = 65 or older We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel (it’s hard to spot this here but if you don’t fix it we’ll get errors in subsequent steps). You’ll learn about str_replace() in strings, but the basic idea is pretty simple: replace the characters “newrel” with “new_rel”. This makes all variable names consistent. who2 &lt;- who1 %&gt;% mutate(key = stringr::str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;)) who2 #&gt; # A tibble: 76,046 x 6 #&gt; country iso2 iso3 year key casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 #&gt; 2 Afghanistan AF AFG 1998 new_sp_m014 30 #&gt; 3 Afghanistan AF AFG 1999 new_sp_m014 8 #&gt; 4 Afghanistan AF AFG 2000 new_sp_m014 52 #&gt; 5 Afghanistan AF AFG 2001 new_sp_m014 129 #&gt; 6 Afghanistan AF AFG 2002 new_sp_m014 90 #&gt; # … with 7.604e+04 more rows We can separate the values in each code with two passes of separate(). The first pass will split the codes at each underscore. who3 &lt;- who2 %&gt;% separate(key, c(&quot;new&quot;, &quot;tipo&quot;, &quot;sexage&quot;), sep = &quot;_&quot;) who3 #&gt; # A tibble: 76,046 x 8 #&gt; country iso2 iso3 year new tipo sexage casos #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 new sp m014 0 #&gt; 2 Afghanistan AF AFG 1998 new sp m014 30 #&gt; 3 Afghanistan AF AFG 1999 new sp m014 8 #&gt; 4 Afghanistan AF AFG 2000 new sp m014 52 #&gt; 5 Afghanistan AF AFG 2001 new sp m014 129 #&gt; 6 Afghanistan AF AFG 2002 new sp m014 90 #&gt; # … with 7.604e+04 more rows Then we might as well drop the new column because it’s constant in this dataset. While we’re dropping columns, let’s also drop iso2 and iso3 since they’re redundant. who3 %&gt;% count(new) #&gt; # A tibble: 1 x 2 #&gt; new n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 new 76046 who4 &lt;- who3 %&gt;% select(-new, -iso2, -iso3) Next we’ll separate sexage into sex and age by splitting after the first character: who5 &lt;- who4 %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) who5 #&gt; # A tibble: 76,046 x 6 #&gt; country year tipo sex age casos #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1997 sp m 014 0 #&gt; 2 Afghanistan 1998 sp m 014 30 #&gt; 3 Afghanistan 1999 sp m 014 8 #&gt; 4 Afghanistan 2000 sp m 014 52 #&gt; 5 Afghanistan 2001 sp m 014 129 #&gt; 6 Afghanistan 2002 sp m 014 90 #&gt; # … with 7.604e+04 more rows The who dataset is now tidy! I’ve shown you the code a piece at a time, assigning each interim result to a new variable. This typically isn’t how you’d work interactively. Instead, you’d gradually build up a complex pipe: who %&gt;% gather(key, value, new_sp_m014:newrel_f65, na.rm = TRUE) %&gt;% mutate(key = stringr::str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;)) %&gt;% separate(key, c(&quot;new&quot;, &quot;var&quot;, &quot;sexage&quot;)) %&gt;% select(-new, -iso2, -iso3) %&gt;% separate(sexage, c(&quot;sex&quot;, &quot;age&quot;), sep = 1) 12.6.1 Exercises In this case study I set na.rm = TRUE just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero? What happens if you neglect the mutate() step? (mutate(key = stringr::str_replace(key, &quot;newrel&quot;, &quot;new_rel&quot;))) I claimed that iso2 and iso3 were redundant with pais. Confirm this claim. For each pais, anio, and sex compute the total number of casos of TB. Make an informative visualisation of the data. 12.7 Non-tidy data Before we continue on to other topics, it’s worth talking briefly about non-tidy data. Earlier in the chapter, I used the pejorative term “messy” to refer to non-tidy data. That’s an oversimplification: there are lots of useful and well-founded data structures that are not tidy data. There are two main reasons to use other data structures: Alternative representations may have substantial performance or space advantages. Specialised fields have evolved their own conventions for storing data that may be quite different to the conventions of tidy data. Either of these reasons means you’ll need something other than a tibble (or data frame). If your data does fit naturally into a rectangular structure composed of observations and variables, I think tidy data should be your default choice. But there are good reasons to use other structures; tidy data is not the only way. If you’d like to learn more about non-tidy data, I’d highly recommend this thoughtful blog post by Jeff Leek: http://simplystatistics.org/2016/02/17/non-tidy-data/ "],
["relational-data.html", "13 Relational data 13.1 Introduction 13.2 nycflights13 13.3 Keys 13.4 Mutating joins 13.5 Filtering joins 13.6 Join problems 13.7 Set operations", " 13 Relational data 13.1 Introduction It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. Relations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair. Sometimes both elements of a pair can be the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents. To work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data: Mutating joins, which add new variables to one data frame from matching observations in another. Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table. Set operations, which treat observations as if they were set elements. The most common place to find relational data is in a relational database management system (or RDBMS), a term that encompasses almost all modern databases. If you’ve used a database before, you’ve almost certainly used SQL. If so, you should find the concepts in this chapter familiar, although their expression in dplyr is a little different. Generally, dplyr is a little easier to use than SQL because dplyr is specialised to do data analysis: it makes common data analysis operations easier, at the expense of making it more difficult to do other things that aren’t commonly needed for data analysis. 13.1.1 Prerequisites We will explore relational data from nycflights13 using the two-table verbs from dplyr. library(tidyverse) library(nycflights13) 13.2 nycflights13 We will use the nycflights13 package to learn about relational data. nycflights13 contains four tibbles that are related to the flights table that you used in data transformation: airlines lets you look up the full carrier name from its abbreviated code: airlines #&gt; # A tibble: 16 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E Endeavor Air Inc. #&gt; 2 AA American Airlines Inc. #&gt; 3 AS Alaska Airlines Inc. #&gt; 4 B6 JetBlue Airways #&gt; 5 DL Delta Air Lines Inc. #&gt; 6 EV ExpressJet Airlines Inc. #&gt; # … with 10 more rows airports gives information about each airport, identified by the faa airport code: airports #&gt; # A tibble: 1,458 x 8 #&gt; faa name lat lon alt tz dst tzone #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/New… #&gt; 2 06A Moton Field Municipal A… 32.5 -85.7 264 -6 A America/Chi… #&gt; 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/Chi… #&gt; 4 06N Randall Airport 41.4 -74.4 523 -5 A America/New… #&gt; 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/New… #&gt; 6 0A9 Elizabethton Municipal … 36.4 -82.2 1593 -5 A America/New… #&gt; # … with 1,452 more rows planes gives information about each plane, identified by its tailnum: planes #&gt; # A tibble: 3,322 x 9 #&gt; tailnum year type manufacturer model engines seats speed engine #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 N10156 2004 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… #&gt; 2 N102UW 1998 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 3 N103US 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 4 N104UW 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; 5 N10575 2002 Fixed win… EMBRAER EMB-1… 2 55 NA Turbo… #&gt; 6 N105UW 1999 Fixed win… AIRBUS INDUST… A320-… 2 182 NA Turbo… #&gt; # … with 3,316 more rows weather gives the weather at each NYC airport for each hour: weather #&gt; # A tibble: 26,115 x 15 #&gt; origin year month day hour temp dewp humid wind_dir wind_speed #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 EWR 2013 1 1 1 39.0 26.1 59.4 270 10.4 #&gt; 2 EWR 2013 1 1 2 39.0 27.0 61.6 250 8.06 #&gt; 3 EWR 2013 1 1 3 39.0 28.0 64.4 240 11.5 #&gt; 4 EWR 2013 1 1 4 39.9 28.0 62.2 250 12.7 #&gt; 5 EWR 2013 1 1 5 39.0 28.0 64.4 260 12.7 #&gt; 6 EWR 2013 1 1 6 37.9 28.0 67.2 240 11.5 #&gt; # … with 2.611e+04 more rows, and 5 more variables: wind_gust &lt;dbl&gt;, #&gt; # precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt; One way to show the relationships between the different tables is with a drawing: This diagram is a little overwhelming, but it’s simple compared to some you’ll see in the wild! The key to understanding diagrams like this is to remember each relation always concerns a pair of tables. You don’t need to understand the whole thing; you just need to understand the chain of relations between the tables that you are interested in. For nycflights13: flights connects to planes via a single variable, tailnum. flights connects to airlines through the carrier variable. flights connects to airports in two ways: via the origin and dest variables. flights connects to weather via origin (the location), and year, month, day and hour (the time). 13.2.1 Exercises Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? 13.3 Keys The variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation. For example, each plane is uniquely identified by its tailnum. In other cases, multiple variables may be needed. For example, to identify an observation in weather you need five variables: year, month, day, hour, and origin. There are two types of keys: A primary key uniquely identifies an observation in its own table. For example, planes$tailnum is a primary key because it uniquely identifies each plane in the planes table. A foreign key uniquely identifies an observation in another table. For example, the flights$tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane. A variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airport table. Once you’ve identified the primary keys in your tables, it’s good practice to verify that they do indeed uniquely identify each observation. One way to do that is to count() the primary keys and look for entries where n is greater than one: planes %&gt;% count(tailnum) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 0 x 2 #&gt; # … with 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt; weather %&gt;% count(year, month, day, hour, origin) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 3 x 6 #&gt; year month day hour origin n #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2013 11 3 1 EWR 2 #&gt; 2 2013 11 3 1 JFK 2 #&gt; 3 2013 11 3 1 LGA 2 Sometimes a table doesn’t have an explicit primary key: each row is an observation, but no combination of variables reliably identifies it. For example, what’s the primary key in the flights table? You might think it would be the date plus the flight or tail number, but neither of those are unique: flights %&gt;% count(year, month, day, flight) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 29,768 x 5 #&gt; year month day flight n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 1 2 #&gt; 2 2013 1 1 3 2 #&gt; 3 2013 1 1 4 2 #&gt; 4 2013 1 1 11 3 #&gt; 5 2013 1 1 15 2 #&gt; 6 2013 1 1 21 2 #&gt; # … with 2.976e+04 more rows flights %&gt;% count(year, month, day, tailnum) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 64,928 x 5 #&gt; year month day tailnum n #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2013 1 1 N0EGMQ 2 #&gt; 2 2013 1 1 N11189 2 #&gt; 3 2013 1 1 N11536 2 #&gt; 4 2013 1 1 N11544 3 #&gt; 5 2013 1 1 N11551 2 #&gt; 6 2013 1 1 N12540 2 #&gt; # … with 6.492e+04 more rows When starting to work with this data, I had naively assumed that each flight number would be only used once per day: that would make it much easier to communicate problems with a specific flight. Unfortunately that is not the case! If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key. A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines. 13.3.1 Exercises Add a surrogate key to flights. Identify the keys in the following datasets Lahman::Batting, babynames::babynames nasaweather::atmos fueleconomy::vehicles ggplot2::diamonds (You might need to install some packages and read some documentation.) Draw a diagram illustrating the connections between the Batting, Master, and Salaries tables in the Lahman package. Draw another diagram that shows the relationship between Master, Managers, AwardsManagers. How would you characterise the relationship between the Batting, Pitching, and Fielding tables? 13.4 Mutating joins The first tool we’ll look at for combining a pair of tables is the mutating join. A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other. Like mutate(), the join functions add variables to the right, so if you have a lot of variables already, the new variables won’t get printed out. For these examples, we’ll make it easier to see what’s going on in the examples by creating a narrower dataset: flights2 &lt;- flights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier) flights2 #&gt; # A tibble: 336,776 x 8 #&gt; year month day hour origin dest tailnum carrier #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA #&gt; 2 2013 1 1 5 LGA IAH N24211 UA #&gt; 3 2013 1 1 5 JFK MIA N619AA AA #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL #&gt; 6 2013 1 1 5 EWR ORD N39463 UA #&gt; # … with 3.368e+05 more rows (Remember, when you’re in RStudio, you can also use View() to avoid this problem.) Imagine you want to add the full airline name to the flights2 data. You can combine the airlines and flights2 data frames with left_join(): flights2 %&gt;% select(-origin, -dest) %&gt;% left_join(airlines, by = &quot;carrier&quot;) #&gt; # A tibble: 336,776 x 7 #&gt; year month day hour tailnum carrier name #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 N14228 UA United Air Lines Inc. #&gt; 2 2013 1 1 5 N24211 UA United Air Lines Inc. #&gt; 3 2013 1 1 5 N619AA AA American Airlines Inc. #&gt; 4 2013 1 1 5 N804JB B6 JetBlue Airways #&gt; 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. #&gt; 6 2013 1 1 5 N39463 UA United Air Lines Inc. #&gt; # … with 3.368e+05 more rows The result of joining airlines to flights2 is an additional variable: name. This is why I call this type of join a mutating join. In this case, you could have got to the same place using mutate() and R’s base subsetting: flights2 %&gt;% select(-origin, -dest) %&gt;% mutate(name = airlines$name[match(carrier, airlines$carrier)]) #&gt; # A tibble: 336,776 x 7 #&gt; year month day hour tailnum carrier name #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 N14228 UA United Air Lines Inc. #&gt; 2 2013 1 1 5 N24211 UA United Air Lines Inc. #&gt; 3 2013 1 1 5 N619AA AA American Airlines Inc. #&gt; 4 2013 1 1 5 N804JB B6 JetBlue Airways #&gt; 5 2013 1 1 6 N668DN DL Delta Air Lines Inc. #&gt; 6 2013 1 1 5 N39463 UA United Air Lines Inc. #&gt; # … with 3.368e+05 more rows But this is hard to generalise when you need to match multiple variables, and takes close reading to figure out the overall intent. The following sections explain, in detail, how mutating joins work. You’ll start by learning a useful visual representation of joins. We’ll then use that to explain the four mutating join functions: the inner join, and the three outer joins. When working with real data, keys don’t always uniquely identify observations, so next we’ll talk about what happens when there isn’t a unique match. Finally, you’ll learn how to tell dplyr which variables are the keys for a given join. 13.4.1 Understanding joins To help you learn how joins work, I’m going to use a visual representation: x &lt;- tribble( ~ key, ~ val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot; ) y &lt;- tribble( ~ key, ~ val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; ) The coloured column represents the “key” variable: these are used to match the rows between the tables. The grey column represents the “value” column that is carried along for the ride. In these examples I’ll show a single key variable, but the idea generalises in a straightforward way to multiple keys and multiple values. A join is a way of connecting each row in x to zero, one, or more rows in y. The following diagram shows each potential match as an intersection of a pair of lines. (If you look closely, you might notice that we’ve switched the order of the key and value columns in x. This is to emphasise that joins match based on the key; the value is just carried along for the ride.) In an actual join, matches will be indicated with dots. The number of dots = the number of matches = the number of rows in the output. 13.4.2 Inner join The simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal: (To be precise, this is an inner equijoin because the keys are matched using the equality operator. Since most joins are equijoins we usually drop that specification.) The output of an inner join is a new data frame that contains the key, the x values, and the y values. We use by to tell dplyr which variable is the key: x %&gt;% inner_join(y, by = &quot;key&quot;) #&gt; # A tibble: 2 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 The most important property of an inner join is that unmatched rows are not included in the result. This means that generally inner joins are usually not appropriate for use in analysis because it’s too easy to lose observations. 13.4.3 Outer joins An inner join keeps observations that appear in both tables. An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins: A left join keeps all observations in x. A right join keeps all observations in y. A full join keeps all observations in x and y. These joins work by adding an additional “virtual” observation to each table. This observation has a key that always matches (if no other key matches), and a value filled with NA. Graphically, that looks like: The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others. Another way to depict the different types of joins is with a Venn diagram: However, this is not a great representation. It might jog your memory about which join preserves the observations in which table, but it suffers from a major limitation: a Venn diagram can’t show what happens when keys don’t uniquely identify an observation. 13.4.4 Duplicate keys So far all the diagrams have assumed that the keys are unique. But that’s not always the case. This section explains what happens when the keys are not unique. There are two possibilities: One table has duplicate keys. This is useful when you want to add in additional information as there is typically a one-to-many relationship. Note that I’ve put the key column in a slightly different position in the output. This reflects that the key is a primary key in y and a foreign key in x. x &lt;- tribble( ~ key, ~ val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 2, &quot;x3&quot;, 1, &quot;x4&quot; ) y &lt;- tribble( ~ key, ~ val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot; ) left_join(x, y, by = &quot;key&quot;) #&gt; # A tibble: 4 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 #&gt; 3 2 x3 y2 #&gt; 4 1 x4 y1 Both tables have duplicate keys. This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product: x &lt;- tribble( ~ key, ~ val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 2, &quot;x3&quot;, 3, &quot;x4&quot; ) y &lt;- tribble( ~ key, ~ val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 2, &quot;y3&quot;, 3, &quot;y4&quot; ) left_join(x, y, by = &quot;key&quot;) #&gt; # A tibble: 6 x 3 #&gt; key val_x val_y #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 x1 y1 #&gt; 2 2 x2 y2 #&gt; 3 2 x2 y3 #&gt; 4 2 x3 y2 #&gt; 5 2 x3 y3 #&gt; 6 3 x4 y4 13.4.5 Defining the key columns So far, the pairs of tables have always been joined by a single variable, and that variable has the same name in both tables. That constraint was encoded by by = &quot;key&quot;. You can use other values for by to connect the tables in other ways: The default, by = NULL, uses all variables that appear in both tables, the so called natural join. For example, the flights and weather tables match on their common variables: year, month, day, hour and origin. flights2 %&gt;% left_join(weather) #&gt; Joining, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;) #&gt; # A tibble: 336,776 x 18 #&gt; year month day hour origin dest tailnum carrier temp dewp humid #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA 39.0 28.0 64.4 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA 39.9 25.0 54.8 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA 39.0 27.0 61.6 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 39.0 27.0 61.6 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL 39.9 25.0 54.8 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA 39.0 28.0 64.4 #&gt; # … with 3.368e+05 more rows, and 7 more variables: wind_dir &lt;dbl&gt;, #&gt; # wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, #&gt; # visib &lt;dbl&gt;, time_hour &lt;dttm&gt; A character vector, by = &quot;x&quot;. This is like a natural join, but uses only some of the common variables. For example, flights and planes have year variables, but they mean different things so we only want to join by tailnum. flights2 %&gt;% left_join(planes, by = &quot;tailnum&quot;) #&gt; # A tibble: 336,776 x 16 #&gt; year.x month day hour origin dest tailnum carrier year.y type #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA 1999 Fixe… #&gt; 2 2013 1 1 5 LGA IAH N24211 UA 1998 Fixe… #&gt; 3 2013 1 1 5 JFK MIA N619AA AA 1990 Fixe… #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 2012 Fixe… #&gt; 5 2013 1 1 6 LGA ATL N668DN DL 1991 Fixe… #&gt; 6 2013 1 1 5 EWR ORD N39463 UA 2012 Fixe… #&gt; # … with 3.368e+05 more rows, and 6 more variables: manufacturer &lt;chr&gt;, #&gt; # model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt; Note that the year variables (which appear in both input data frames, but are not constrained to be equal) are disambiguated in the output with a suffix. A named character vector: by = c(&quot;a&quot; = &quot;b&quot;). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. For example, if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to: flights2 %&gt;% left_join(airports, c(&quot;dest&quot; = &quot;faa&quot;)) #&gt; # A tibble: 336,776 x 15 #&gt; year month day hour origin dest tailnum carrier name lat lon #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA Geor… 30.0 -95.3 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA Geor… 30.0 -95.3 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA Miam… 25.8 -80.3 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 &lt;NA&gt; NA NA #&gt; 5 2013 1 1 6 LGA ATL N668DN DL Hart… 33.6 -84.4 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA Chic… 42.0 -87.9 #&gt; # … with 3.368e+05 more rows, and 4 more variables: alt &lt;int&gt;, tz &lt;dbl&gt;, #&gt; # dst &lt;chr&gt;, tzone &lt;chr&gt; flights2 %&gt;% left_join(airports, c(&quot;origin&quot; = &quot;faa&quot;)) #&gt; # A tibble: 336,776 x 15 #&gt; year month day hour origin dest tailnum carrier name lat lon #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 EWR IAH N14228 UA Newa… 40.7 -74.2 #&gt; 2 2013 1 1 5 LGA IAH N24211 UA La G… 40.8 -73.9 #&gt; 3 2013 1 1 5 JFK MIA N619AA AA John… 40.6 -73.8 #&gt; 4 2013 1 1 5 JFK BQN N804JB B6 John… 40.6 -73.8 #&gt; 5 2013 1 1 6 LGA ATL N668DN DL La G… 40.8 -73.9 #&gt; 6 2013 1 1 5 EWR ORD N39463 UA Newa… 40.7 -74.2 #&gt; # … with 3.368e+05 more rows, and 4 more variables: alt &lt;int&gt;, tz &lt;dbl&gt;, #&gt; # dst &lt;chr&gt;, tzone &lt;chr&gt; 13.4.6 Exercises Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States: airports %&gt;% semi_join(flights, c(&quot;faa&quot; = &quot;dest&quot;)) %&gt;% ggplot(aes(lon, lat)) + borders(&quot;state&quot;) + geom_point() + coord_quickmap() (Don’t worry if you don’t understand what semi_join() does — you’ll learn about it next.) You might want to use the size or colour of the points to display the average delay for each airport. Add the location of the origin and destination (i.e. the lat and lon) to flights. Is there a relationship between the age of a plane and its delays? What weather conditions make it more likely to see a delay? What happened on June 13 2013? Display the spatial pattern of delays, and then use Google to cross-reference with the weather. 13.4.7 Other implementations base::merge() can perform all four types of mutating join: dplyr merge inner_join(x, y) merge(x, y) left_join(x, y) merge(x, y, all.x = TRUE) right_join(x, y) merge(x, y, all.y = TRUE), full_join(x, y) merge(x, y, all.x = TRUE, all.y = TRUE) The advantages of the specific dplyr verbs is that they more clearly convey the intent of your code: the difference between the joins is really important but concealed in the arguments of merge(). dplyr’s joins are considerably faster and don’t mess with the order of the rows. SQL is the inspiration for dplyr’s conventions, so the translation is straightforward: dplyr SQL inner_join(x, y, by = &quot;z&quot;) SELECT * FROM x INNER JOIN y USING (z) left_join(x, y, by = &quot;z&quot;) SELECT * FROM x LEFT OUTER JOIN y USING (z) right_join(x, y, by = &quot;z&quot;) SELECT * FROM x RIGHT OUTER JOIN y USING (z) full_join(x, y, by = &quot;z&quot;) SELECT * FROM x FULL OUTER JOIN y USING (z) Note that “INNER” and “OUTER” are optional, and often omitted. Joining different variables between the tables, e.g. inner_join(x, y, by = c(&quot;a&quot; = &quot;b&quot;)) uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than dplyr because you can connect the tables using constraints other than equality (sometimes called non-equijoins). 13.5 Filtering joins Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables. There are two types: semi_join(x, y) keeps all observations in x that have a match in y. anti_join(x, y) drops all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows. For example, imagine you’ve found the top ten most popular destinations: top_dest &lt;- flights %&gt;% count(dest, sort = TRUE) %&gt;% head(10) top_dest #&gt; # A tibble: 10 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ORD 17283 #&gt; 2 ATL 17215 #&gt; 3 LAX 16174 #&gt; 4 BOS 15508 #&gt; 5 MCO 14082 #&gt; 6 CLT 14064 #&gt; # … with 4 more rows Now you want to find each flight that went to one of those destinations. You could construct a filter yourself: flights %&gt;% filter(dest %in% top_dest$dest) #&gt; # A tibble: 141,145 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 542 540 2 923 #&gt; 2 2013 1 1 554 600 -6 812 #&gt; 3 2013 1 1 554 558 -4 740 #&gt; 4 2013 1 1 555 600 -5 913 #&gt; 5 2013 1 1 557 600 -3 838 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # … with 1.411e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; But it’s difficult to extend that approach to multiple variables. For example, imagine that you’d found the 10 days with highest average delays. How would you construct the filter statement that used year, month, and day to match it back to flights? Instead you can use a semi-join, which connects the two tables like a mutating join, but instead of adding new columns, only keeps the rows in x that have a match in y: flights %&gt;% semi_join(top_dest) #&gt; Joining, by = &quot;dest&quot; #&gt; # A tibble: 141,145 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 542 540 2 923 #&gt; 2 2013 1 1 554 600 -6 812 #&gt; 3 2013 1 1 554 558 -4 740 #&gt; 4 2013 1 1 555 600 -5 913 #&gt; 5 2013 1 1 557 600 -3 838 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # … with 1.411e+05 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Graphically, a semi-join looks like this: Only the existence of a match is important; it doesn’t matter which observation is matched. This means that filtering joins never duplicate rows like mutating joins do: The inverse of a semi-join is an anti-join. An anti-join keeps the rows that don’t have a match: Anti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don’t have a match in planes: flights %&gt;% anti_join(planes, by = &quot;tailnum&quot;) %&gt;% count(tailnum, sort = TRUE) #&gt; # A tibble: 722 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 2512 #&gt; 2 N725MQ 575 #&gt; 3 N722MQ 513 #&gt; 4 N723MQ 507 #&gt; 5 N713MQ 483 #&gt; 6 N735MQ 396 #&gt; # … with 716 more rows 13.5.1 Exercises What does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) Filter flights to only show flights with planes that have flown at least 100 flights. Combine fueleconomy::vehicles and fueleconomy::common to find only the records for the most common models. Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? What does anti_join(flights, airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) tell you? What does anti_join(airports, flights, by = c(&quot;faa&quot; = &quot;dest&quot;)) tell you? You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. 13.6 Join problems The data you’ve been working with in this chapter has been cleaned up so that you’ll have as few problems as possible. Your own data is unlikely to be so nice, so there are a few things that you should do with your own data to make your joins go smoothly. Start by identifying the variables that form the primary key in each table. You should usually do this based on your understanding of the data, not empirically by looking for a combination of variables that give a unique identifier. If you just look for variables without thinking about what they mean, you might get (un)lucky and find a combination that’s unique in your current data but the relationship might not be true in general. For example, the altitude and longitude uniquely identify each airport, but they are not good identifiers! airports %&gt;% count(alt, lon) %&gt;% filter(n &gt; 1) #&gt; # A tibble: 0 x 3 #&gt; # … with 3 variables: alt &lt;int&gt;, lon &lt;dbl&gt;, n &lt;int&gt; Check that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation! Check that your foreign keys match primary keys in another table. The best way to do this is with an anti_join(). It’s common for keys not to match because of data entry errors. Fixing these is often a lot of work. If you do have missing keys, you’ll need to be thoughtful about your use of inner vs. outer joins, carefully considering whether or not you want to drop rows that don’t have a match. Be aware that simply checking the number of rows before and after the join is not sufficient to ensure that your join has gone smoothly. If you have an inner join with duplicate keys in both tables, you might get unlucky as the number of dropped rows might exactly equal the number of duplicated rows! 13.7 Set operations The final type of two-table verb are the set operations. Generally, I use these the least frequently, but they are occasionally useful when you want to break a single complex filter into simpler pieces. All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets: intersect(x, y): return only observations in both x and y. union(x, y): return unique observations in x and y. setdiff(x, y): return observations in x, but not in y. Given this simple data: df1 &lt;- tribble( ~ x, ~ y, 1, 1, 2, 1 ) df2 &lt;- tribble( ~ x, ~ y, 1, 1, 1, 2 ) The four possibilities are: intersect(df1, df2) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 # Note that we get 3 rows, not 4 union(df1, df2) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 #&gt; 2 2 1 #&gt; 3 1 1 setdiff(df1, df2) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 1 setdiff(df2, df1) #&gt; # A tibble: 1 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 "],
["strings.html", "14 Strings 14.1 Introduction 14.2 String basics 14.3 Matching patterns with regular expressions 14.4 Tools 14.5 Other types of pattern 14.6 Other uses of regular expressions 14.7 stringi", " 14 Strings 14.1 Introduction This chapter introduces you to string manipulation in R. You’ll learn the basics of how strings work and how to create them by hand, but the focus of this chapter will be on regular expressions, or regexps for short. Regular expressions are useful because strings usually contain unstructured or semi-structured data, and regexps are a concise language for describing patterns in strings. When you first look at a regexp, you’ll think a cat walked across your keyboard, but as your understanding improves they will soon start to make sense. 14.1.1 Prerequisites This chapter will focus on the stringr package for string manipulation. stringr is not part of the core tidyverse because you don’t always have textual data, so we need to load it explicitly. library(tidyverse) library(stringr) 14.2 String basics You can create strings with either single quotes or double quotes. Unlike other languages, there is no difference in behaviour. I recommend always using &quot;, unless you want to create a string that contains multiple &quot;. string1 &lt;- &quot;This is a string&quot; string2 &lt;- &#39;If I want to include a &quot;quote&quot; inside a string, I use single quotes&#39; If you forget to close a quote, you’ll see +, the continuation character: &gt; &quot;This is a string without a closing quote + + + HELP I&#39;M STUCK If this happen to you, press Escape and try again! To include a literal single or double quote in a string you can use \\ to “escape” it: double_quote &lt;- &quot;\\&quot;&quot; # or &#39;&quot;&#39; single_quote &lt;- &#39;\\&#39;&#39; # or &quot;&#39;&quot; That means if you want to include a literal backslash, you’ll need to double it up: &quot;\\\\&quot;. Beware that the printed representation of a string is not the same as string itself, because the printed representation shows the escapes. To see the raw contents of the string, use writeLines(): x &lt;- c(&quot;\\&quot;&quot;, &quot;\\\\&quot;) x #&gt; [1] &quot;\\&quot;&quot; &quot;\\\\&quot; writeLines(x) #&gt; &quot; #&gt; \\ There are a handful of other special characters. The most common are &quot;\\n&quot;, newline, and &quot;\\t&quot;, tab, but you can see the complete list by requesting help on &quot;: ?'&quot;', or ?&quot;'&quot;. You’ll also sometimes see strings like &quot;\\u00b5&quot;, this is a way of writing non-English characters that works on all platforms: x &lt;- &quot;\\u00b5&quot; x #&gt; [1] &quot;µ&quot; Multiple strings are often stored in a character vector, which you can create with c(): c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;) #&gt; [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; 14.2.1 String length Base R contains many functions to work with strings but we’ll avoid them because they can be inconsistent, which makes them hard to remember. Instead we’ll use functions from stringr. These have more intuitive names, and all start with str_. For example, str_length() tells you the number of characters in a string: str_length(c(&quot;a&quot;, &quot;R for data science&quot;, NA)) #&gt; [1] 1 18 NA The common str_ prefix is particularly useful if you use RStudio, because typing str_ will trigger autocomplete, allowing you to see all stringr functions: 14.2.2 Combining strings To combine two or more strings, use str_c(): str_c(&quot;x&quot;, &quot;y&quot;) #&gt; [1] &quot;xy&quot; str_c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) #&gt; [1] &quot;xyz&quot; Use the sep argument to control how they’re separated: str_c(&quot;x&quot;, &quot;y&quot;, sep = &quot;, &quot;) #&gt; [1] &quot;x, y&quot; Like most other functions in R, missing values are contagious. If you want them to print as &quot;NA&quot;, use str_replace_na(): x &lt;- c(&quot;abc&quot;, NA) str_c(&quot;|-&quot;, x, &quot;-|&quot;) #&gt; [1] &quot;|-abc-|&quot; NA str_c(&quot;|-&quot;, str_replace_na(x), &quot;-|&quot;) #&gt; [1] &quot;|-abc-|&quot; &quot;|-NA-|&quot; As shown above, str_c() is vectorised, and it automatically recycles shorter vectors to the same length as the longest: str_c(&quot;prefix-&quot;, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), &quot;-suffix&quot;) #&gt; [1] &quot;prefix-a-suffix&quot; &quot;prefix-b-suffix&quot; &quot;prefix-c-suffix&quot; Objects of length 0 are silently dropped. This is particularly useful in conjunction with if: name &lt;- &quot;Hadley&quot; time_of_day &lt;- &quot;morning&quot; birthday &lt;- FALSE str_c( &quot;Good &quot;, time_of_day, &quot; &quot;, name, if (birthday) &quot; and HAPPY BIRTHDAY&quot;, &quot;.&quot; ) #&gt; [1] &quot;Good morning Hadley.&quot; To collapse a vector of strings into a single string, use collapse: str_c(c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), collapse = &quot;, &quot;) #&gt; [1] &quot;x, y, z&quot; 14.2.3 Subsetting strings You can extract parts of a string using str_sub(). As well as the string, str_sub() takes start and end arguments which give the (inclusive) position of the substring: x &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;) str_sub(x, 1, 3) #&gt; [1] &quot;App&quot; &quot;Ban&quot; &quot;Pea&quot; # negative numbers count backwards from end str_sub(x, -3, -1) #&gt; [1] &quot;ple&quot; &quot;ana&quot; &quot;ear&quot; Note that str_sub() won’t fail if the string is too short: it will just return as much as possible: str_sub(&quot;a&quot;, 1, 5) #&gt; [1] &quot;a&quot; You can also use the assignment form of str_sub() to modify strings: str_sub(x, 1, 1) &lt;- str_to_lower(str_sub(x, 1, 1)) x #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;pear&quot; 14.2.4 Locales Above I used str_to_lower() to change the text to lower case. You can also use str_to_upper() or str_to_title(). However, changing case is more complicated than it might at first appear because different languages have different rules for changing case. You can pick which set of rules to use by specifying a locale: # Turkish has two i&#39;s: with and without a dot, and it # has a different rule for capitalising them: str_to_upper(c(&quot;i&quot;, &quot;ı&quot;)) #&gt; [1] &quot;I&quot; &quot;I&quot; str_to_upper(c(&quot;i&quot;, &quot;ı&quot;), locale = &quot;tr&quot;) #&gt; [1] &quot;İ&quot; &quot;I&quot; The locale is specified as a ISO 639 language code, which is a two or three letter abbreviation. If you don’t already know the code for your language, Wikipedia has a good list. If you leave the locale blank, it will use the current locale, as provided by your operating system. Another important operation that’s affected by the locale is sorting. The base R order() and sort() functions sort strings using the current locale. If you want robust behaviour across different computers, you may want to use str_sort() and str_order() which take an additional locale argument: x &lt;- c(&quot;apple&quot;, &quot;eggplant&quot;, &quot;banana&quot;) str_sort(x, locale = &quot;en&quot;) # English #&gt; [1] &quot;apple&quot; &quot;banana&quot; &quot;eggplant&quot; str_sort(x, locale = &quot;haw&quot;) # Hawaiian #&gt; [1] &quot;apple&quot; &quot;eggplant&quot; &quot;banana&quot; 14.2.5 Exercises In code that doesn’t use stringr, you’ll often see paste() and paste0(). What’s the difference between the two functions? What stringr function are they equivalent to? How do the functions differ in their handling of NA? In your own words, describe the difference between the sep and collapse arguments to str_c(). Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters? What does str_wrap() do? When might you want to use it? What does str_trim() do? What’s the opposite of str_trim()? Write a function that turns (e.g.) a vector c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) into the string a, b, and c. Think carefully about what it should do if given a vector of length 0, 1, or 2. 14.3 Matching patterns with regular expressions Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful. To learn regular expressions, we’ll use str_view() and str_view_all(). These functions take a character vector and a regular expression, and show you how they match. We’ll start with very simple regular expressions and then gradually get more and more complicated. Once you’ve mastered pattern matching, you’ll learn how to apply those ideas with various stringr functions. 14.3.1 Basic matches The simplest patterns match exact strings: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;an&quot;) The next step up in complexity is ., which matches any character (except a newline): str_view(x, &quot;.a.&quot;) But if “.” matches any character, how do you match the character “.”? You need to use an “escape” to tell the regular expression you want to match it exactly, not use its special behaviour. Like strings, regexps use the backslash, \\, to escape special behaviour. So to match an ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So to create the regular expression \\. we need the string &quot;\\\\.&quot;. # To create the regular expression, we need \\\\ dot &lt;- &quot;\\\\.&quot; # But the expression itself only contains one: writeLines(dot) #&gt; \\. # And this tells R to look for an explicit . str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;bef&quot;), &quot;a\\\\.c&quot;) If \\ is used as an escape character in regular expressions, how do you match a literal \\? Well you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write &quot;\\\\\\\\&quot; — you need four backslashes to match one! x &lt;- &quot;a\\\\b&quot; writeLines(x) #&gt; a\\b str_view(x, &quot;\\\\\\\\&quot;) In this book, I’ll write regular expression as \\. and strings that represent the regular expression as &quot;\\\\.&quot;. 14.3.1.1 Exercises Explain why each of these strings don’t match a \\: &quot;\\&quot;, &quot;\\\\&quot;, &quot;\\\\\\&quot;. How would you match the sequence &quot;'\\? What patterns will the regular expression \\..\\..\\.. match? How would you represent it as a string? 14.3.2 Anchors By default, regular expressions will match any part of a string. It’s often useful to anchor the regular expression so that it matches from the start or end of the string. You can use: ^ to match the start of the string. $ to match the end of the string. x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;^a&quot;) str_view(x, &quot;a$&quot;) To remember which is which, try this mnemonic which I learned from Evan Misshula: if you begin with power (^), you end up with money ($). To force a regular expression to only match a complete string, anchor it with both ^ and $: x &lt;- c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;) str_view(x, &quot;apple&quot;) str_view(x, &quot;^apple$&quot;) You can also match the boundary between words with \\b. I don’t often use this in R, but I will sometimes use it when I’m doing a search in RStudio when I want to find the name of a function that’s a component of other functions. For example, I’ll search for \\bsum\\b to avoid matching summarise, summary, rowsum and so on. 14.3.2.1 Exercises How would you match the literal string &quot;$^$&quot;? Given the corpus of common words in stringr::words, create regular expressions that find all words that: Start with “y”. End with “x” Are exactly three letters long. (Don’t cheat by using str_length()!) Have seven letters or more. Since this list is long, you might want to use the match argument to str_view() to show only the matching or non-matching words. 14.3.3 Character classes and alternatives There are a number of special patterns that match more than one character. You’ve already seen ., which matches any character apart from a newline. There are four other useful tools: \\d: matches any digit. \\s: matches any whitespace (e.g. space, tab, newline). [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. Remember, to create a regular expression containing \\d or \\s, you’ll need to escape the \\ for the string, so you’ll type &quot;\\\\d&quot; or &quot;\\\\s&quot;. A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex. Many people find this more readable. # Look for a literal character that normally has special meaning in a regex str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;a[.]c&quot;) str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;.[*]c&quot;) str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;), &quot;a[ ]&quot;) This works for most (but not all) regex metacharacters: $ . | ? * + ( ) [ {. Unfortunately, a few characters have special meaning even inside a character class and must be handled with backslash escapes: ] \\ ^ and -. You can use alternation to pick between one or more alternative patterns. For example, abc|d..f will match either ‘“abc”’, or &quot;deaf&quot;. Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz. Like with mathematical expressions, if precedence ever gets confusing, use parentheses to make it clear what you want: str_view(c(&quot;grey&quot;, &quot;gray&quot;), &quot;gr(e|a)y&quot;) 14.3.3.1 Exercises Create regular expressions to find all words that: Start with a vowel. That only contain consonants. (Hint: thinking about matching “not”-vowels.) End with ed, but not with eed. End with ing or ise. Empirically verify the rule “i before e except after c”. Is “q” always followed by a “u”? Write a regular expression that matches a word if it’s probably written in British English, not American English. Create a regular expression that will match telephone numbers as commonly written in your country. 14.3.4 Repetition The next step up in power involves controlling how many times a pattern matches: ?: 0 or 1 +: 1 or more *: 0 or more x &lt;- &quot;1888 is the longest year in Roman numerals: MDCCCLXXXVIII&quot; str_view(x, &quot;CC?&quot;) str_view(x, &quot;CC+&quot;) str_view(x, &#39;C[LX]+&#39;) Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+. You can also specify the number of matches precisely: {n}: exactly n {n,}: n or more {,m}: at most m {n,m}: between n and m str_view(x, &quot;C{2}&quot;) str_view(x, &quot;C{2,}&quot;) str_view(x, &quot;C{2,3}&quot;) By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them. This is an advanced feature of regular expressions, but it’s useful to know that it exists: str_view(x, &#39;C{2,3}?&#39;) str_view(x, &#39;C[LX]+?&#39;) 14.3.4.1 Exercises Describe the equivalents of ?, +, * in {m,n} form. Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ &quot;\\\\{.+\\\\}&quot; \\d{4}-\\d{2}-\\d{2} &quot;\\\\\\\\{4}&quot; Create regular expressions to find all words that: Start with three consonants. Have three or more vowels in a row. Have two or more vowel-consonant pairs in a row. Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner. 14.3.5 Grouping and backreferences Earlier, you learned about parentheses as a way to disambiguate complex expressions. Parentheses also create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \\1, \\2 etc. For example, the following regular expression finds all fruits that have a repeated pair of letters. str_view(fruit, &quot;(..)\\\\1&quot;, match = TRUE) (Shortly, you’ll also see how they’re useful in conjunction with str_match().) 14.3.5.1 Exercises Describe, in words, what these expressions will match: (.)\\1\\1 &quot;(.)(.)\\\\2\\\\1&quot; (..)\\1 &quot;(.).\\\\1.\\\\1&quot; &quot;(.)(.)(.).*\\\\3\\\\2\\\\1&quot; Construct regular expressions to match words that: Start and end with the same character. Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.) Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.) 14.4 Tools Now that you’ve learned the basics of regular expressions, it’s time to learn how to apply them to real problems. In this section you’ll learn a wide array of stringr functions that let you: Determine which strings match a pattern. Find the positions of matches. Extract the content of matches. Replace matches with new values. Split a string based on a match. A word of caution before we continue: because regular expressions are so powerful, it’s easy to try and solve every problem with a single regular expression. In the words of Jamie Zawinski: Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. As a cautionary tale, check out this regular expression that checks if a email address is valid: (?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?: \\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:( ?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\0 31]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\ ](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+ (?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?: (?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n) ?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\ r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n) ?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t] )*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])* )(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*) *:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+ |\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r \\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?: \\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t ]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031 ]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\]( ?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(? :(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(? :\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(? :(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)? [ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*:(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]| \\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt; @,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot; (?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(? :[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[ \\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000- \\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|( ?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,; :\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([ ^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot; .\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\ ]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\ [\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\ r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\] |\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\0 00-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\ .|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@, ;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(? :[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])* (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[ ^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\] ]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)(?:,\\s*( ?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:( ?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[ \\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t ])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t ])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(? :\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+| \\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?: [^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\ ]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n) ?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot; ()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n) ?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt; @,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@, ;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)? (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?: \\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[ &quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t]) *))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]) +|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\ .(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:( ?:\\r\\n)?[ \\t])*))*)?;\\s*) This is a somewhat pathological example (because email addresses are actually surprisingly complex), but is used in real code. See the stackoverflow discussion at http://stackoverflow.com/a/201378 for more details. Don’t forget that you’re in a programming language and you have other tools at your disposal. Instead of creating one complex regular expression, it’s often easier to write a series of simpler regexps. If you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one. 14.4.1 Detect matches To determine if a character vector matches a pattern, use str_detect(). It returns a logical vector the same length as the input: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_detect(x, &quot;e&quot;) #&gt; [1] TRUE FALSE TRUE Remember that when you use a logical vector in a numeric context, FALSE becomes 0 and TRUE becomes 1. That makes sum() and mean() useful if you want to answer questions about matches across a larger vector: # How many common words start with t? sum(str_detect(words, &quot;^t&quot;)) #&gt; [1] 65 # What proportion of common words end with a vowel? mean(str_detect(words, &quot;[aeiou]$&quot;)) #&gt; [1] 0.277 When you have complex logical conditions (e.g. match a or b but not c unless d) it’s often easier to combine multiple str_detect() calls with logical operators, rather than trying to create a single regular expression. For example, here are two ways to find all words that don’t contain any vowels: # Find all words containing at least one vowel, and negate no_vowels_1 &lt;- !str_detect(words, &quot;[aeiou]&quot;) # Find all words consisting only of consonants (non-vowels) no_vowels_2 &lt;- str_detect(words, &quot;^[^aeiou]+$&quot;) identical(no_vowels_1, no_vowels_2) #&gt; [1] TRUE The results are identical, but I think the first approach is significantly easier to understand. If your regular expression gets overly complicated, try breaking it up into smaller pieces, giving each piece a name, and then combining the pieces with logical operations. A common use of str_detect() is to select the elements that match a pattern. You can do this with logical subsetting, or the convenient str_subset() wrapper: words[str_detect(words, &quot;x$&quot;)] #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; str_subset(words, &quot;x$&quot;) #&gt; [1] &quot;box&quot; &quot;sex&quot; &quot;six&quot; &quot;tax&quot; Typically, however, your strings will be one column of a data frame, and you’ll want to use filter instead: df &lt;- tibble( word = words, i = seq_along(word) ) df %&gt;% filter(str_detect(words, &quot;x$&quot;)) #&gt; # A tibble: 4 x 2 #&gt; word i #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 box 108 #&gt; 2 sex 747 #&gt; 3 six 772 #&gt; 4 tax 841 A variation on str_detect() is str_count(): rather than a simple yes or no, it tells you how many matches there are in a string: x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_count(x, &quot;a&quot;) #&gt; [1] 1 3 1 # On average, how many vowels per word? mean(str_count(words, &quot;[aeiou]&quot;)) #&gt; [1] 1.99 It’s natural to use str_count() with mutate(): df %&gt;% mutate( vowels = str_count(word, &quot;[aeiou]&quot;), consonants = str_count(word, &quot;[^aeiou]&quot;) ) #&gt; # A tibble: 980 x 4 #&gt; word i vowels consonants #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 a 1 1 0 #&gt; 2 able 2 2 2 #&gt; 3 about 3 3 2 #&gt; 4 absolute 4 4 4 #&gt; 5 accept 5 2 4 #&gt; 6 account 6 3 4 #&gt; # … with 974 more rows Note that matches never overlap. For example, in &quot;abababa&quot;, how many times will the pattern &quot;aba&quot; match? Regular expressions say two, not three: str_count(&quot;abababa&quot;, &quot;aba&quot;) #&gt; [1] 2 str_view_all(&quot;abababa&quot;, &quot;aba&quot;) Note the use of str_view_all(). As you’ll shortly learn, many stringr functions come in pairs: one function works with a single match, and the other works with all matches. The second function will have the suffix _all. 14.4.2 Exercises For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls. Find all words that start or end with x. Find all words that start with a vowel and end with a consonant. Are there any words that contain at least one of each different vowel? What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) 14.4.3 Extract matches To extract the actual text of a match, use str_extract(). To show that off, we’re going to need a more complicated example. I’m going to use the Harvard sentences, which were designed to test VOIP systems, but are also useful for practicing regexps. These are provided in stringr::sentences: length(sentences) #&gt; [1] 720 head(sentences) #&gt; [1] &quot;The birch canoe slid on the smooth planks.&quot; #&gt; [2] &quot;Glue the sheet to the dark blue background.&quot; #&gt; [3] &quot;It&#39;s easy to tell the depth of a well.&quot; #&gt; [4] &quot;These days a chicken leg is a rare dish.&quot; #&gt; [5] &quot;Rice is often served in round bowls.&quot; #&gt; [6] &quot;The juice of lemons makes fine punch.&quot; Imagine we want to find all sentences that contain a colour. We first create a vector of colour names, and then turn it into a single regular expression: colours &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) colour_match #&gt; [1] &quot;red|orange|yellow|green|blue|purple&quot; Now we can select the sentences that contain a colour, and then extract the colour to figure out which one it is: has_colour &lt;- str_subset(sentences, colour_match) matches &lt;- str_extract(has_colour, colour_match) head(matches) #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;blue&quot; Note that str_extract() only extracts the first match. We can see that most easily by first selecting all the sentences that have more than 1 match: more &lt;- sentences[str_count(sentences, colour_match) &gt; 1] str_view_all(more, colour_match) str_extract(more, colour_match) #&gt; [1] &quot;blue&quot; &quot;green&quot; &quot;orange&quot; This is a common pattern for stringr functions, because working with a single match allows you to use much simpler data structures. To get all matches, use str_extract_all(). It returns a list: str_extract_all(more, colour_match) #&gt; [[1]] #&gt; [1] &quot;blue&quot; &quot;red&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;green&quot; &quot;red&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;orange&quot; &quot;red&quot; You’ll learn more about lists in lists and iteration. If you use simplify = TRUE, str_extract_all() will return a matrix with short matches expanded to the same length as the longest: str_extract_all(more, colour_match, simplify = TRUE) #&gt; [,1] [,2] #&gt; [1,] &quot;blue&quot; &quot;red&quot; #&gt; [2,] &quot;green&quot; &quot;red&quot; #&gt; [3,] &quot;orange&quot; &quot;red&quot; x &lt;- c(&quot;a&quot;, &quot;a b&quot;, &quot;a b c&quot;) str_extract_all(x, &quot;[a-z]&quot;, simplify = TRUE) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;a&quot; &quot;&quot; &quot;&quot; #&gt; [2,] &quot;a&quot; &quot;b&quot; &quot;&quot; #&gt; [3,] &quot;a&quot; &quot;b&quot; &quot;c&quot; 14.4.3.1 Exercises In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem. From the Harvard sentences data, extract: The first word from each sentence. All words ending in ing. All plurals. 14.4.4 Grouped matches Earlier in this chapter we talked about the use of parentheses for clarifying precedence and for backreferences when matching. You can also use parentheses to extract parts of a complex match. For example, imagine we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space. noun &lt;- &quot;(a|the) ([^ ]+)&quot; has_noun &lt;- sentences %&gt;% str_subset(noun) %&gt;% head(10) has_noun %&gt;% str_extract(noun) #&gt; [1] &quot;the smooth&quot; &quot;the sheet&quot; &quot;the depth&quot; &quot;a chicken&quot; &quot;the parked&quot; #&gt; [6] &quot;the sun&quot; &quot;the huge&quot; &quot;the ball&quot; &quot;the woman&quot; &quot;a helps&quot; str_extract() gives us the complete match; str_match() gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group: has_noun %&gt;% str_match(noun) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; #&gt; [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; #&gt; [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; #&gt; [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; #&gt; [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; #&gt; [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; #&gt; [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; #&gt; [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; #&gt; [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; (Unsurprisingly, our heuristic for detecting nouns is poor, and also picks up adjectives like smooth and parked.) If your data is in a tibble, it’s often easier to use tidyr::extract(). It works like str_match() but requires you to name the matches, which are then placed in new columns: tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;article&quot;, &quot;noun&quot;), &quot;(a|the) ([^ ]+)&quot;, remove = FALSE ) #&gt; # A tibble: 720 x 3 #&gt; sentence article noun #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 The birch canoe slid on the smooth planks. the smooth #&gt; 2 Glue the sheet to the dark blue background. the sheet #&gt; 3 It&#39;s easy to tell the depth of a well. the depth #&gt; 4 These days a chicken leg is a rare dish. a chicken #&gt; 5 Rice is often served in round bowls. &lt;NA&gt; &lt;NA&gt; #&gt; 6 The juice of lemons makes fine punch. &lt;NA&gt; &lt;NA&gt; #&gt; # … with 714 more rows Like str_extract(), if you want all matches for each string, you’ll need str_match_all(). 14.4.4.1 Exercises Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. Find all contractions. Separate out the pieces before and after the apostrophe. 14.4.5 Replacing matches str_replace() and str_replace_all() allow you to replace matches with new strings. The simplest use is to replace a pattern with a fixed string: x &lt;- c(&quot;apple&quot;, &quot;pear&quot;, &quot;banana&quot;) str_replace(x, &quot;[aeiou]&quot;, &quot;-&quot;) #&gt; [1] &quot;-pple&quot; &quot;p-ar&quot; &quot;b-nana&quot; str_replace_all(x, &quot;[aeiou]&quot;, &quot;-&quot;) #&gt; [1] &quot;-ppl-&quot; &quot;p--r&quot; &quot;b-n-n-&quot; With str_replace_all() you can perform multiple replacements by supplying a named vector: x &lt;- c(&quot;1 house&quot;, &quot;2 cars&quot;, &quot;3 people&quot;) str_replace_all(x, c(&quot;1&quot; = &quot;one&quot;, &quot;2&quot; = &quot;two&quot;, &quot;3&quot; = &quot;three&quot;)) #&gt; [1] &quot;one house&quot; &quot;two cars&quot; &quot;three people&quot; Instead of replacing with a fixed string you can use backreferences to insert components of the match. In the following code, I flip the order of the second and third words. sentences %&gt;% str_replace(&quot;([^ ]+) ([^ ]+) ([^ ]+)&quot;, &quot;\\\\1 \\\\3 \\\\2&quot;) %&gt;% head(5) #&gt; [1] &quot;The canoe birch slid on the smooth planks.&quot; #&gt; [2] &quot;Glue sheet the to the dark blue background.&quot; #&gt; [3] &quot;It&#39;s to easy tell the depth of a well.&quot; #&gt; [4] &quot;These a days chicken leg is a rare dish.&quot; #&gt; [5] &quot;Rice often is served in round bowls.&quot; 14.4.5.1 Exercises Replace all forward slashes in a string with backslashes. Implement a simple version of str_to_lower() using replace_all(). Switch the first and last letters in words. Which of those strings are still words? 14.4.6 Splitting Use str_split() to split a string up into pieces. For example, we could split sentences into words: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;) #&gt; [[1]] #&gt; [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [8] &quot;planks.&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; #&gt; [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; #&gt; #&gt; [[4]] #&gt; [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; #&gt; [8] &quot;rare&quot; &quot;dish.&quot; #&gt; #&gt; [[5]] #&gt; [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; Because each component might contain a different number of pieces, this returns a list. If you’re working with a length-1 vector, the easiest thing is to just extract the first element of the list: &quot;a|b|c|d&quot; %&gt;% str_split(&quot;\\\\|&quot;) %&gt;% .[[1]] #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; Otherwise, like the other stringr functions that return a list, you can use simplify = TRUE to return a matrix: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;, simplify = TRUE) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; #&gt; [2,] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; &quot;dark&quot; &quot;blue&quot; #&gt; [3,] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; #&gt; [4,] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; #&gt; [5,] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; #&gt; [,8] [,9] #&gt; [1,] &quot;planks.&quot; &quot;&quot; #&gt; [2,] &quot;background.&quot; &quot;&quot; #&gt; [3,] &quot;a&quot; &quot;well.&quot; #&gt; [4,] &quot;rare&quot; &quot;dish.&quot; #&gt; [5,] &quot;&quot; &quot;&quot; You can also request a maximum number of pieces: fields &lt;- c(&quot;Name: Hadley&quot;, &quot;Country: NZ&quot;, &quot;Age: 35&quot;) fields %&gt;% str_split(&quot;: &quot;, n = 2, simplify = TRUE) #&gt; [,1] [,2] #&gt; [1,] &quot;Name&quot; &quot;Hadley&quot; #&gt; [2,] &quot;Country&quot; &quot;NZ&quot; #&gt; [3,] &quot;Age&quot; &quot;35&quot; Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word boundary()s: x &lt;- &quot;This is a sentence. This is another sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) str_split(x, &quot; &quot;)[[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence.&quot; &quot;&quot; &quot;This&quot; #&gt; [7] &quot;is&quot; &quot;another&quot; &quot;sentence.&quot; str_split(x, boundary(&quot;word&quot;))[[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; &quot;This&quot; &quot;is&quot; #&gt; [7] &quot;another&quot; &quot;sentence&quot; 14.4.6.1 Exercises Split up a string like &quot;apples, pears, and bananas&quot; into individual components. Why is it better to split up by boundary(&quot;word&quot;) than &quot; &quot;? What does splitting with an empty string (&quot;&quot;) do? Experiment, and then read the documentation. 14.4.7 Find matches str_locate() and str_locate_all() give you the starting and ending positions of each match. These are particularly useful when none of the other functions does exactly what you want. You can use str_locate() to find the matching pattern, str_sub() to extract and/or modify them. 14.5 Other types of pattern When you use a pattern that’s a string, it’s automatically wrapped into a call to regex(): # The regular call: str_view(fruit, &quot;nana&quot;) # Is shorthand for str_view(fruit, regex(&quot;nana&quot;)) You can use the other arguments of regex() to control details of the match: ignore_case = TRUE allows characters to match either their uppercase or lowercase forms. This always uses the current locale. bananas &lt;- c(&quot;banana&quot;, &quot;Banana&quot;, &quot;BANANA&quot;) str_view(bananas, &quot;banana&quot;) str_view(bananas, regex(&quot;banana&quot;, ignore_case = TRUE)) multiline = TRUE allows ^ and $ to match the start and end of each line rather than the start and end of the complete string. x &lt;- &quot;Line 1\\nLine 2\\nLine 3&quot; str_extract_all(x, &quot;^Line&quot;)[[1]] #&gt; [1] &quot;Line&quot; str_extract_all(x, regex(&quot;^Line&quot;, multiline = TRUE))[[1]] #&gt; [1] &quot;Line&quot; &quot;Line&quot; &quot;Line&quot; comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: &quot;\\\\ &quot;. phone &lt;- regex(&quot; \\\\(? # optional opening parens (\\\\d{3}) # area code [) -]? # optional closing parens, space, or dash (\\\\d{3}) # another three numbers [ -]? # optional space or dash (\\\\d{3}) # three more numbers &quot;, comments = TRUE) str_match(&quot;514-791-8141&quot;, phone) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] &quot;514-791-814&quot; &quot;514&quot; &quot;791&quot; &quot;814&quot; dotall = TRUE allows . to match everything, including \\n. There are three other functions you can use instead of regex(): fixed(): matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions. The following microbenchmark shows that it’s about 3x faster for a simple example. microbenchmark::microbenchmark( fixed = str_detect(sentences, fixed(&quot;the&quot;)), regex = str_detect(sentences, &quot;the&quot;), times = 20 ) #&gt; Unit: microseconds #&gt; expr min lq mean median uq max neval #&gt; fixed 56.2 63.5 90.9 74.6 92.5 357 20 #&gt; regex 291.3 302.4 329.8 308.7 318.0 659 20 Beware using fixed() with non-English data. It is problematic because there are often multiple ways of representing the same character. For example, there are two ways to define “á”: either as a single character or as an “a” plus an accent: a1 &lt;- &quot;\\u00e1&quot; a2 &lt;- &quot;a\\u0301&quot; c(a1, a2) #&gt; [1] &quot;á&quot; &quot;á&quot; a1 == a2 #&gt; [1] FALSE They render identically, but because they’re defined differently, fixed() doesn’t find a match. Instead, you can use coll(), defined next, to respect human character comparison rules: str_detect(a1, fixed(a2)) #&gt; [1] FALSE str_detect(a1, coll(a2)) #&gt; [1] TRUE coll(): compare strings using standard collation rules. This is useful for doing case insensitive matching. Note that coll() takes a locale parameter that controls which rules are used for comparing characters. Unfortunately different parts of the world use different rules! # That means you also need to be aware of the difference # when doing case insensitive matches: i &lt;- c(&quot;I&quot;, &quot;İ&quot;, &quot;i&quot;, &quot;ı&quot;) i #&gt; [1] &quot;I&quot; &quot;İ&quot; &quot;i&quot; &quot;ı&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE)) #&gt; [1] &quot;I&quot; &quot;i&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE, locale = &quot;tr&quot;)) #&gt; [1] &quot;İ&quot; &quot;i&quot; Both fixed() and regex() have ignore_case arguments, but they do not allow you to pick the locale: they always use the default locale. You can see what that is with the following code; more on stringi later. stringi::stri_locale_info() #&gt; $Language #&gt; [1] &quot;en&quot; #&gt; #&gt; $Country #&gt; [1] &quot;US&quot; #&gt; #&gt; $Variant #&gt; [1] &quot;&quot; #&gt; #&gt; $Name #&gt; [1] &quot;en_US&quot; The downside of coll() is speed; because the rules for recognising which characters are the same are complicated, coll() is relatively slow compared to regex() and fixed(). As you saw with str_split() you can use boundary() to match boundaries. You can also use it with the other functions: x &lt;- &quot;This is a sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) str_extract_all(x, boundary(&quot;word&quot;)) #&gt; [[1]] #&gt; [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; 14.5.1 Exercises How would you find all strings containing \\ with regex() vs. with fixed()? What are the five most common words in sentences? 14.6 Other uses of regular expressions There are two useful function in base R that also use regular expressions: apropos() searches all objects available from the global environment. This is useful if you can’t quite remember the name of the function. apropos(&quot;replace&quot;) #&gt; [1] &quot;%+replace%&quot; &quot;replace&quot; &quot;replace_na&quot; #&gt; [4] &quot;setReplaceMethod&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; #&gt; [7] &quot;str_replace_na&quot; &quot;theme_replace&quot; dir() lists all the files in a directory. The pattern argument takes a regular expression and only returns file names that match the pattern. For example, you can find all the R Markdown files in the current directory with: head(dir(pattern = &quot;\\\\.Rmd$&quot;)) #&gt; [1] &quot;communicate-plots.Rmd&quot; &quot;communicate.Rmd&quot; &quot;datetimes.Rmd&quot; #&gt; [4] &quot;EDA.Rmd&quot; &quot;explore.Rmd&quot; &quot;factors.Rmd&quot; (If you’re more comfortable with “globs” like *.Rmd, you can convert them to regular expressions with glob2rx()): 14.7 stringi stringr is built on top of the stringi package. stringr is useful when you’re learning because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions. stringi, on the other hand, is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions to stringr’s 46. If you find yourself struggling to do something in stringr, it’s worth taking a look at stringi. The packages work very similarly, so you should be able to translate your stringr knowledge in a natural way. The main difference is the prefix: str_ vs. stri_. 14.7.1 Exercises Find the stringi functions that: Count the number of words. Find duplicated strings. Generate random text. How do you control the language that stri_sort() uses for sorting? "],
["factores.html", "15 Factores 15.1 Introducción 15.2 Creando factores 15.3 Encuesta social general 15.4 Modificar el orden de los factores 15.5 Modificar los niveles de los factores", " 15 Factores 15.1 Introducción En R, los factores se usan para trabajar con variables categóricas, es decir, variables que tienen un conjunto fijo y conocido de valores posibles. Además, son útiles cuando se quiere mostrar vectores de caracteres en un orden no alfabético. Históricamente, los factores eran más sencillos de trabajar que los caracteres. Como resultado, muchas de las funciones de R base automáticamente convierten los caracteres a factores. Esto significa que, a menudo, los factores aparecen en lugares donde no son realmente útiles. Afortunadamente, no tienes que preocuparte de eso en el tidyverse, y puedes concentrarte en situaciones donde los factores son genuinamente útiles. 15.1.1 Requisitos previos Para trabajar con factores, vamos a usar el paquete forcats (del inglés para cadenas), que provee herramientas para lidear con variables categóricas (¡y es un anagrama de factores!). Este paquete provee un amplio rango de ayudantes para trabajar con factores. forcats no es parte de los paquetes centrales de tidyverse, así que necesitamos cargarlo de forma explícita. library(tidyverse) library(forcats) library(datos) 15.1.2 Aprendiendo más Si quieres aprender más sobre los factores, recomiendo leer el artículo de Amelia McNamara y Nicholas Horton, Wrangling categorical data in R (el nombre significa Luchando con Datos Categóricos en R). Este artículo cuenta parte de la historia discutida en stringsAsFactors: An unauthorized biography (del inglés cadenasComoFactores: Una Biografía No Autorizada) y stringsAsFactors = &lt;sigh&gt; (del inglés cadenasComoFactores = &lt;suspiro&gt;), y compara las propuestas tidy para los datos categóricos demostrados en este libro, contra los métodos base de R. Una versión temprana de este artículo ayudó a motivar y limitar el paquete forcats; ¡Gracias Amelia y Nick! 15.2 Creando factores Imagina que tienes una variable que guarda los meses: x1 &lt;- c(&quot;Dic&quot;, &quot;Abr&quot;, &quot;Ene&quot;, &quot;Mar&quot;) Usar una cadena de caracteres (o string, en inglés) para guardar esta variable tiene dos problemas: Sólo hay doce meses posibles, y no hay nada que te resguarde de errores de tipeo: x2 &lt;- c(&quot;Dic&quot;, &quot;Abr&quot;, &quot;Eme&quot;, &quot;Mar&quot;) No ordena de una forma útil: sort(x1) #&gt; [1] &quot;Abr&quot; &quot;Dic&quot; &quot;Ene&quot; &quot;Mar&quot; Puedes solucionar ambos problemas con un factor. Para crearlo debes empezar armando una lista de los niveles válidos: niveles_meses &lt;- c( &quot;Ene&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Abr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Ago&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dic&quot; ) Ahora puedes crear un factor: y1 &lt;- factor(x1, levels = niveles_meses) y1 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic sort(y1) #&gt; [1] Ene Mar Abr Dic #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic Y cualquier valor no fijado en el conjunto, será convertido a NA de forma silenciosa: y2 &lt;- factor(x2, levels = niveles_meses) y2 #&gt; [1] Dic Abr &lt;NA&gt; Mar #&gt; Levels: Ene Feb Mar Abr May Jun Jul Ago Sep Oct Nov Dic Si quieres una advertencia, puedes usar readr::parse_factor() (del inglés analizar gramaticalmente un factor): y2 &lt;- parse_factor(x2, levels = niveles_meses) #&gt; Warning: 1 parsing failure. #&gt; row col expected actual #&gt; 3 -- value in level set Eme Si omites los niveles, se van a tomar desde los datos en orden alfabético: factor(x1) #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Abr Dic Ene Mar En algunos momentos, es preferible que el orden de los niveles se corresponda con su primera aparición en los datos. Puedes hacer esto cuando creas el factor, al marcar los niveles con unique(x) (del inglés único), o después del hecho, con fct_inorder() (del inglés factores en orden): f1 &lt;- factor(x1, levels = unique(x1)) f1 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Dic Abr Ene Mar f2 &lt;- x1 %&gt;% factor() %&gt;% fct_inorder() f2 #&gt; [1] Dic Abr Ene Mar #&gt; Levels: Dic Abr Ene Mar Si alguna vez necesitas acceso directo al conjunto de niveles válidos, puedes hacerlo con levels() (del inglés niveles): levels(f2) #&gt; [1] &quot;Dic&quot; &quot;Abr&quot; &quot;Ene&quot; &quot;Mar&quot; 15.3 Encuesta social general Por el resto del capítulo, nos vamos a concentrar en encuesta. Ésta es la versión traducida al español de un conjunto de datos de ejemplo de General Social Survey; ésta es una encuesta realizada en Estados Unidos desde hace mucho tiempo, conducida por la organización de investigación independiente llamada NORC, en la Universidad de Chicago. La encuesta tiene miles de preguntas, así que en el conjunto de datos he seleccionado aquellas que ilustran algunos de los desafíos comunes que encontrarás al trabajar con factores. library(datos) encuesta #&gt; # A tibble: 21,483 x 9 #&gt; anio estado_civil edad raza ingreso partido religion denominacion #&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 2000 Nunca se ha… 26 Blan… $8000 … Indepe… Protest… Baptistas d… #&gt; 2 2000 Divorciado 48 Blan… $8000 … Republ… Protest… Baptista, n… #&gt; 3 2000 Viudo 67 Blan… Not ap… Indepe… Protest… No denomina… #&gt; 4 2000 Nunca se ha… 39 Blan… Not ap… Indepe… Cristia… No aplica #&gt; 5 2000 Divorciado 25 Blan… Not ap… Demócr… Ninguna No aplica #&gt; 6 2000 Casado 25 Blan… $20000… Demócr… Protest… Baptistas d… #&gt; # … with 2.148e+04 more rows, and 1 more variable: horas_tv &lt;int&gt; (Recuerda, dado que este conjunto de datos está provisto por un paquete, puedes obtener más información de las variables con ?encuesta.) Cuando los factores están almacenados en un tibble (se pronuncia /tibl/), no puedes ver sus niveles tán fácilmente. Una forma de verlos es con count() (del inglés contar): encuesta %&gt;% count(raza) #&gt; # A tibble: 3 x 2 #&gt; raza n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Otra 1959 #&gt; 2 Negra 3129 #&gt; 3 Blanca 16395 O con un gráfico de barras: ggplot(encuesta, aes(raza)) + geom_bar() Por defecto, ggplot2 retira los niveles que no tienen valores. Puedes forzarlos para que se visualicen con: ggplot(encuesta, aes(raza)) + geom_bar() + scale_x_discrete(drop = FALSE) Estos niveles representan valores válidos que simplemente no ocurren en este conjunto de datos. Desafortunadamente, dplyr no tiene una opción de drop (del inglés descartar, eliminar), pero lo hará en el futuro. Cuando se trabaja con factores, las dos operaciones más comunes son cambiar el orden de los niveles, y cambiar los valores de los niveles. Estas operaciones se describen en las siguientes secciones. 15.3.1 Ejercicios Explora la distribución de ingreso. ¿Qué hace que el gráfico de barras por defecto sea tan difícil de comprender? ¿Cómo podrías mejorarlo? ¿Cuál es la religion más común de la encuesta? ¿Cuál es el partido más común? ¿A qué religion se aplica cada denominacion? ¿Cómo puedes encontrarlo con una tabla? ¿Cómo lo puedes descubrir con una visualización? 15.4 Modificar el orden de los factores A menudo resulta útil cambiar el orden de los niveles de factores en una visualización. Por ejemplo, imagina que quieres explorar el número promedio de horas consumidas mirando televisión por día, para cada religión: resumen_religion &lt;- encuesta %&gt;% group_by(religion) %&gt;% summarise( edad = mean(edad, na.rm = TRUE), horas_tv = mean(horas_tv, na.rm = TRUE), n = n() ) ggplot(resumen_religion, aes(horas_tv, religion)) + geom_point() Este gráfico resulta dificil de interpretar porque no hay un patrón general. Podemos mejorarlo al ordenar los niveles de religion usando fct_reorder() (del inglés, reordenar factores). fct_reorder() requiere tres argumentos: f, el factor cuyos niveles quieres modificar. x, un vector numérico que quieres usar para reordenar los niveles. Opcionalmente, fun, una función que se usa si hay múltiples valores de x para cada valor de f. El valor por defecto es un median (del inglés, mediana). ggplot(resumen_religion, aes(horas_tv, fct_reorder(religion, horas_tv))) + geom_point() Reordenar la columna religión (religion) hace que sea más sencillo ver que las personas en la categoría “No same” ven más televisión, mientras que “Hinduismo” ven mucho menos. Cuando haces transformaciones más complicadas, yo recomiendo que las remuevas de aes() (del inglés, abreviatura de estética) hacia un paso de mutación separado, con mutate() (del inglés, mutar). Por ejemplo, puedes reescribir ese gráfico de la siguiente forma: resumen_religion %&gt;% mutate(religion = fct_reorder(religion, horas_tv)) %&gt;% ggplot(aes(horas_tv, religion)) + geom_point() ¿Qué sucede si creamos un gráfico para observar cómo varía la edad promedio para cada ingreso reportado? resumen_ingreso &lt;- encuesta %&gt;% group_by(ingreso) %&gt;% summarise( edad = mean(edad, na.rm = TRUE), horas_tv = mean(horas_tv, na.rm = TRUE), n = n() ) ggplot(resumen_ingreso, aes(edad, fct_reorder(ingreso, edad))) + geom_point() ¡Aquí, reordenar los niveles arbitrariamente no es una buena idea! Eso es porque ingreso ya tiene un orden basado en un principio determinado, con el cual no deberíamos meternos. Reserva fct_reorder() para factores cuyos niveles están ordenados arbitrariamente. Sin embargo, sí tiene sentido mover “No Aplica” al frente, junto a los otros niveles especiales. Puedes usar fct_relevel() (del inglés cambiar niveles). Ésta función recibe como argumento un factor, f, y luego cualquier número de niveles que quieres mover al principio de la línea. ggplot(resumen_ingreso, aes(edad, fct_relevel(ingreso, &quot;No aplica&quot;))) + geom_point() #&gt; Warning: Unknown levels in `f`: No aplica #&gt; Warning: Unknown levels in `f`: No aplica ¿Por qué crees que la edad promedio para “No aplica” es tan alta? Otro tipo de reordenamiento es útil cuando estás coloreando las líneas de un gráfico. fct_reorder2() (del inglés, reorganizar niveles número dos) reordena el factor mediante los valores y, asociados con los valores x más grandes. Esto hace que el gráfico sea más sencillo de leer, porque los colores de líneas se ajustan con la leyenda. por_edad &lt;- encuesta %&gt;% filter(!is.na(edad)) %&gt;% count(edad, estado_civil) %&gt;% group_by(edad) %&gt;% mutate(prop = n / sum(n)) ggplot(por_edad, aes(edad, prop, colour = estado_civil)) + geom_line(na.rm = TRUE) ggplot(por_edad, aes(edad, prop, colour = fct_reorder2(estado_civil, edad, prop))) + geom_line() + labs(colour = &quot;estado_civil&quot;) Finalmente, para los gráficos de barra, puedes usar fct_infreq() (del inglés, frecuencia incremental de factores) para ordenar los niveles incrementalmente según su frecuencia: este es el ordenamiento más sencillo porque no requiere de variables adicionales. Puedes querer combinarlo con fct_rev() (del inglés, revisar factores). encuesta %&gt;% mutate(estado_civil = estado_civil %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;% ggplot(aes(estado_civil)) + geom_bar() 15.4.1 Ejercicios Hay algunos números sospechosamente grandes en horas_tv. ¿Es la media un buen resumen? Identifica, para cada factor en encuesta, si el orden de los niveles es arbitrario o responde a algún principio. ¿Por qué mover “No aplica” al inicio de los niveles lo llevó al final del gráfico? 15.5 Modificar los niveles de los factores Más poderoso que cambiar el orden de los niveles es cambiar sus valores. Esto te permite clarificar etiquetas para publicación, y colapsar niveles para visualizaciones de alto nivel. La herramienta más general y más poderosa es fct_recode() (del inglés, recodificar factores). Ésta función te permite recodificar, o cambiar, el valor de cada nivel. Por ejemplo, toma la columna encuesta$partido: encuesta %&gt;% count(partido) #&gt; # A tibble: 10 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Sin respuesta 154 #&gt; 2 No sabe 1 #&gt; 3 Otro partido 393 #&gt; 4 Republicano duro 2314 #&gt; 5 Republicano moderano 3032 #&gt; 6 Independiente pro republicano 1791 #&gt; # … with 4 more rows Los niveles son tersos e inconsistentes. Hay que correjirlos un poco para que sean más largos, y poder usar una construcción paralela. encuesta %&gt;% mutate(partido = fct_recode(partido, &quot;Republicano duro&quot; = &quot;Republicano Acérrimo&quot;, &quot;Republicano moderado&quot; = &quot;Republicado No Acérrimo&quot;, &quot;Independiente pro republicano&quot; = &quot;Independiente, pro-Rep&quot;, &quot;Independiente pro demócrata&quot; = &quot;Independiente, pro-Dem&quot;, &quot;Demócrata moderado&quot; = &quot;Demócrata No Acérrimo&quot;, &quot;Demócrata duro&quot; = &quot;Demócrata Acérrimo&quot; )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 10 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Sin respuesta 154 #&gt; 2 No sabe 1 #&gt; 3 Otro partido 393 #&gt; 4 Republicano duro 2314 #&gt; 5 Republicano moderano 3032 #&gt; 6 Independiente pro republicano 1791 #&gt; # … with 4 more rows fct_recode() no modificará los niveles que no han sido mencionados explícitamente, y te advertirá si accidentalmente te refieres a un nivel que no existe. Para combinar grupos, puedes asignar múltiples niveles viejos, al mismo nivel nuevo: encuesta %&gt;% mutate(partido = fct_recode(partido, &quot;Republicano duro&quot; = &quot;Republicano Acérrimo&quot;, &quot;Republicano moderado&quot; = &quot;Republicado No Acérrimo&quot;, &quot;Independiente pro republicano&quot; = &quot;Independiente, pro-Rep&quot;, &quot;Independiente pro demócrata&quot; = &quot;Independiente, pro-Dem&quot;, &quot;Demócrata moderado&quot; = &quot;Demócrata No Acérrimo&quot;, &quot;Demócrata duro&quot; = &quot;Demócrata Acérrimo&quot;, &quot;Otro&quot; = &quot;Sin respuesta&quot;, &quot;Otro&quot; = &quot;No sabe&quot;, &quot;Otro&quot; = &quot;Otro partido&quot; )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 8 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Otro 548 #&gt; 2 Republicano duro 2314 #&gt; 3 Republicano moderano 3032 #&gt; 4 Independiente pro republicano 1791 #&gt; 5 Independiente 4119 #&gt; 6 Independiente pro demócrata 2499 #&gt; # … with 2 more rows Debes usar esta técnica con cuidado: si agrupas categorías que son realmente diferentes, obtendrás resultados confusos y/o engañosos. Si quieres colapsar muchos niveles, fct_collapse() (del inglés, colapsar factores) es una variante muy útil de fct_recode(). Para cada nueva variable, puedes proveer un vector de niveles viejos: encuesta %&gt;% mutate(partido = fct_collapse(partido, otro = c(&quot;Sin respuesta&quot;, &quot;No sabe&quot;, &quot;Otro partido&quot;), republicano = c(&quot;Republicano Acérrimo&quot;, &quot;Republicado No Acérrimo&quot;), independiente = c(&quot;Independiente, pro-Rep&quot;, &quot;Independiente&quot;, &quot;Independiente, pro-Dem&quot;), democrata = c(&quot;Demócrata No Acérrimo&quot;, &quot;Demócrata Acérrimo&quot;) )) %&gt;% count(partido) #&gt; Warning: Unknown levels in `f`: Republicano Acérrimo, Republicado No #&gt; Acérrimo, Independiente, pro-Rep, Independiente, pro-Dem, Demócrata No #&gt; Acérrimo, Demócrata Acérrimo #&gt; # A tibble: 8 x 2 #&gt; partido n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 otro 548 #&gt; 2 Republicano duro 2314 #&gt; 3 Republicano moderano 3032 #&gt; 4 Independiente pro republicano 1791 #&gt; 5 independiente 4119 #&gt; 6 Independiente pro demócrata 2499 #&gt; # … with 2 more rows A veces, simplemente quieres agrupar todos los grupos pequeños para simplificar un gráfico o tabla. Ese es un trabajo para fct_lump() (del inglés, agrupar factores): encuesta %&gt;% mutate(religion = fct_lump(religion)) %&gt;% count(religion) #&gt; # A tibble: 2 x 2 #&gt; religion n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestante 10846 #&gt; 2 Other 10637 El comportamiento por defecto es agrupar los grupos pequeños de forma progresiva, asegurando que la agregación continúa siendo el grupo más pequeño. En este caso, esto no resulta demasiado útil: es cierto que la mayoría de Americanos en esta encuesta son Protestantes, pero probablemente hemos colapsado en exceso. En cambio, podemos usar el parámetro n para especificar cuántos grupos (excluyendo otros) queremos colapsar: encuesta %&gt;% mutate(religion = fct_lump(religion, n = 10)) %&gt;% count(religion, sort = TRUE) %&gt;% print(n = Inf) #&gt; # A tibble: 11 x 2 #&gt; religion n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Protestante 10846 #&gt; 2 Católica 5124 #&gt; 3 Ninguna 3523 #&gt; 4 Cristiana 689 #&gt; 5 Judía 388 #&gt; 6 Other 234 #&gt; 7 Otra 224 #&gt; 8 Budismo 147 #&gt; 9 Inter o no confensional 109 #&gt; 10 Musulmana/Islam 104 #&gt; 11 Cristiana ortodoxa 95 15.5.1 Ejercicios ¿Cómo han cambiado en el tiempo las proporciones de personas que se identifican como Demócratas, Republicanas e Independientes? ¿Cómo podrías colapsar ingreso en un grupo más pequeño de categorías? "],
["horas-y-fechas.html", "16 Horas y fechas 16.1 Introducción 16.2 Creando horas/fechas 16.3 Componentes fecha-hora 16.4 Plazos de tiempo 16.5 Husos horarios", " 16 Horas y fechas 16.1 Introducción Este capítulo te mostrará cómo trabajar con fechas y horas en R. A primera vista, esto parece sencillo. Las usas en todo momento en tu vida regular, y no parecen causar demasiada confusión. Sin embargo, cuanto más aprendes de horas y fechas, más complicadas se vuelven. Para prepararnos, intenta estas preguntas sencillas: ¿Todos los años tienen 365 días? ¿Todos los días tienen 24 horas? ¿Cada minuto tiene 60 segundos? Estoy seguro que sabes que no todos los años tienen 365 días ¿pero acaso conoces la regla entera para determinar si un año es bisiesto? (Tiene tres partes, de hecho). Puedes recordar que muchas partes del mundo usan horarios de verano (las siglas en inglés son DST, por daylight savings time), así que algunos días tienen 23 horas, y otros tienen 25. Puede ser que no supieras que algunos minutos tienen 61 segundos, porque de vez en cuando se agregan segundos adicionales ya que la rotación de la tierra se hace cada vez más lenta. Las fechas y las horas son complicadas porque tienen que reconciliar dos fenómenos físicos (la rotación de la Tierra, y su órbita alrededor del sol), con todo un conjunto de fenómenos geopolíticos que incluyen a los meses, los husos horarios y los horarios de verano (DST). Este capítulo no te enseñará cada detalle sobre las horas y fechas, pero te dará un sólido fundamento de habilidades prácticas que te ayudarán con los desafíos más comunes de análisis de datos. 16.1.1 Requisitos previos Este capítulo se centra en el paquete lubridate, que simplifica el trabajo con fechas y tiempo en R. lubridate no es parte de los paquetes centrales de tidyverse porque sólo se necesita al trabajar con fechas/horas. A su vez, necesitaremos vuelos del conjunto de datos traducidos que acompañan a este libro. library(tidyverse) library(lubridate) library(datos) 16.2 Creando horas/fechas Hay tres tipos de datos de horas/fechas que se refieren a un instante en el tiempo: Un date o fecha (del inglés, fecha). Un tibble lo imprime como &lt;date&gt;. Un time u hora (del ingles, hora) dentro de un día. Los tibbles lo imprimen como &lt;time&gt;. Un date-time o fecha-hora (del inglés, fecha-hora) es una fecha con una hora adjunta: identifica de forma única como un instante en el tiempo (típicamente al segundo más cercano). Los tibbles imprimen esto como &lt;dttm&gt;. En otras partes de R, éstos se llaman POSIXct, pero yo no creo que sea un nombre muy útil. En este capítulo sólo nos concentraremos en fechas (date) y fechas con horas (date-time) ya que R no tiene una clase nativa para almacenar horas. Si necesitas una, puedes usar el paquete hms. Siempre deberías usar el tipo de datos más sencillo que se ajusta a tus necesidades. Esto significa que si puedes usar un date en lugar de un date-time, deberías hacerlo. Las fechas-horas son sustancialmente más complicadas porque necesitas gestionar los husos horarios, a los cuales estudiaremos al final del capítulo. Para obtener la fecha (date) o fecha-hora (date-time) actual, puedes usar today() (del inglés, hoy) o now() (del inglés, ahora): today() #&gt; [1] &quot;2019-01-19&quot; now() #&gt; [1] &quot;2019-01-19 18:47:15 UTC&quot; De otra forma, hay tres modos en los que puedes crear una fecha/hora: Desde una cadena de caracteres (o string, en inglés). Desde componentes date-time individuales. Desde un objeto fecha-hora existente. Éstos funcionan de la siguiente manera. 16.2.1 Desde cadenas de caracteres Los datos de fecha/hora a menudo vienen como cadenas de caracteres. Ya has visto una forma de análisis gramatical de strings hacia date-times en date-times. Otra forma es usar los ayudantes provistos por lubridate. Ellos automáticamente trabajan el formato una vez que especificas el orden de los componentes. Para usarlos, identifica el orden en el cual el año, mes y día aparecen en tus fechas, y luego agrega “y” (del inglés year), “m” (mes) y “d” (día) en el mismo orden. Esto te da el nombre de la función lubridate que traducirá tu fecha. Por ejemplo: ymd(&quot;2017-01-31&quot;) #&gt; [1] &quot;2017-01-31&quot; mdy(&quot;Enero 31, 2017&quot;) #&gt; Warning: All formats failed to parse. No formats found. #&gt; [1] NA dmy(&quot;31-Ene-2017&quot;) #&gt; Warning: All formats failed to parse. No formats found. #&gt; [1] NA Estas funciones también reciben números sin comillas. Esta es la forma más concisa de crear un sólo objeto fecha/hora, ya que puedes necesitarlo cuando filtres datos temporales. ymd() (del inglés año-mes-día) es corto y no ambigüo: ymd(20170131) #&gt; [1] &quot;2017-01-31&quot; ymd() y funciones amigas crean fechas (date). Para generar un date-time, agrega un guión bajo y uno o más de “h”, “m” y “s” al nombre de la función de análisis: ymd_hms(&quot;2017-01-31 20:11:59&quot;) #&gt; [1] &quot;2017-01-31 20:11:59 UTC&quot; mdy_hm(&quot;01/31/2017 08:01&quot;) #&gt; [1] &quot;2017-01-31 08:01:00 UTC&quot; También puedes forzar la creación de un date-time desde una fecha, al proveer un huso horario: ymd(20170131, tz = &quot;UTC&quot;) #&gt; [1] &quot;2017-01-31 UTC&quot; 16.2.2 Desde componentes individuales En lugar de una cadena de caracteres simple, a veces tienes los componentes individuales de una fecha/hora esparcidos en múltiples columnas. Esto es lo que tenemos en los datos de vuelos: vuelos %&gt;% select(anio, mes, dia, hora, minuto) #&gt; # A tibble: 336,776 x 5 #&gt; anio mes dia hora minuto #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 5 15 #&gt; 2 2013 1 1 5 29 #&gt; 3 2013 1 1 5 40 #&gt; 4 2013 1 1 5 45 #&gt; 5 2013 1 1 6 0 #&gt; 6 2013 1 1 5 58 #&gt; # … with 3.368e+05 more rows Para crear una fecha y hora desde este tipo de entrada, usa make_date() (del inglés, crear fechas) para las fechas, o make_datetime() (del inglés crear fecha-hora) para las fechas-horas: vuelos %&gt;% select(anio, mes, dia, hora, minuto) %&gt;% mutate(salida = make_datetime(anio, mes, dia, hora, minuto)) #&gt; # A tibble: 336,776 x 6 #&gt; anio mes dia hora minuto salida #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 2013 1 1 5 15 2013-01-01 05:15:00 #&gt; 2 2013 1 1 5 29 2013-01-01 05:29:00 #&gt; 3 2013 1 1 5 40 2013-01-01 05:40:00 #&gt; 4 2013 1 1 5 45 2013-01-01 05:45:00 #&gt; 5 2013 1 1 6 0 2013-01-01 06:00:00 #&gt; 6 2013 1 1 5 58 2013-01-01 05:58:00 #&gt; # … with 3.368e+05 more rows Hagamos esto mismo para cada una de las cuatro columnas de tiempo en vuelos. Las horas están representadas en un formato ligeramente más extraño, así que usamos el módulo aritmético para extraer los componentes de horas y minutos. Una vez que hayamos creado las variables date-time, nos centraremos en las variables que usaremos por el resto del capítulo1. hacer_fechahora_100 &lt;- function(anio, mes, dia, tiempo) { make_datetime(anio, mes, dia, tiempo %/% 100, tiempo %% 100) } vuelos_dt &lt;- vuelos %&gt;% filter(!is.na(horario_salida), !is.na(horario_llegada)) %&gt;% mutate( horario_salida = hacer_fechahora_100(anio, mes, dia, horario_salida), horario_llegada = hacer_fechahora_100(anio, mes, dia, horario_llegada), salida_programada = hacer_fechahora_100(anio, mes, dia, salida_programada), llegada_programada = hacer_fechahora_100(anio, mes, dia, llegada_programada) ) %&gt;% select(origen, destino, starts_with(&quot;atraso&quot;), starts_with(&quot;horario&quot;), ends_with(&quot;programada&quot;), tiempo_vuelo) vuelos_dt #&gt; # A tibble: 328,063 x 9 #&gt; origen destino atraso_salida atraso_llegada horario_salida #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 EWR IAH 2 11 2013-01-01 05:17:00 #&gt; 2 LGA IAH 4 20 2013-01-01 05:33:00 #&gt; 3 JFK MIA 2 33 2013-01-01 05:42:00 #&gt; 4 JFK BQN -1 -18 2013-01-01 05:44:00 #&gt; 5 LGA ATL -6 -25 2013-01-01 05:54:00 #&gt; 6 EWR ORD -4 12 2013-01-01 05:54:00 #&gt; # … with 3.281e+05 more rows, and 4 more variables: #&gt; # horario_llegada &lt;dttm&gt;, salida_programada &lt;dttm&gt;, #&gt; # llegada_programada &lt;dttm&gt;, tiempo_vuelo &lt;dbl&gt; Con estos datos, puedo visualizar la distribución de las horas de salida a lo largo del año: vuelos_dt %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 86400) # 86400 segundos = 1 día O para un solo día: vuelos_dt %&gt;% filter(horario_salida &lt; ymd(20130102)) %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 600) # 600 segundos = 10 minutos Mira con detenimiento, ya que cuando usas date-times en un contexto numérico (como en un histograma), 1 significa un segundo, entonces un binwidth (del inglés, ancho del cajón) de 86400 significa un día. Para las fechas, 1 significa un día. 16.2.3 Desde otros tipos Puedes querer cambiar entre una fecha-hora y una fecha. Ese es el trabajo de as_datetime() (del inglés, como fecha-hora) y as_date() (del inglés, como fecha): as_datetime(today()) #&gt; [1] &quot;2019-01-19 UTC&quot; as_date(now()) #&gt; [1] &quot;2019-01-19&quot; A veces, tendrás fechas/horas como desfasajes numéricos de la “Época Unix” similares a 1970-01-01. Si el desfasaje es un segundos, usa as_datetime(); si es en días, usa as_date(). as_datetime(60 * 60 * 10) #&gt; [1] &quot;1970-01-01 10:00:00 UTC&quot; as_date(365 * 10 + 2) #&gt; [1] &quot;1980-01-01&quot; 16.2.4 Ejercicios ¿Qué sucede si analizas una cadena de caracteres que contiene fechas inválidas? ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) ¿Qué hace el argumento tzone (del inglés, huso horario abreviado) para today()? ¿Por qué es importante? Usa la función de lubridate apropiada para analizar las siguientes fechas: d1 &lt;- &quot;Enero 1, 2010&quot; d2 &lt;- &quot;2015-Mar-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; d4 &lt;- c(&quot;Agosto 19 (2015)&quot;, &quot;Julio 1 (2015)&quot;) d5 &lt;- &quot;12/30/14&quot; # Diciembre 30, 2014 16.3 Componentes fecha-hora Ahora que ya conoces cómo tener datos de fechas y horas en las estructuras de datos de R, vamos a explorar qué puedes hacer con ellos. Esta sección se concentrará en las funciones de acceso que te permiten obtener y configurar componentes individuales. La siguiente sección mirará el trabajo aritmético con fechas-horas. 16.3.1 Obteniendo los componentes Puedes obtener las partes individuales de una fecha con las funciones de acceso year() (del inglés, año), month() (del inglés, mes), mday() (del inglés, día del mes), yday() (del inglés, día del año), wday() (del inglés, día de la semana), hour() (del inglés, hora), minute() (del inglés, minuto), y second() (del inglés, segundo). fechahora &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;) year(fechahora) #&gt; [1] 2016 month(fechahora) #&gt; [1] 7 mday(fechahora) #&gt; [1] 8 yday(fechahora) #&gt; [1] 190 wday(fechahora) #&gt; [1] 6 Para month() y wday() puedes configurar label = TRUE para retornar el nombre abreviado del mes o del día de la semana. Usa abbr = FALSE para retornar el nombre completo. month(fechahora, label = TRUE) #&gt; [1] Jul #&gt; 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec wday(fechahora, label = TRUE, abbr = FALSE) #&gt; [1] Friday #&gt; 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday Podemos usar wday() para ver que más vuelos salen durante la semana que durante el fin de semana: vuelos_dt %&gt;% mutate(dia_semana = wday(horario_salida, label = TRUE)) %&gt;% ggplot(aes(x = dia_semana)) + geom_bar() Hay un patrón interesante si miramos a la demora promedio por minuto dentro de la hora. ¡Parece que los vuelos que salen en los minutos 20-30 y 50-30 tienen mucho más demora que en el resto de las horas! vuelos_dt %&gt;% mutate(minuto = minute(horario_salida)) %&gt;% group_by(minuto) %&gt;% summarise( atraso_promedio = mean(atraso_llegada, na.rm = TRUE), n = n() ) %&gt;% ggplot(aes(minuto, atraso_promedio)) + geom_line() De forma interesante, si miramos al horario programado de salida, no vemos un patrón tan prominente: salida_programada &lt;- vuelos_dt %&gt;% mutate(minuto = minute(salida_programada)) %&gt;% group_by(minuto) %&gt;% summarise( atraso_promedio = mean(atraso_llegada, na.rm = TRUE), n = n() ) ggplot(salida_programada, aes(minuto, atraso_promedio)) + geom_line() Entonces ¿por qué vemos ese patrón con los horarios reales de salida? Bueno, como muchos datos recolectados por los humanos, hay un sesgo importante hacia los vuelos que salen en horas “lindas”. ¡Mantente siempre alerta respecto a este tipo de patrón, cuando sea que trabajes con datos que involucran al juicio humano! ggplot(salida_programada, aes(minuto, n)) + geom_line() 16.3.2 Redondeo Un método alternativo para graficar los componentes individuales es redondear la fecha a una unidad de tiempo cercana, con floor_date() (del inglés, redondear fecha hacia abajo), round_date() (del inglés, redondear fecha), y ceiling_date() (del inglés, redondear fecha hacia arriba). Cada función toma un vector de fechas para ajustarlas, y luego el nombre de la unidad redondeada hacia abajo (con floor), hacia arriba (con ceiling), o al mas cercano. Esto, por ejemplo, nos permite graficiar el número de vuelos por semana: vuelos_dt %&gt;% count(semana = floor_date(horario_salida, &quot;week&quot;)) %&gt;% ggplot(aes(semana, n)) + geom_line() Calcular la diferencia entre una fecha redondeada y una sin redondear puede ser particularmente útil. 16.3.3 Configurando componentes También puedes usar las funciones de acceso para darle un valor a los componentes de las fechas/horas: (fechahora &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;)) #&gt; [1] &quot;2016-07-08 12:34:56 UTC&quot; year(fechahora) &lt;- 2020 fechahora #&gt; [1] &quot;2020-07-08 12:34:56 UTC&quot; month(fechahora) &lt;- 01 fechahora #&gt; [1] &quot;2020-01-08 12:34:56 UTC&quot; hour(fechahora) &lt;- hour(fechahora) + 1 fechahora #&gt; [1] &quot;2020-01-08 13:34:56 UTC&quot; Alternativamente, en lugar de modificar en un sólo lugar, puedes crear una nueva fecha-hora con update() (del inglés, actualizar). Esto también te permite configurar múltiples valores al mismo tiempo. update(fechahora, year = 2020, month = 2, mday = 2, hour = 2) #&gt; [1] &quot;2020-02-02 02:34:56 UTC&quot; Si los valores son demasiado grandes, darán la vuelta: ymd(&quot;2015-02-01&quot;) %&gt;% update(mday = 30) #&gt; [1] &quot;2015-03-02&quot; ymd(&quot;2015-02-01&quot;) %&gt;% update(hour = 400) #&gt; [1] &quot;2015-02-17 16:00:00 UTC&quot; Puedes usar update() para mostrar la distribución de los vuelos a lo largo del día para cada día del año: vuelos_dt %&gt;% mutate(horario_salida = update(horario_salida, yday = 1)) %&gt;% ggplot(aes(horario_salida)) + geom_freqpoly(binwidth = 300) Fijar los componentes más grandes de una fecha con una constante es una técnica que te permite explorar patrones en los componentes más pequeños. 16.3.4 Ejercicios ¿Cómo cambia la distribución de las horas de los vuelos dentro de un día, a lo largo del año? Compara horario_salida, salida_programada and atraso_salida. ¿Son consistentes? Explica tus hallazgos. Compara tiempo_vuelo con la duración entre la salida y la llegada. Explica tus hallazgos. (Pista: considera la ubicación del aeropuerto). ¿Cómo cambia la demora promedio durante el curso de un día? ¿Deberías usar horario_salida o salida_programada? ¿Por qué? ¿En qué día de la semana deberías salir, si quieres minimizar las posibilidades de una demora? ¿Qué hace que la distribución de diamantes$carat y vuelos$salida_programada sean similares? Confirma mi hipótesis de que salidas programas en los minutos 20-30 y 50-60 están casuadas por los vuelos programados que salen más temprano. Pista: crea una variable binaria que te diga si un vuelo tuvo o no demora. 16.4 Plazos de tiempo Ahora, aprenderás cómo trabaja la aritmética con fechas, incluyendo la sustracción, adición y división. En el camino, aprenderás sobre tres importantes clases que representan períodos de tiempo: durations (del inglés, duraciones), que representa un número exacto de segundos. periods (del inglés, períodos), que representan unidades humanas como semanas o meses. intervals (del inglés, intervalos), que representan un punto de inicio y uno de finalización. 16.4.1 Duraciones En R, cuando restas dos fechas, obtienes un objeto de diferencia temporal (en inglés, difftimes): # ¿Cuán viejo es Hadley? edad_h &lt;- today() - ymd(19791014) edad_h #&gt; Time difference of 14342 days Un objeto de clase difftime registra un período de tiempo se segundos, minutos, horas, días o semanas. Esta ambiguedad hacia que los difftimes sean un poco complicados de trabjar, por lo que lubridate provee una alternativa que siempre usa segundos: la duration. as.duration(edad_h) #&gt; [1] &quot;1239148800s (~39.27 years)&quot; Las duraciones traen un conveniente grupo de constructores: dseconds(15) #&gt; [1] &quot;15s&quot; dminutes(10) #&gt; [1] &quot;600s (~10 minutes)&quot; dhours(c(12, 24)) #&gt; [1] &quot;43200s (~12 hours)&quot; &quot;86400s (~1 days)&quot; ddays(0:5) #&gt; [1] &quot;0s&quot; &quot;86400s (~1 days)&quot; &quot;172800s (~2 days)&quot; #&gt; [4] &quot;259200s (~3 days)&quot; &quot;345600s (~4 days)&quot; &quot;432000s (~5 days)&quot; dweeks(3) #&gt; [1] &quot;1814400s (~3 weeks)&quot; dyears(1) #&gt; [1] &quot;31536000s (~52.14 weeks)&quot; Las duraciones siempre registran el período de tiempo en segundos. Las unidades más grandes se crean al convertir minutos, horas, días, semanas y años a segundos, mediante una conversión estándar (60 segundos en un minuto, 60 minutos en una hora, 24 horas en un día, 7 días en una semana, 365 días en un año). Puedes agregar y multiplicar duraciones: 2 * dyears(1) #&gt; [1] &quot;63072000s (~2 years)&quot; dyears(1) + dweeks(12) + dhours(15) #&gt; [1] &quot;38847600s (~1.23 years)&quot; Puedes sumar y restar duraciones a días: ayer &lt;- today() + ddays(1) anio_pasado &lt;- today() - dyears(1) Sin embargo, como las duraciones representan un número exacto de segundos, a veces puedes obtener un resultado inesperado: una_pm &lt;- ymd_hms(&quot;2016-03-12 13:00:00&quot;, tz = &quot;America/New_York&quot;) una_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; una_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; ¿¡Por qué un día antes de la 1PM del 12 de Marzo, resulta ser las 2PM del 13 de Marzo!? Si miras con cuidado a la fecha, te darás cuenta que los husos horarios han cambiado. Debido al horario de verano (DST, o EDT para el horario de verano de la costa Este), el 12 de Marzo sólo tiene 23 horas, por lo que si agregamos un día entero de segundos, terminamos con una hora diferente. 16.4.2 Períodos Para resolver este problema, lubridate provee a periods (del inglés, períodos). Estos son plazos de tiempo que no tienen un largo fijo en segundos, sino que funcionan con tiempos “humanos”, como días o meses. Esto les permite trabajar en una forma más intuitiva: una_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; una_pm + days(1) #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; Como las duraciones, los períodos pueden ser creados mediante un número de funciones constructoras amigables. seconds(15) #&gt; [1] &quot;15S&quot; minutes(10) #&gt; [1] &quot;10M 0S&quot; hours(c(12, 24)) #&gt; [1] &quot;12H 0M 0S&quot; &quot;24H 0M 0S&quot; days(7) #&gt; [1] &quot;7d 0H 0M 0S&quot; months(1:6) #&gt; [1] &quot;1m 0d 0H 0M 0S&quot; &quot;2m 0d 0H 0M 0S&quot; &quot;3m 0d 0H 0M 0S&quot; &quot;4m 0d 0H 0M 0S&quot; #&gt; [5] &quot;5m 0d 0H 0M 0S&quot; &quot;6m 0d 0H 0M 0S&quot; weeks(3) #&gt; [1] &quot;21d 0H 0M 0S&quot; years(1) #&gt; [1] &quot;1y 0m 0d 0H 0M 0S&quot; Puedes sumar y multiplicar períodos: 10 * (months(6) + days(1)) #&gt; [1] &quot;60m 10d 0H 0M 0S&quot; days(50) + hours(25) + minutes(2) #&gt; [1] &quot;50d 25H 2M 0S&quot; Y, por supuesto, puedes sumarlos a las fechas. Comparados a las duraciones, los períodos son más propensos a hacer lo que esperas que hagan: # Un año bisiesto ymd(&quot;2016-01-01&quot;) + dyears(1) #&gt; [1] &quot;2016-12-31&quot; ymd(&quot;2016-01-01&quot;) + years(1) #&gt; [1] &quot;2017-01-01&quot; # Horarios de verano una_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; una_pm + days(1) #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; Usemos los períodos para arreglar una rareza relacionada a nuestras fechas de vuelos. Algunos aviones parecen arrivar a su destino antes de salir de la ciudad de Nueva York. vuelos_dt %&gt;% filter(horario_llegada &lt; horario_salida) #&gt; # A tibble: 10,633 x 9 #&gt; origen destino atraso_salida atraso_llegada horario_salida #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 EWR BQN 9 -4 2013-01-01 19:29:00 #&gt; 2 JFK DFW 59 NA 2013-01-01 19:39:00 #&gt; 3 EWR TPA -2 9 2013-01-01 20:58:00 #&gt; 4 EWR SJU -6 -12 2013-01-01 21:02:00 #&gt; 5 EWR SFO 11 -14 2013-01-01 21:08:00 #&gt; 6 LGA FLL -10 -2 2013-01-01 21:20:00 #&gt; # … with 1.063e+04 more rows, and 4 more variables: #&gt; # horario_llegada &lt;dttm&gt;, salida_programada &lt;dttm&gt;, #&gt; # llegada_programada &lt;dttm&gt;, tiempo_vuelo &lt;dbl&gt; Estos son vuelos nocturnos. Usamos la misma información de fecha para los horarios de salida y llegada, pero estos vuelos llegaron en el día siguiente. Podemos arreglarlo al sumar days(1) a la fecha de llegada de cada vuelo nocturno. vuelos_dt &lt;- vuelos_dt %&gt;% mutate( nocturno = horario_llegada &lt; horario_salida, horario_llegada = horario_llegada + days(nocturno * 1), llegada_programada = llegada_programada + days(nocturno * 1) ) Ahora todos los vuelos obedecen a las leyes de la física. vuelos_dt %&gt;% filter(nocturno, horario_llegada &lt; horario_salida) #&gt; # A tibble: 0 x 10 #&gt; # … with 10 variables: origen &lt;fct&gt;, destino &lt;fct&gt;, atraso_salida &lt;dbl&gt;, #&gt; # atraso_llegada &lt;dbl&gt;, horario_salida &lt;dttm&gt;, horario_llegada &lt;dttm&gt;, #&gt; # salida_programada &lt;dttm&gt;, llegada_programada &lt;dttm&gt;, #&gt; # tiempo_vuelo &lt;dbl&gt;, nocturno &lt;lgl&gt; 16.4.3 Intervalos Resulta obvio lo que dyears(1) / ddays(365) debería retornar: uno, porque las duraciones siempre se representan por un número de segundos, y la duración de un año se define como 365 días convertidos a segundos. ¿Qué debería devolver years(1) / days(1)? Bueno, si el año fuera 2015 debería retornar 365, ¡pero si fuera 2016 debería retornar 366! No hay suficiente información para que lubridate nos de una sóla respuesta sencilla. Entonces, lo que hace es darnos una estimación, con una advertencia: years(1) / days(1) #&gt; estimate only: convert to intervals for accuracy #&gt; [1] 365 Si quieres una medida más precisa, tendrás que usar un interval (del inglés, intervalo). Un intervalo es una duración con un punto de partida: eso lo hace preciso, porque puedes determinar exactamente cuán largo es: siguiente_anio &lt;- today() + years(1) (today() %--% siguiente_anio) / ddays(1) #&gt; [1] 365 Para encontrar cuántos períodos caen dentro de un intervalo, tienes que usar la división entera: (today() %--% siguiente_anio) %/% days(1) #&gt; Note: method with signature &#39;Timespan#Timespan&#39; chosen for function &#39;%/%&#39;, #&gt; target signature &#39;Interval#Period&#39;. #&gt; &quot;Interval#ANY&quot;, &quot;ANY#Period&quot; would also be valid #&gt; [1] 365 16.4.4 Resumen ¿Cómo eliges entre duraciones, períodos e intervalos? Como siempre, selecciona la estructura de datos más sencilla que resuelva tu problema. Si sólo necesitas tiempo físico, usa una duración; si necesitas agregar tiempos humanos, usa un período; si tienes que deducir cuán largo es un plazo en unidades humanas, usa un intervalo. La figura 16.1 resume las operaciones artiméticas permitidas entre los tipos de datos. Figure 16.1: Las operaciones artiméticas permitidas entre pares de clases fecha/hora. 16.4.5 Ejercicios ¿Por qué hay months() pero no dmonths() (del inglés, días del mes)? Explica days(nocturno * 1) a alguien que apenas comienza a aprender R. ¿Cómo funciona? Crea un vector de fechas dando el primer día de cada mes en 2015. Crea un vector de fechas dando el primer día de cada mes del año actual. Crea una función en la cual, dado tu cumpleaños (como una fecha), retorne cuán viejo eres en años. ¿Por qué no puedes hacer funcionar a (today() %--% (today() + years(1)) / months(1) ? 16.5 Husos horarios Los husos horarios son un tema enormemente complicado, debido a su interacción con entidades geopolíticas. Afortunadamente, no necesitamos excarbar en todos los detalles, ya que no son tan necesarios para el análisis de datos, pero hay algunos desafíos sobre los que tenemos que trabajar. El primer desafío es que los nombres comunes de los husos horarios tienden a ser ambiguos. Por ejemplo, si eres Americano probablemente estés familiarizado con EST, (siglas en inglés de Tiempo Este Estándar). Sin embargo, ¡Canadá y Australia también tienen EST! Para evitar la confusión, R usa el estándar internacional IANA para husos horarios. Estos tienen un esquema de nombres que sigue el formato “/”, típicamente escrito como “&lt;continente&gt;/&lt;ciudad&gt;”, en idioma inglés (hay algunas pocas excepciones porque no todos los países están ubicados en un continente). Ejemplos incluyen: “America/New_York”, “Europe/Paris”, and “Pacific/Auckland”. Puedes preguntarte por qué un huso horario usa una ciudad, cuando típicamente piensas en ellos como asociados a un país, o a una región dentro de un país. Esto es porque la base de datos de IANA tiene que registrar décadas de reglamentos sobre husos horarios. En el curso de las décadas, los países cambian nombres (o desaparecen) de forma bastante frecuente, pero los nombres de las ciudades tienden a mantenerse igual. Otro problema es que los nombres tienen que reflejar no sólo el comportamiento actual, sino también la historia completa. Por ejemplo, hay husos horarios tanto para “America/New_York” como para “America/Detroit”. Actualmente, ambas ciudades usan el EST, pero entre 1969 y 1972, Michigan (el estado en el cual Detroit está ubicado), no empleaba el horario de verano, así que necesita un nombre diferente. ¡Vale mucho leer la base de datos cruda (disponible en http://www.iana.org/time-zones) sólo para enterarse de algunas de estas historias! Puedes encontrar cuál es tu huso horario actual para R, usando Sys.timezone() (del inglés, sistema, huso horario): Sys.timezone() #&gt; [1] &quot;UTC&quot; (Si R no lo sabe, obtendrás un NA.) Y puedes ver la lista completa de todos los husos horarios con OlsonNames(): length(OlsonNames()) #&gt; [1] 606 head(OlsonNames()) #&gt; [1] &quot;Africa/Abidjan&quot; &quot;Africa/Accra&quot; &quot;Africa/Addis_Ababa&quot; #&gt; [4] &quot;Africa/Algiers&quot; &quot;Africa/Asmara&quot; &quot;Africa/Asmera&quot; En R, el huso horario es un atributo de la fecha-hora (date-time) que sólo controla la impresión. Por ejemplo, estos tres objetos representan el mismo instante en el tiempo: (x1 &lt;- ymd_hms(&quot;2015-06-01 12:00:00&quot;, tz = &quot;America/New_York&quot;)) #&gt; [1] &quot;2015-06-01 12:00:00 EDT&quot; (x2 &lt;- ymd_hms(&quot;2015-06-01 18:00:00&quot;, tz = &quot;Europe/Copenhagen&quot;)) #&gt; [1] &quot;2015-06-01 18:00:00 CEST&quot; (x3 &lt;- ymd_hms(&quot;2015-06-02 04:00:00&quot;, tz = &quot;Pacific/Auckland&quot;)) #&gt; [1] &quot;2015-06-02 04:00:00 NZST&quot; Puedes verificar que son lo mismo al usar una resta: x1 - x2 #&gt; Time difference of 0 secs x1 - x3 #&gt; Time difference of 0 secs Excepto que sea especificado claramente, lubridate siempre usa UTC. UTC significa Tiempo Universal Coordinado (por las siglas en inglés), y es el huso horario estándar empleado por la comunidad científica, y es aproximadamente equivalente a su predecesor GMT (Tiempo del Meridiano de Greenwich, por las siglas en inglés). No tiene horario de verano, por lo que resulta una representación conveniente para la computación. Las operaciones que combinan fechas y horas, como c(), a menudo descartan el huso horario. En ese caso, las fechas y horas se muestran en tu huso local: x4 &lt;- c(x1, x2, x3) x4 #&gt; [1] &quot;2015-06-01 12:00:00 EDT&quot; &quot;2015-06-01 12:00:00 EDT&quot; #&gt; [3] &quot;2015-06-01 12:00:00 EDT&quot; Puedes cambiar el huso horario de dos formas: Mantener el instante en el tiempo igual, y cambiar sólo cómo se representa. Usa esto cuando el instante es correcto, pero quieres una visualización más natural. x4a &lt;- with_tz(x4, tzone = &quot;Australia/Lord_Howe&quot;) x4a #&gt; [1] &quot;2015-06-02 02:30:00 +1030&quot; &quot;2015-06-02 02:30:00 +1030&quot; #&gt; [3] &quot;2015-06-02 02:30:00 +1030&quot; x4a - x4 #&gt; Time differences in secs #&gt; [1] 0 0 0 (Esto también ilustra otro desafío de los husos horarios: ¡no todos los desfasajes son horas como números enteros!) Cambia el instante en el tiempo. Usa esto cuando tienes un instante que ha sido etiquetado con un huso horario incorrecto, y necesitas arreglarlo. x4b &lt;- force_tz(x4, tzone = &quot;Australia/Lord_Howe&quot;) x4b #&gt; [1] &quot;2015-06-01 12:00:00 +1030&quot; &quot;2015-06-01 12:00:00 +1030&quot; #&gt; [3] &quot;2015-06-01 12:00:00 +1030&quot; x4b - x4 #&gt; Time differences in hours #&gt; [1] -14.5 -14.5 -14.5 N. del T.: El ultimo select de esta parte difiere del original o de lo contrario obtenemos un dataset con menos columnas que en el original haciendo que fallen los ejemplos posteriores. Esto se hizo ya que el castellano algunas palabras se invierten de orden.↩ "],
["programar-intro.html", "17 Introducción 17.1 Aprendiendo más", " 17 Introducción En esta parte del libro, mejorarás tus aptitudes de programación. La programación es una habilidad que cruza transversalmente todos los trabajos de ciencia de datos: deberás usar una computadora para hacer ciencia de datos, no puedes hacerlo sólo con tu cabeza o con lápiz y papel. Programar produce código, y el código es una herramienta de comunicación. Obviamente el código le dice a la computadora qué es lo que quieres que haga, pero también comunica el significado a otros humanos. Es importante pensar el código como un medio de comunicación, porque todo proyecto que realices es esencialmente colaborativo. Aún cuando no estés trabajando con otras personas, seguramente en el futuro trabajarás con ese mismo código. Por eso es necesario escribir código claro, para que otros (o tú en el futuro) puedan entender por qué encaraste un análisis de la manera que lo hiciste. Esto significa que mejorando la programación también mejorarás la comunicación. Pasado el tiempo, querrás que tu código resulte no solo fácil de escribir, sino también fácil de leer para otros. Escribir código es similar, en muchas formas, a escribir prosa. Un paralelismo que encuentro particularmente útil es que en ambos casos reescribir es clave para la claridad. La primera expresión de tus ideas difícilmente sea clara, y posiblemente necesitarás reescribir muchas veces. Después de resolver el problema de análisis de datos, en general es mejor mirar tu código y pensar si es obvio o no lo que has hecho. Si utilizas un poco de tiempo reescribiendo tu código mientras las ideas están frescas, puedes ganar mucho tiempo después, cuando necesites recrear el código que hiciste. Esto no significa que debas reescribir todas las funciones: necesitas equilibrar qué es lo que tienes que mejorar ahora para ganar tiempo a largo plazo. (Pero cuanto más reescribas tus funciones, más probable será que tu primer intento sea cada vez más claro). En los próximos cuatro capítulos, aprenderás habilidades que te permitirán abordar nuevos programas como también resolver problemas ya existentes de manera más sencilla y con mayor claridad: En [pipes], navegarás dentro del pipe, %&gt;%, y aprenderás más sobre cómo trabaja, qué alternativas existen y cuándo no conviene usarlo. Copiar y pegar (copy-and-paste) es muy práctico, pero deberías evitar usarlo más de dos veces. Repetir el código dentro de un programa es peligroso porque puede llevar a errores e inconsistencias. Por eso, en [funciones], aprenderás a escribir funciones que permiten extraer código repertido y convertirlo en código fácilmente reutilizable. A medida que comiences a escribir funciones más potentes, necesitarás una base sólida en las estructuras de datos de R, que te proporcionará vectores. Deberás dominar los cuatro tipos de vectores atómicos clásicos, también las tres clases S3 construídas sobre ellos, y entender los misterios de las listas y los data frame. Las funciones previenen que repitamos código, pero muchas veces necesitas repertir las mismas acciones con diferentes entradas. Necesitas, entonces, herramientas de iteración que te permitan hacer cosas similares una y otra vez. Estas herramientas incluyen ciclos (loops) y programación funcional que aprenderás en [iteración]. 17.1 Aprendiendo más El objetivo de estos capítulos es enseñarte lo básico de programación que necesitas para hacer ciencia de datos, que resulta una cantidad considerable de información. Una vez que hayas dominado el material de este libro, creo firmemente que deberías invertir tiempo en mejorar tus habilidades de programación. Aprender más sobre programación es una inversión a largo plazo: no obtendrás resultados inmediatos, pero con el tiempo te permitirá resolver nuevos problemas más rápidamente, y te permitirá reutilizar tus ideas de problemas anteriores en nuevos escenarios. Para profundizar necesitas estudiar R como un lenguaje de programación, no sólo como un ambiente interactivo para ciencia de datos. Escribimos dos libros que te ayudarán en ese sentido. Estos libros sólo están disponibles en inglés: Hands on Programming with R, por Garrett Grolemund. Ésta es una introducción a R como lenguaje de programación y es un buen lugar para empezar si R es tu primer lenguaje de programación. Cubre casi el mismo material que estos capítulos, pero con un estilo diferente y con distintos ejemplos de motivación (basados en el casino). Es un complemento muy útil si consideras que estos cuatros capítulos van demasiado rápido. Advanced R por Hadley Wickham. Este libro bucea en los detalles del lenguaje de programación R. Éste es un buen punto por donde empezar si ya tienes experiencia en programación. Es también una buena manera de seguir una vez que hayas internalizado las ideas de estos cuatro capítulos. Puedes leerlo online en el sitio http://adv-r.had.co.nz. "],
["pipes.html", "18 Pipes 18.1 Introduction 18.2 Piping alternatives 18.3 When not to use the pipe 18.4 Other tools from magrittr", " 18 Pipes 18.1 Introduction Pipes are a powerful tool for clearly expressing a sequence of multiple operations. So far, you’ve been using them without knowing how they work, or what the alternatives are. Now, in this chapter, it’s time to explore the pipe in more detail. You’ll learn the alternatives to the pipe, when you shouldn’t use the pipe, and some useful related tools. 18.1.1 Prerequisites The pipe, %&gt;%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %&gt;% for you automatically, so you don’t usually load magrittr explicitly. Here, however, we’re focussing on piping, and we aren’t loading any other packages, so we will load it explicitly. library(magrittr) 18.2 Piping alternatives The point of the pipe is to help you write code in a way that is easier to read and understand. To see why the pipe is so useful, we’re going to explore a number of ways of writing the same code. Let’s use code to tell a story about a little bunny named Foo Foo: Little bunny Foo Foo Went hopping through the forest Scooping up the field mice And bopping them on the head This is a popular Children’s poem that is accompanied by hand actions. We’ll start by defining an object to represent little bunny Foo Foo: foo_foo &lt;- little_bunny() And we’ll use a function for each key verb: hop(), scoop(), and bop(). Using this object and these verbs, there are (at least) four ways we could retell the story in code: Save each intermediate step as a new object. Overwrite the original object many times. Compose functions. Use the pipe. We’ll work through each approach, showing you the code and talking about the advantages and disadvantages. 18.2.1 Intermediate steps The simplest approach is to save each step as a new object: foo_foo_1 &lt;- hop(foo_foo, through = forest) foo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice) foo_foo_3 &lt;- bop(foo_foo_2, on = head) The main downside of this form is that it forces you to name each intermediate element. If there are natural names, this is a good idea, and you should do it. But many times, like this in this example, there aren’t natural names, and you add numeric suffixes to make the names unique. That leads to two problems: The code is cluttered with unimportant names You have to carefully increment the suffix on each line. Whenever I write code like this, I invariably use the wrong number on one line and then spend 10 minutes scratching my head and trying to figure out what went wrong with my code. You may also worry that this form creates many copies of your data and takes up a lot of memory. Surprisingly, that’s not the case. First, note that proactively worrying about memory is not a useful way to spend your time: worry about it when it becomes a problem (i.e. you run out of memory), not before. Second, R isn’t stupid, and it will share columns across data frames, where possible. Let’s take a look at an actual data manipulation pipeline where we add a new column to ggplot2::diamonds: diamonds &lt;- ggplot2::diamonds diamonds2 &lt;- diamonds %&gt;% dplyr::mutate(price_per_carat = price / carat) pryr::object_size(diamonds) #&gt; 3.46 MB pryr::object_size(diamonds2) #&gt; 3.89 MB pryr::object_size(diamonds, diamonds2) #&gt; 3.89 MB pryr::object_size() gives the memory occupied by all of its arguments. The results seem counterintuitive at first: diamonds takes up 3.46 MB, diamonds2 takes up 3.89 MB, diamonds and diamonds2 together take up 3.89 MB! How can that work? Well, diamonds2 has 10 columns in common with diamonds: there’s no need to duplicate all that data, so the two data frames have variables in common. These variables will only get copied if you modify one of them. In the following example, we modify a single value in diamonds$carat. That means the carat variable can no longer be shared between the two data frames, and a copy must be made. The size of each data frame is unchanged, but the collective size increases: diamonds$carat[1] &lt;- NA pryr::object_size(diamonds) #&gt; 3.46 MB pryr::object_size(diamonds2) #&gt; 3.89 MB pryr::object_size(diamonds, diamonds2) #&gt; 4.32 MB (Note that we use pryr::object_size() here, not the built-in object.size(). object.size() only takes a single object so it can’t compute how data is shared across multiple objects.) 18.2.2 Overwrite the original Instead of creating intermediate objects at each step, we could overwrite the original object: foo_foo &lt;- hop(foo_foo, through = forest) foo_foo &lt;- scoop(foo_foo, up = field_mice) foo_foo &lt;- bop(foo_foo, on = head) This is less typing (and less thinking), so you’re less likely to make mistakes. However, there are two problems: Debugging is painful: if you make a mistake you’ll need to re-run the complete pipeline from the beginning. The repetition of the object being transformed (we’ve written foo_foo six times!) obscures what’s changing on each line. 18.2.3 Function composition Another approach is to abandon assignment and just string the function calls together: bop( scoop( hop(foo_foo, through = forest), up = field_mice ), on = head ) Here the disadvantage is that you have to read from inside-out, from right-to-left, and that the arguments end up spread far apart (evocatively called the dagwood sandwhich problem). In short, this code is hard for a human to consume. 18.2.4 Use the pipe Finally, we can use the pipe: foo_foo %&gt;% hop(through = forest) %&gt;% scoop(up = field_mice) %&gt;% bop(on = head) This is my favourite form, because it focusses on verbs, not nouns. You can read this series of function compositions like it’s a set of imperative actions. Foo Foo hops, then scoops, then bops. The downside, of course, is that you need to be familiar with the pipe. If you’ve never seen %&gt;% before, you’ll have no idea what this code does. Fortunately, most people pick up the idea very quickly, so when you share your code with others who aren’t familiar with the pipe, you can easily teach them. The pipe works by performing a “lexical transformation”: behind the scenes, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. When you run a pipe like the one above, magrittr does something like this: my_pipe &lt;- function(.) { . &lt;- hop(., through = forest) . &lt;- scoop(., up = field_mice) bop(., on = head) } my_pipe(foo_foo) This means that the pipe won’t work for two classes of functions: Functions that use the current environment. For example, assign() will create a new variable with the given name in the current environment: assign(&quot;x&quot;, 10) x #&gt; [1] 10 &quot;x&quot; %&gt;% assign(100) x #&gt; [1] 10 The use of assign with the pipe does not work because it assigns it to a temporary environment used by %&gt;%. If you do want to use assign with the pipe, you must be explicit about the environment: env &lt;- environment() &quot;x&quot; %&gt;% assign(100, envir = env) x #&gt; [1] 100 Other functions with this problem include get() and load(). Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behaviour. One place that this is a problem is tryCatch(), which lets you capture and handle errors: tryCatch(stop(&quot;!&quot;), error = function(e) &quot;An error&quot;) #&gt; [1] &quot;An error&quot; stop(&quot;!&quot;) %&gt;% tryCatch(error = function(e) &quot;An error&quot;) #&gt; Error in eval(lhs, parent, parent): ! There are a relatively wide class of functions with this behaviour, including try(), suppressMessages(), and suppressWarnings() in base R. 18.3 When not to use the pipe The pipe is a powerful tool, but it’s not the only tool at your disposal, and it doesn’t solve every problem! Pipes are most useful for rewriting a fairly short linear sequence of operations. I think you should reach for another tool when: Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent. You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe. You are starting to think about a directed graph with a complex dependency structure. Pipes are fundamentally linear and expressing complex relationships with them will typically yield confusing code. 18.4 Other tools from magrittr All packages in the tidyverse automatically make %&gt;% available for you, so you don’t normally load magrittr explicitly. However, there are some other useful tools inside magrittr that you might want to try out: When working with more complex pipes, it’s sometimes useful to call a function for its side-effects. Maybe you want to print out the current object, or plot it, or save it to disk. Many times, such functions don’t return anything, effectively terminating the pipe. To work around this problem, you can use the “tee” pipe. %T&gt;% works like %&gt;% except that it returns the left-hand side instead of the right-hand side. It’s called “tee” because it’s like a literal T-shaped pipe. rnorm(100) %&gt;% matrix(ncol = 2) %&gt;% plot() %&gt;% str() #&gt; NULL rnorm(100) %&gt;% matrix(ncol = 2) %T&gt;% plot() %&gt;% str() #&gt; num [1:50, 1:2] -0.387 -0.785 -1.057 -0.796 -1.756 ... If you’re working with functions that don’t have a data frame based API (i.e. you pass them individual vectors, not a data frame and expressions to be evaluated in the context of that data frame), you might find %$% useful. It “explodes” out the variables in a data frame so that you can refer to them explicitly. This is useful when working with many functions in base R: mtcars %$% cor(disp, mpg) #&gt; [1] -0.848 For assignment magrittr provides the %&lt;&gt;% operator which allows you to replace code like: mtcars &lt;- mtcars %&gt;% transform(cyl = cyl * 2) with mtcars %&lt;&gt;% transform(cyl = cyl * 2) I’m not a fan of this operator because I think assignment is such a special operation that it should always be clear when it’s occurring. In my opinion, a little bit of duplication (i.e. repeating the name of the object twice) is fine in return for making assignment more explicit. "],
["functions.html", "19 Functions 19.1 Introduction 19.2 When should you write a function? 19.3 Functions are for humans and computers 19.4 Conditional execution 19.5 Function arguments 19.6 Return values 19.7 Environment", " 19 Functions 19.1 Introduction One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste: You can give a function an evocative name that makes your code easier to understand. As requirements change, you only need to update code in one place, instead of many. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another). Writing good functions is a lifetime journey. Even after using R for many years I still learn new techniques and better ways of approaching old problems. The goal of this chapter is not to teach you every esoteric detail of functions but to get you started with some pragmatic advice that you can apply immediately. As well as practical advice for writing functions, this chapter also gives you some suggestions for how to style your code. Good code style is like correct punctuation. Youcanmanagewithoutit, but it sure makes things easier to read! As with styles of punctuation, there are many possible variations. Here we present the style we use in our code, but the most important thing is to be consistent. 19.1.1 Prerequisites The focus of this chapter is on writing functions in base R, so you won’t need any extra packages. 19.2 When should you write a function? You should consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). For example, take a look at this code. What does it do? df &lt;- tibble::tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) df$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE)) df$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE)) df$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE)) You might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? I made an error when copying-and-pasting the code for df$b: I forgot to change an a to a b. Extracting repeated code out into a function is a good idea because it prevents you from making this type of mistake. To write a function you need to first analyse the code. How many inputs does it have? (df$a - min(df$a, na.rm = TRUE)) / (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE)) This code only has one input: df$a. (If you’re surprised that TRUE is not an input, you can explore why in the exercise below.) To make the inputs more clear, it’s a good idea to rewrite the code using temporary variables with general names. Here this code only requires a single numeric vector, so I’ll call it x: x &lt;- df$a (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) #&gt; [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601 There is some duplication in this code. We’re computing the range of the data three times, so it makes sense to do it in one step: rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) #&gt; [1] 0.289 0.751 0.000 0.678 0.853 1.000 0.172 0.611 0.612 0.601 Pulling out intermediate calculations into named variables is a good practice because it makes it more clear what the code is doing. Now that I’ve simplified the code, and checked that it still works, I can turn it into a function: rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01(c(0, 5, 10)) #&gt; [1] 0.0 0.5 1.0 There are three key steps to creating a new function: You need to pick a name for the function. Here I’ve used rescale01 because this function rescales a vector to lie between 0 and 1. You list the inputs, or arguments, to the function inside function. Here we have just one argument. If we had more the call would look like function(x, y, z). You place the code you have developed in body of the function, a { block that immediately follows function(...). Note the overall process: I only made the function after I’d figured out how to make it work with a simple input. It’s easier to start with working code and turn it into a function; it’s harder to create a function and then try to make it work. At this point it’s a good idea to check your function with a few different inputs: rescale01(c(-10, 0, 10)) #&gt; [1] 0.0 0.5 1.0 rescale01(c(1, 2, 3, NA, 5)) #&gt; [1] 0.00 0.25 0.50 NA 1.00 As you write more and more functions you’ll eventually want to convert these informal, interactive tests into formal, automated tests. That process is called unit testing. Unfortunately, it’s beyond the scope of this book, but you can learn about it in http://r-pkgs.had.co.nz/tests.html. We can simplify the original example now that we have a function: df$a &lt;- rescale01(df$a) df$b &lt;- rescale01(df$b) df$c &lt;- rescale01(df$c) df$d &lt;- rescale01(df$d) Compared to the original, this code is easier to understand and we’ve eliminated one class of copy-and-paste errors. There is still quite a bit of duplication since we’re doing the same thing to multiple columns. We’ll learn how to eliminate that duplication in iteration, once you’ve learned more about R’s data structures in [vectors]. Another advantage of functions is that if our requirements change, we only need to make the change in one place. For example, we might discover that some of our variables include infinite values, and rescale01() fails: x &lt;- c(1:10, Inf) rescale01(x) #&gt; [1] 0 0 0 0 0 0 0 0 0 0 NaN Because we’ve extracted the code into a function, we only need to make the fix in one place: rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE, finite = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } rescale01(x) #&gt; [1] 0.000 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000 Inf This is an important part of the “do not repeat yourself” (or DRY) principle. The more repetition you have in your code, the more places you need to remember to update when things change (and they always do!), and the more likely you are to create bugs over time. 19.2.1 Practice Why is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE? In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1. Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative? mean(is.na(x)) x / sum(x, na.rm = TRUE) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) Follow http://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector. Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors. What do the following functions do? Why are they useful even though they are so short? is_directory &lt;- function(x) file.info(x)$isdir is_readable &lt;- function(x) file.access(x, 4) == 0 Read the complete lyrics to “Little Bunny Foo Foo”. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication. 19.3 Functions are for humans and computers It’s important to remember that functions are not just for the computer, but are also for humans. R doesn’t care what your function is called, or what comments it contains, but these are important for human readers. This section discusses some things that you should bear in mind when writing functions that humans can understand. The name of a function is important. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names. Generally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). A good sign that a noun might be a better choice is if you’re using a very broad verb like “get”, “compute”, “calculate”, or “determine”. Use your best judgement and don’t be afraid to rename a function if you figure out a better name later. # Too short f() # Not a verb, or descriptive my_awesome_function() # Long, but clear impute_missing() collapse_years() If your function name is composed of multiple words, I recommend using “snake_case”, where each lowercase word is separated by an underscore. camelCase is a popular alternative. It doesn’t really matter which one you pick, the important thing is to be consistent: pick one or the other and stick with it. R itself is not very consistent, but there’s nothing you can do about that. Make sure you don’t fall into the same trap by making your code as consistent as possible. # Never do this! col_mins &lt;- function(x, y) {} rowMaxes &lt;- function(y, x) {} If you have a family of functions that do similar things, make sure they have consistent names and arguments. Use a common prefix to indicate that they are connected. That’s better than a common suffix because autocomplete allows you to type the prefix and see all the members of the family. # Good input_select() input_checkbox() input_text() # Not so good select_input() checkbox_input() text_input() A good example of this design is the stringr package: if you don’t remember exactly which function you need, you can type str_ and jog your memory. Where possible, avoid overriding existing functions and variables. It’s impossible to do in general because so many good names are already taken by other packages, but avoiding the most common names from base R will avoid confusion. # Don&#39;t do this! T &lt;- FALSE c &lt;- 10 mean &lt;- function(x) sum(x) Use comments, lines starting with #, to explain the “why” of your code. You generally should avoid comments that explain the “what” or the “how”. If you can’t understand what the code does from reading it, you should think about how to rewrite it to be more clear. Do you need to add some intermediate variables with useful names? Do you need to break out a subcomponent of a large function so you can name it? However, your code can never capture the reasoning behind your decisions: why did you choose this approach instead of an alternative? What else did you try that didn’t work? It’s a great idea to capture that sort of thinking in a comment. Another important use of comments is to break up your file into easily readable chunks. Use long lines of - and = to make it easy to spot the breaks. # Load data -------------------------------------- # Plot data -------------------------------------- RStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl + Shift + R), and will display them in the code navigation drop-down at the bottom-left of the editor: 19.3.1 Exercises Read the source code for each of the following three functions, puzzle out what they do, and then brainstorm better names. f1 &lt;- function(string, prefix) { substr(string, 1, nchar(prefix)) == prefix } f2 &lt;- function(x) { if (length(x) &lt;= 1) return(NULL) x[-length(x)] } f3 &lt;- function(x, y) { rep(y, length.out = length(x)) } Take a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments. Compare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent? Make a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite. 19.4 Conditional execution An if statement allows you to conditionally execute code. It looks like this: if (condition) { # code executed when condition is TRUE } else { # code executed when condition is FALSE } To get help on if you need to surround it in backticks: ?`if`. The help isn’t particularly helpful if you’re not already an experienced programmer, but at least you know how to get to it! Here’s a simple function that uses an if statement. The goal of this function is to return a logical vector describing whether or not each element of a vector is named. has_name &lt;- function(x) { nms &lt;- names(x) if (is.null(nms)) { rep(FALSE, length(x)) } else { !is.na(nms) &amp; nms != &quot;&quot; } } This function takes advantage of the standard return rule: a function returns the last value that it computed. Here that is either one of the two branches of the if statement. 19.4.1 Conditions The condition must evaluate to either TRUE or FALSE. If it’s a vector, you’ll get a warning message; if it’s an NA, you’ll get an error. Watch out for these messages in your own code: if (c(TRUE, FALSE)) {} #&gt; Warning in if (c(TRUE, FALSE)) {: the condition has length &gt; 1 and only the #&gt; first element will be used #&gt; NULL if (NA) {} #&gt; Error in if (NA) {: missing value where TRUE/FALSE needed You can use || (or) and &amp;&amp; (and) to combine multiple logical expressions. These operators are “short-circuiting”: as soon as || sees the first TRUE it returns TRUE without computing anything else. As soon as &amp;&amp; sees the first FALSE it returns FALSE. You should never use | or &amp; in an if statement: these are vectorised operations that apply to multiple values (that’s why you use them in filter()). If you do have a logical vector, you can use any() or all() to collapse it to a single value. Be careful when testing for equality. == is vectorised, which means that it’s easy to get more than one output. Either check the length is already 1, collapse with all() or any(), or use the non-vectorised identical(). identical() is very strict: it always returns either a single TRUE or a single FALSE, and doesn’t coerce types. This means that you need to be careful when comparing integers and doubles: identical(0L, 0) #&gt; [1] FALSE You also need to be wary of floating point numbers: x &lt;- sqrt(2) ^ 2 x #&gt; [1] 2 x == 2 #&gt; [1] FALSE x - 2 #&gt; [1] 4.44e-16 Instead use dplyr::near() for comparisons, as described in comparisons. And remember, x == NA doesn’t do anything useful! 19.4.2 Multiple conditions You can chain multiple if statements together: if (this) { # do that } else if (that) { # do something else } else { # } But if you end up with a very long series of chained if statements, you should consider rewriting. One useful technique is the switch() function. It allows you to evaluate selected code based on position or name. #&gt; function(x, y, op) { #&gt; switch(op, #&gt; plus = x + y, #&gt; minus = x - y, #&gt; times = x * y, #&gt; divide = x / y, #&gt; stop(&quot;Unknown op!&quot;) #&gt; ) #&gt; } Another useful function that can often eliminate long chains of if statements is cut(). It’s used to discretise continuous variables. 19.4.3 Code style Both if and function should (almost) always be followed by squiggly brackets ({}), and the contents should be indented by two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin. An opening curly brace should never go on its own line and should always be followed by a new line. A closing curly brace should always go on its own line, unless it’s followed by else. Always indent the code inside curly braces. # Good if (y &lt; 0 &amp;&amp; debug) { message(&quot;Y is negative&quot;) } if (y == 0) { log(x) } else { y ^ x } # Bad if (y &lt; 0 &amp;&amp; debug) message(&quot;Y is negative&quot;) if (y == 0) { log(x) } else { y ^ x } It’s ok to drop the curly braces if you have a very short if statement that can fit on one line: y &lt;- 10 x &lt;- if (y &lt; 20) &quot;Too low&quot; else &quot;Too high&quot; I recommend this only for very brief if statements. Otherwise, the full form is easier to read: if (y &lt; 20) { x &lt;- &quot;Too low&quot; } else { x &lt;- &quot;Too high&quot; } 19.4.4 Exercises What’s the difference between if and ifelse()? Carefully read the help and construct three examples that illustrate the key differences. Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.) Implement a fizzbuzz function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function. How could you use cut() to simplify this set of nested if-else statements? if (temp &lt;= 0) { &quot;freezing&quot; } else if (temp &lt;= 10) { &quot;cold&quot; } else if (temp &lt;= 20) { &quot;cool&quot; } else if (temp &lt;= 30) { &quot;warm&quot; } else { &quot;hot&quot; } How would you change the call to cut() if I’d used &lt; instead of &lt;=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?) What happens if you use switch() with numeric values? What does this switch() call do? What happens if x is “e”? switch(x, a = , b = &quot;ab&quot;, c = , d = &quot;cd&quot; ) Experiment, then carefully read the documentation. 19.5 Function arguments The arguments to a function typically fall into two broad sets: one set supplies the data to compute on, and the other supplies arguments that control the details of the computation. For example: In log(), the data is x, and the detail is the base of the logarithm. In mean(), the data is x, and the details are how much data to trim from the ends (trim) and how to handle missing values (na.rm). In t.test(), the data are x and y, and the details of the test are alternative, mu, paired, var.equal, and conf.level. In str_c() you can supply any number of strings to ..., and the details of the concatenation are controlled by sep and collapse. Generally, data arguments should come first. Detail arguments should go on the end, and usually should have default values. You specify a default value in the same way you call a function with a named argument: # Compute confidence interval around mean using normal approximation mean_ci &lt;- function(x, conf = 0.95) { se &lt;- sd(x) / sqrt(length(x)) alpha &lt;- 1 - conf mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2)) } x &lt;- runif(100) mean_ci(x) #&gt; [1] 0.498 0.610 mean_ci(x, conf = 0.99) #&gt; [1] 0.480 0.628 The default value should almost always be the most common value. The few exceptions to this rule are to do with safety. For example, it makes sense for na.rm to default to FALSE because missing values are important. Even though na.rm = TRUE is what you usually put in your code, it’s a bad idea to silently ignore missing values by default. When you call a function, you typically omit the names of the data arguments, because they are used so commonly. If you override the default value of a detail argument, you should use the full name: # Good mean(1:10, na.rm = TRUE) # Bad mean(x = 1:10, , FALSE) mean(, TRUE, x = c(1:10, NA)) You can refer to an argument by its unique prefix (e.g. mean(x, n = TRUE)), but this is generally best avoided given the possibilities for confusion. Notice that when you call a function, you should place a space around = in function calls, and always put a space after a comma, not before (just like in regular English). Using whitespace makes it easier to skim the function for the important components. # Good average &lt;- mean(feet / 12 + inches, na.rm = TRUE) # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) 19.5.1 Choosing names The names of the arguments are also important. R doesn’t care, but the readers of your code (including future-you!) will. Generally you should prefer longer, more descriptive names, but there are a handful of very common, very short names. It’s worth memorising these: x, y, z: vectors. w: a vector of weights. df: a data frame. i, j: numeric indices (typically rows and columns). n: length, or number of rows. p: number of columns. Otherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed. 19.5.2 Checking values As you start to write more functions, you’ll eventually get to the point where you don’t remember exactly how your function works. At this point it’s easy to call your function with invalid inputs. To avoid this problem, it’s often useful to make constraints explicit. For example, imagine you’ve written some functions for computing weighted summary statistics: wt_mean &lt;- function(x, w) { sum(x * w) / sum(w) } wt_var &lt;- function(x, w) { mu &lt;- wt_mean(x, w) sum(w * (x - mu) ^ 2) / sum(w) } wt_sd &lt;- function(x, w) { sqrt(wt_var(x, w)) } What happens if x and w are not the same length? wt_mean(1:6, 1:3) #&gt; [1] 7.67 In this case, because of R’s vector recycling rules, we don’t get an error. It’s good practice to check important preconditions, and throw an error (with stop()), if they are not true: wt_mean &lt;- function(x, w) { if (length(x) != length(w)) { stop(&quot;`x` and `w` must be the same length&quot;, call. = FALSE) } sum(w * x) / sum(w) } Be careful not to take this too far. There’s a tradeoff between how much time you spend making your function robust, versus how long you spend writing it. For example, if you also added a na.rm argument, I probably wouldn’t check it carefully: wt_mean &lt;- function(x, w, na.rm = FALSE) { if (!is.logical(na.rm)) { stop(&quot;`na.rm` must be logical&quot;) } if (length(na.rm) != 1) { stop(&quot;`na.rm` must be length 1&quot;) } if (length(x) != length(w)) { stop(&quot;`x` and `w` must be the same length&quot;, call. = FALSE) } if (na.rm) { miss &lt;- is.na(x) | is.na(w) x &lt;- x[!miss] w &lt;- w[!miss] } sum(w * x) / sum(w) } This is a lot of extra work for little additional gain. A useful compromise is the built-in stopifnot(): it checks that each argument is TRUE, and produces a generic error message if not. wt_mean &lt;- function(x, w, na.rm = FALSE) { stopifnot(is.logical(na.rm), length(na.rm) == 1) stopifnot(length(x) == length(w)) if (na.rm) { miss &lt;- is.na(x) | is.na(w) x &lt;- x[!miss] w &lt;- w[!miss] } sum(w * x) / sum(w) } wt_mean(1:6, 6:1, na.rm = &quot;foo&quot;) #&gt; Error in wt_mean(1:6, 6:1, na.rm = &quot;foo&quot;): is.logical(na.rm) is not TRUE Note that when using stopifnot() you assert what should be true rather than checking for what might be wrong. 19.5.3 Dot-dot-dot (…) Many functions in R take an arbitrary number of inputs: sum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) #&gt; [1] 55 stringr::str_c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) #&gt; [1] &quot;abcdef&quot; How do these functions work? They rely on a special argument: ... (pronounced dot-dot-dot). This special argument captures any number of arguments that aren’t otherwise matched. It’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function. For example, I commonly create these helper functions that wrap around str_c(): commas &lt;- function(...) stringr::str_c(..., collapse = &quot;, &quot;) commas(letters[1:10]) #&gt; [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; rule &lt;- function(..., pad = &quot;-&quot;) { title &lt;- paste0(...) width &lt;- getOption(&quot;width&quot;) - nchar(title) - 5 cat(title, &quot; &quot;, stringr::str_dup(pad, width), &quot;\\n&quot;, sep = &quot;&quot;) } rule(&quot;Important output&quot;) #&gt; Important output ------------------------------------------------------ Here ... lets me forward on any arguments that I don’t want to deal with to str_c(). It’s a very convenient technique. But it does come at a price: any misspelled arguments will not raise an error. This makes it easy for typos to go unnoticed: x &lt;- c(1, 2) sum(x, na.mr = TRUE) #&gt; [1] 4 If you just want to capture the values of the ..., use list(...). 19.5.4 Lazy evaluation Arguments in R are lazily evaluated: they’re not computed until they’re needed. That means if they’re never used, they’re never called. This is an important property of R as a programming language, but is generally not important when you’re writing your own functions for data analysis. You can read more about lazy evaluation at http://adv-r.had.co.nz/Functions.html#lazy-evaluation. 19.5.5 Exercises What does commas(letters, collapse = &quot;-&quot;) do? Why? It’d be nice if you could supply multiple characters to the pad argument, e.g. rule(&quot;Title&quot;, pad = &quot;-+&quot;). Why doesn’t this currently work? How could you fix it? What does the trim argument to mean() do? When might you use it? The default value for the method argument to cor() is c(&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;). What does that mean? What value is used by default? 19.6 Return values Figuring out what your function should return is usually straightforward: it’s why you created the function in the first place! There are two things you should consider when returning a value: Does returning early make your function easier to read? Can you make your function pipeable? 19.6.1 Explicit return statements The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). I think it’s best to save the use of return() to signal that you can return early with a simpler solution. A common reason to do this is because the inputs are empty: complicated_function &lt;- function(x, y, z) { if (length(x) == 0 || length(y) == 0) { return(0) } # Complicated code here } Another reason is because you have a if statement with one complex block and one simple block. For example, you might write an if statement like this: f &lt;- function() { if (x) { # Do # something # that # takes # many # lines # to # express } else { # return something short } } But if the first block is very long, by the time you get to the else, you’ve forgotten the condition. One way to rewrite it is to use an early return for the simple case: f &lt;- function() { if (!x) { return(something_short) } # Do # something # that # takes # many # lines # to # express } This tends to make the code easier to understand, because you don’t need quite so much context to understand it. 19.6.2 Writing pipeable functions If you want to write your own pipeable functions, it’s important to think about the return value. Knowing the return value’s object type will mean that your pipeline will “just work”. For example, with dplyr and tidyr the object type is the data frame. There are two basic types of pipeable functions: transformations and side-effects. With transformations, an object is passed to the function’s first argument and a modified object is returned. With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline. For example, this simple function prints the number of missing values in a data frame: show_missings &lt;- function(df) { n &lt;- sum(is.na(df)) cat(&quot;Missing values: &quot;, n, &quot;\\n&quot;, sep = &quot;&quot;) invisible(df) } If we call it interactively, the invisible() means that the input df doesn’t get printed out: show_missings(mtcars) #&gt; Missing values: 0 But it’s still there, it’s just not printed by default: x &lt;- show_missings(mtcars) #&gt; Missing values: 0 class(x) #&gt; [1] &quot;data.frame&quot; dim(x) #&gt; [1] 32 11 And we can still use it in a pipe: mtcars %&gt;% show_missings() %&gt;% mutate(mpg = ifelse(mpg &lt; 20, NA, mpg)) %&gt;% show_missings() #&gt; Missing values: 0 #&gt; Missing values: 18 19.7 Environment The last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work. The environment of a function controls how R finds the value associated with a name. For example, take this function: f &lt;- function(x) { x + y } In many programming languages, this would be an error, because y is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since y is not defined inside the function, R will look in the environment where the function was defined: y &lt;- 100 f(10) #&gt; [1] 110 y &lt;- 1000 f(10) #&gt; [1] 1010 This behaviour seems like a recipe for bugs, and indeed you should avoid creating functions like this deliberately, but by and large it doesn’t cause too many problems (especially if you regularly restart R to get to a clean slate). The advantage of this behaviour is that from a language standpoint it allows R to be very consistent. Every name is looked up using the same set of rules. For f() that includes the behaviour of two things that you might not expect: { and +. This allows you to do devious things like: `+` &lt;- function(x, y) { if (runif(1) &lt; 0.1) { sum(x, y) } else { sum(x, y) * 1.1 } } table(replicate(1000, 1 + 2)) #&gt; #&gt; 3 3.3 #&gt; 100 900 rm(`+`) This is a common phenomenon in R. R places few limits on your power. You can do many things that you can’t do in other programming languages. You can do many things that 99% of the time are extremely ill-advised (like overriding how addition works!). But this power and flexibility is what makes tools like ggplot2 and dplyr possible. Learning how to make best use of this flexibility is beyond the scope of this book, but you can read about in Advanced R. "],
["vectores.html", "20 Vectores 20.1 Introducción 20.2 Pre-requisitos 20.3 Vectores básicos 20.4 Tipos importantes de vectores atómicos 20.5 Usando vectores atómicos 20.6 Vectores Recursivos (listas) 20.7 Visualizando listas 20.8 Listas de Condimentos 20.9 Atributos 20.10 Vectores Aumentados", " 20 Vectores 20.1 Introducción Hasta ahora este libro se ha enfocado en tibbles y sus paquetes correspondientes. Pero como empezaste a escribir tus propias funciones, y a profundizar en R, es que necesitas aprender sobre vectores, es decir, sobre los objetos que soportan los tibbles. Por esto, es mejor empezar con tibbles ya que inmediatamente puedes ver su utilidad, y luego trabajar a tu manera con los componentes que están debajo, los vectores. Los vectores son particularmente importantes, al igual que la mayoría de las funciones que escribirás y utilizarás con dichos vectores. Es posible desarrollar funciones que trabajen con tibbles (como ggplot2, dplyr and tidyr) pero las herramientas que necesitas para ello son peculiares e inmaduras. Por esto, estoy desarrollando un mejor enfoque, el cual puedes consultar en https://github.com/hadley/lazyeval, pero este no estará listo a tiempo para la publicación del libro. Incluso aún cuando esté completo, de todas maneras necesitarás entender el concepto de vectores, esto solo facilitará la escritura de capas finales que sean user-friendly (amigables al usuario). 20.2 Pre-requisitos Este capítulo se enfoca en las estructuras de datos de R base, por lo que no es esencial cargar ningún paquete. Sin embargo, usaremos un conjunto de funciones del paquete purrr para evitar algunas insonsistencias en R Base. library(tidyverse) 20.3 Vectores básicos There are two types of vectors: _Hay dos tipos de vectores: Vectores atómicos, de los cuales existen seis tipos: lógico o booleano, entero, doble o real, caracter, complejo y raw (que consisten en datos sin procesar). Los vectores de tipo entero y doble son ampliamente conocidos como vectores númericos. Las listas, las cuales son denominadas en ciertas ocasiones como vectores recursivos debido a que pueden contener otras listas.. La diferencia principal entre vectores atómicos y listas es que los vectores atomicos son homogéneos, mientras las listas pueden ser heterogéneas. Existe, otro objeto relacionado: Null (nulo). El nulo es a menudo usado para representar la ausencia de un vector (como el opuesto a NA el cual es usado para representar la ausencia de un valor en un vector.) Null típicamente se comporta como un vector de longitud cero 0. Figura @ref (fig:datatypes) resume las interrelaciones. Figure 20.1: The hierarchy of R’s vector types Cada vector tiene dos propiedades claves: Su tipo (type), el cual puedes determinarlo con la sentencia typeof() (del inglés tipode). typeof(letters) #&gt; [1] &quot;character&quot; typeof(1:10) #&gt; [1] &quot;integer&quot; Su longitud (length), la cual puedes determinarla con la sentencia length() (del ingés longitud). x &lt;- list(&quot;a&quot;, &quot;b&quot;, 1:10) length(x) #&gt; [1] 3 Los vectores pueden contener también arbitrariamente metadata adicional en forma de atributos. Estos atributos son usados para crear vectores aumentados los cuales implican un comportamiento distinto. Existen tres tipos de vectores aumentados:  Los factores (factors) construidos a partir de vectores de enteros.  Las fechas y fechas-tiempo (date-time) construidos a partir de vectores numéricos.  Los Dataframes y tibbles construidos a partir de listas. Este capítulo te introducirá a lo temas más importantes de vectores, desde lo más simple a lo más complicado. Comenzarás con vectores atómicos, luego seguirás con listas, y finalizarás con vectores aumentados. 20.4 Tipos importantes de vectores atómicos Los cuatro tipos más importantes de vectores atómicos son lógico, entero, real y carácter. Los tipos raw y complejo son raramente usados durante el análisis de datos, por lo tanto no discutiremos sobre ellos aquí. 20.4.1 Lógico o Booleano Los vectores de tipo lógico o booleano son el tipo más sencillo de vectores atómicos porque ellos solo pueden tomar tres valores posibles: falso, verdadero y Na. Los vectores lógicos son construidos usualmente con operadores de comparación, como se describe en [comparaciones]. También puedes crearlos manualmente con la función c (): 1:10 %% 3 == 0 #&gt; [1] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE c(TRUE, TRUE, FALSE, NA) #&gt; [1] TRUE TRUE FALSE NA 20.4.2 Numérico Los vectores compuestos por enteros o reales son conocidos ampliamente como vectores numéricos. En R, los números son por defecto, reales. Por lo que, para generar un entero, debes colocar una L después del número: typeof(1) #&gt; [1] &quot;double&quot; typeof(1L) #&gt; [1] &quot;integer&quot; 1.5L #&gt; [1] 1.5 La distinción entre enteros y reales no es realmente importante aunque existen dos diferencias relevantes de las que debes ser consciente: 1. Los números dobles o reales son aproximaciones. Los mismos representan números de punto flotante que no pueden ser precisamente representados con un monto fijo de memoria. Esto significa que debes considerar que todos los reales sean aproximaciones. Por ejemplo, ¿cuál es el cuadrado de la raíz cuadrada de dos? ```r x &lt;- sqrt(2) ^ 2 x #&gt; [1] 2 x - 2 #&gt; [1] 4.44e-16 ``` Este compartamiento es común cuando trabajas con números de punto flotante: la mayoría de los cálculos incluyen algunos errores de aproximación. En lugar de comparar números de punto flotante usando ==, debes usar ´dplyr::near()`, el cual provee tolerancia numérica. Los números reales tienen cuatro tipos de valores posibles: NA, NaN, Inf and –Inf. Estos tres valores especiales NaN, Inf and -Inf pueden surgir a partir de la división de c(-1, 0, 1) / 0 Evita usar == para chequear estos valores especiales. En su lugar usa la funciones de ayuda is.finite(), is.infinite(), y is.nan(): 0 Inf NA NaN is.finite() x is.infinite() x is.na() x x is.nan() x 20.4.3 Caracter Los vectores compuestos por carácteres son los tipos más complejos de vectores atómicos, porque cada elemento del mismo es un string, y un string puede contener una cantidad arbitraria de datos. Ya has aprendido un montón acerca de cómo trabajar con strings en strings. En este punto quiero mencionar una característica importante y fundamental en la implementación de un string: R usa una reserva global de strings. Esto significa que cada string solo es almacenado en memoria una vez, y en cada uso de puntos del string a la representación. Esto reduce la cantidad de memoria necesaria por strings duplicados. Puedes ver este comportamiento en práctica con pryr::object_size(): x &lt;- &quot;Esto es un string razonablemente largo.&quot; pryr::object_size(x) #&gt; 152 B y &lt;- rep(x, 1000) pryr::object_size(y) #&gt; 8.14 kB y no utiliza más de 1000x ni tanta memoria como x, porque cada elemento de y es sólo un puntero al mismo string. Un puntero utiliza 8 bytes, entonces 1000 punteros a 136 B string es igual a 8 * 1000 + 136 = 8.13 kB. 20.4.4 Valores perdidos (Missing values) Nota que cada tipo de vector atómico tiene su propio valor perdido (o missing value): NA # logico #&gt; [1] NA NA_integer_ # entero #&gt; [1] NA NA_real_ # real #&gt; [1] NA NA_character_ # caracter #&gt; [1] NA Normalmente no necesitas saber sobre los diferentes tipos porque puedes siempre usar el valor NA (not Available), es decir el valor faltante, y se convertirá al tipo correcto usando las reglas de la coerción implícitas. Sin embargo, existen algunas funciones que son estrictas acerca de sus inputs, por lo tanto es útil tener presente este conocimiento así puedes ser especifico cuando lo necesites. ###Ejercicios Describe la diferencia entre is.finite(x) y !is.infinite(x). Lee el código fuente de dplyr:: near() (Consejo: para ver el código fuente, escribe lo siguiente ()) ¿Funcionó? Un vector de tipo lógico puede tomar 3 valores posibles. ¿Cuántos valores posibles puede tomar un vector de tipo entero? ¿Cuántos valores posibles puede tomar un vector de tipo real? Usa google para realizar buscar información respecto a lo planteado anteriormente. Idea al menos 4 funciones que te permitan convertir un vector del tipo real a entero. ¿En qué difieren las funciones? Sé preciso. ¿Cuáles funciones del paquete readr te permiten convertir un vector del tipo string en un vector del tipo lógico, entero y doble? 20.5 Usando vectores atómicos Ahora que conoces los diferentes tipos de vectores atómicos, es útil repasar algunas herramientas importantes para así poder utilizarlas. Esto incluye: 1. Cómo realizar una conversión de un determinado tipo a otro, y en cuáles casos esto sucede automáticamente. 2. Cómo decidir si un objeto es un tipo específico de un vector. 3. Qué sucede cuando trabajas con vectores de diferentes longitudes. 4. Cómo nombrar los elementos de un vector 5. Cómo obtener los elementos de interés de un vector. 20.5.1 Coerción Existen dos maneras de convertir, o coercer, un tipo de vector a otro: 1. La Coerción explicita sucede cuando defines a una función como as.logical(), as.integer(), as.double(), o as.character(). Cuando te encuentres usando coerción explicita, siempre debes comprobar que sea posible realizar la corrección en sentido ascendente, de esta manera, en primer lugar, estamos seguros que ese vector nunca tuvo tipos incorrectos. Por ejemplo, quizás necesites la especificación de col_types (‘tipos de columna’) del paquete readr. La Coerción implícita sucede cuando usas un vector en un contexto especifico del cual se espera un cierto tipo de vector. Por ejemplo, cuando usas un vector del tipo lógico con la función numérica ‘summary’ (del inglés resumen), o cuando usas un vector del tipo doble donde se espera un vector del tipo entero. Porque la coerción explicita es usada raramente, y es ampliamente fácil de entender, enfocaré sobre la coerción implicita aquí. Anteriormente vimos el tipo más importante de coerción implicita: usando un vector de tipo lógico en un contexto numérico. En ese caso, el valor TRUE (‘VERDADERO’) es convertido a 1 y ‘FALSE’ (‘FALSO’) convertido a 0. Esto significa que la suma de un vector de tipo lógico es el número de los valores verdaderos, y el significado de un vector de tipo lógico es la proporción de valores verdaderos: x &lt;- sample(20, 100, replace = TRUE) y &lt;- x &gt; 10 sum(y) # ¿cuántos valores son más grandes que 10? #&gt; [1] 44 mean(y) # ¿cuál es la porporción de valores que son mayores que 10? #&gt; [1] 0.44 Quizás veas algún código (tipicamente más antiguo) basado en la coerción implicita pero en la dirección opuesta, es decir, de un valor entero a uno lógico if (length(x)) { # do something } En este caso, 0 es convertido a FALSO y todo lo demás es convertido a VERDADERO. Pienso que esto hace más dificil entender el código, por lo que no lo recomiendo. En su lugar, de ser explicito, sugiero utilizar: length(x) &gt; 0. Es también importante entender que pasa cuando creas un vector que contiene múltiples tipos con c(): los tipos más complejos siempre ganan. typeof(c(TRUE, 1L)) #&gt; [1] &quot;integer&quot; typeof(c(1L, 1.5)) #&gt; [1] &quot;double&quot; typeof(c(1.5, &quot;a&quot;)) #&gt; [1] &quot;character&quot; Un vector atómico no puede contener un mix de diferentes tipos porque el tipo es una propiedad de un vector completo, no de elementos individuales. Si necesitas un mix de múltiples tipos en el mismo vector, entonces debes usar una lista, la cual aprenderás en breve. 20.5.2 Funciones de test Algunas veces quieres hacer las cosas de una manera diferente basadas en el tipo de vector. Una de las opciones es el uso de la sentencia typeof(). Otra es usar una función test la cual devuelva un valor TRUE o ‘FALSO’ . R base provee varias funciones como is.vector() y is.atomic(), pero estas a menudo devuelven resultados inesperados. En su lugar, es más acertado usar las funciones is_* provistas por el paquete purrr, las cual están resumidas en la tabla que se muestra a continuación. lgl int dbl chr list is_logical() x is_integer() x is_double() x is_numeric() x x is_character() x is_atomic() x x x x is_list() x is_vector() x x x x x Cada predicado además viene con una version para “escalares”, donde la función is_scalar_atomic(), chequea que la longitud sea 1. Esto es útil, por ejemplo, si quieres chequear en algún argumento que tu función sea un solo valor lógico. 20.5.3 Escalares y reglas de reciclado Así como implicitamente se coercionan los tipos de vectores que son compatibles, R también implicitamente coerciona la longitud de los vectores. Esto se denomina vector recycling, o reciclado de vectores, debido a que el vector de menor longitud se repite, o recicla, hasta igualar la longitud del vector de mayor longitud. Esto es generalmente lo más útil cuando estás trabajando con vectores y “escalares”. Los escalares están puestos en notas porque R en realidad no tiene definido los escalares: en su lugar, un solo número conforma un vector de longitud 1. Debido a que no existen los escalares, la mayoría de las funciones están construidas como vectorizadas, esto significa que operan sobre un vector del tipo númerico. Esto es así porque, por ejemplo, este código funciona: sample(10) + 100 # (del inglés muestreo) #&gt; [1] 109 108 104 102 103 110 106 107 105 101 runif(10) &gt; 0.5 #&gt; [1] TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE En R, las operaciones matemáticas básicas funcionan con vectores. Lo que significa que no necesitarás la ejecución de una interación explicita cuando realices cálculos matemáticos sencillos. Es intuitivo lo que debería pasar si agregas dos vectores de la misma longitud, o un vector y un “escalar”, pero ¿qué sucede si agregas dos vectores de diferentes longitudes? 1:10 + 1:2 #&gt; [1] 2 4 4 6 6 8 8 10 10 12 Aquí, R expandirá el vector de menor longitud a la misma longitud del vector de mayor longitud, a esto es lo que denominamos reciclaje o reutilización de un vector. Esto es una excepción cuando la longitud del vector de mayor longitud no es un múltiplo entero de la longitud del vector más corto: 1:10 + 1:3 #&gt; Warning in 1:10 + 1:3: longer object length is not a multiple of shorter #&gt; object length #&gt; [1] 2 4 6 5 7 9 8 10 12 11 Mientras el vector reciclado puede ser usado para crear código claro y conciso, también puede ocultar problemas de manera silenciosa. Por esta razón, las funciones vectorizadas en tidyverse mostrarán errores cuando recicles cualquier otra cosa que no sea un escalar. Si quieres reutilzar, necesitarás hacerlo tu mismo con la sentencia rep(): tibble(x = 1:4, y = 1:2) #&gt; Error: Tibble columns must have consistent lengths, only values of length one are recycled: #&gt; * Length 2: Column `y` #&gt; * Length 4: Column `x` tibble(x = 1:4, y = rep(1:2, 2)) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 2 tibble(x = 1:4, y = rep(1:2, each = 2)) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 3 2 #&gt; 4 4 2 20.5.4 Nombrando vectores Todos los tipos de vectores pueden ser nombrados. Puedes asignarles un nombre al momento de crearlos con c(): c(x = 1, y = 2, z = 4) #&gt; x y z #&gt; 1 2 4 O después de la creación con purrr::set_names(): set_names(1:3, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) #&gt; a b c #&gt; 1 2 3 Los vectores con nombres son más útiles para subconjuntos, como se describe a continuación. 20.5.5 Subsetting (Subdivisión o creación de subconjuntos) {#vector-subsetting, subdivisión de vectores} Hasta ahora hemos usado dplyr::filter() para filtrar filas en una TIBBLE. La sentencia filter() sólo funciona con TIBBLES, por lo que necesitaremos una nueva herramienta para trabajar con vectores: [. [ representa a la función Subdivisión (Subsetting), la cual nos permite crear subconjuntos o subdivisiones a partir de vectores, y se indica como x[a]. Existen cuatro tipos de cosas en las que puedes subdividir un vector: Un vector numérico contiene sólo enteros. Los enteros deben ser todos positivos, todos negativos, o cero. La Subdivisión con enteros positivos mantiene los elementos en aquellas posiciones: x &lt;- c(&quot;uno&quot;, &quot;dos&quot;, &quot;tres&quot;, &quot;cuatro&quot;, &quot;cinco&quot;) x[c(3, 2, 5)] #&gt; [1] &quot;tres&quot; &quot;dos&quot; &quot;cinco&quot; x[c(3, 2, 5)] #&gt; [1] &quot;tres&quot; &quot;dos&quot; &quot;cinco&quot; Repitiendo una posición, puedes en realidad generar un output de mayor longitud que el input:: x[c(1, 1, 5, 5, 5, 2)] #&gt; [1] &quot;uno&quot; &quot;uno&quot; &quot;cinco&quot; &quot;cinco&quot; &quot;cinco&quot; &quot;dos&quot; Los valores negativos eliminan elementos en posiciones especificas: x[c(-1, -3, -5)] #&gt; [1] &quot;dos&quot; &quot;cuatro&quot; Es un error mezclar valores positivos y negativos: x[c(1, -1)] #&gt; Error in x[c(1, -1)]: only 0&#39;s may be mixed with negative subscripts El mensaje menciona subdivisiones utilizando cero, lo cual no returna valores. ```r x[0] #&gt; character(0) ``` Esto a menudo no es útil, pero puede ser de ayuda si quieres crear estructuras de datos inusuales para testear tus funciones. La subdivisión de un vector lógico mantiene/almacena todos los valores correspondientes al valor TRUE VERDADERO. Esto es a menudo mayormente útil en conjunto con las funciones de comparación. x &lt;- c(10, 3, NA, 5, 8, 1, NA) # Todos los valores non-missing, es decir, distintos de NA de x x[!is.na(x)] #&gt; [1] 10 3 5 8 1 # Todos, incluso los valores (missing) de x x[x %% 2 == 0] #&gt; [1] 10 NA 8 NA Si tienes un vector con nombre, puedes subdivirlo en un vector de tipo caracter. ```r x &lt;- c(abc = 1, def = 2, xyz = 5) x[c(&quot;xyz&quot;, &quot;def&quot;)] #&gt; xyz def #&gt; 5 2 ``` Como con los enteros positivos, también puedes usar un vector del tipo caracter para duplicar entradas individuales. El tipo más sencillo de subsetting es el valor vacío, x[], el cual retorna el valor completo de x. Esto no es útil para vectores subdivididos, aunque si lo es para matrices subdivididas(y otras estructuras de grandes dimensiones) ya que te permite seleccionar toda las filas o todas las columnas, dejando el indice en blanco. Por ejemplo, si x está en la segunda posición, x[1, ] selecciona la primera fila y todas las columnas, y la expresión x[, -1] selecciona todas las filas y todas las columnas excepto la primera. Para aprender más acerca de las aplicaciones de subsetting, puedes leer el capítulo de Subsetting de R Avanzado: http://adv-r.had.co.nz/Subsetting.html#applications. Existe una importante variación de `[`, la cual consiste en `[[`. Esta expresión `[[` sólo extrae un único elemento, y siempre omite nombres. Es una buena idea usarla siempre que quieras dejar en claro que estás extrayendo un único item, como en un bucle for. La diferencia entre `[` y`[[` es más importante para las listas (lists), como veremos en breve. 20.5.6 Ejercicios La expresión mean(is.na(x)), ¿qué dice acerca del vector ‘x’? ¿y qué sucede con la expresión sum(!is.finite(x))? Detenidamente lee la documentación de is.vector(). ¿Para qué se prueba la función realmente? ¿Por qué la función is.atomic() no concuerda con la definición de vectores atómicos vista anteriormente? Compara y contraste setNames() con purrr::set_names(). Crea funciones que tomen un vector como entrada y devuelva: El último valor. ¿Deberás usar [ o [[?. Los elementos en posiciones pares. Cada elemento excepto el último valor. Sólo las posiciones pares (y sin valores perdidos (missing values)). ¿Por qué x[-which(x &gt; 0)] no es lo mismo que x[x &lt;= 0]? ¿Qué sucede cuando realizas un subset (subdivisión) con un entero positivo que es mayor que la longitud del vector? ¿Qué sucede cuando realizas un subset (subdivisión) con un nombre que no existe? 20.6 Vectores Recursivos (listas) Las listas son un escalon más en complejidad partiendo de los vectores atómicos, debido a que las listas pueden contener otras listas. Lo cual las hace adecuadas para representar una estructura jerárquica o de tipo árbol. Puedes crear una lista con ´list()´: x &lt;- list(1, 2, 3) (del inglés lista) x Un herramienta muy útil para trabajar con listas es ´str()´ ya que se enfoca en la estructura, no en los contenidos. str(x) #&gt; Named num [1:3] 1 2 5 #&gt; - attr(*, &quot;names&quot;)= chr [1:3] &quot;abc&quot; &quot;def&quot; &quot;xyz&quot; x_nombrada &lt;- list(a = 1, b = 2, c = 3) str(x_nombrada) #&gt; List of 3 #&gt; $ a: num 1 #&gt; $ b: num 2 #&gt; $ c: num 3 A diferencia de los vectores atómicos, el objeto ´list()´ puede contener una variedad de diferentes objetos: y &lt;- list(&quot;a&quot;, 1L, 1.5, TRUE) str(y) #&gt; List of 4 #&gt; $ : chr &quot;a&quot; #&gt; $ : int 1 #&gt; $ : num 1.5 #&gt; $ : logi TRUE ¡Incluso las listas pueden contener otras listas! z &lt;- list(list(1, 2), list(3, 4)) str(z) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ : num 1 #&gt; ..$ : num 2 #&gt; $ :List of 2 #&gt; ..$ : num 3 #&gt; ..$ : num 4 20.7 Visualizando listas Para explicar funciones de manipulacion de listas más complicadas, es útil tener una representacion visual de las mismas. Por ejemplo, defino estas tres listas: x1 &lt;- list(c(1, 2), c(3, 4)) x2 &lt;- list(list(1, 2), list(3, 4)) x3 &lt;- list(1, list(2, list(3))) Y a continuación, las grafico: Existen tres principios, al momento de observer el gráfico anterior: 1. Las listas tienen esquinas redondeadas, en cambio, los vectores atómicos tienen esquinas cuadradas. 2. Los hijos son representados dentro de sus listas padres, y tienen un fondo ligeramente más oscuro para facilitar la visualización de la jerarquía. 3. No es importante la orientación de los hijos (p.ej. las filas o columnas), entonces utilizaré la orientacion de una fila o columna para almacenar espacio o incluso para ilustrar una propiedad importante en el ejemplo. Subdivisión (Subsetting) Existen tres maneras de subdividir una lista, lo cual ilustraré con una lista denominada ´a´: a &lt;- list(a = 1:3, b = &quot;a string&quot;, c = pi, d = list(-1, -5)) El corchete simple ´[´ extrae una sub-lista. Por lo que, el resultado siempre será una lista. str(a[1:2]) #&gt; List of 2 #&gt; $ a: int [1:3] 1 2 3 #&gt; $ b: chr &quot;a string&quot; str(a[4]) #&gt; List of 1 #&gt; $ d:List of 2 #&gt; ..$ : num -1 #&gt; ..$ : num -5 Al igual que con vectores, puedes subdividirla, en un vector lógico, de enteros o caracteres. El doble corchete ´[[´ extrae un solo componente de una lista. Y remueve un nivel de la jerarquía de la lista. str(a[[1]]) #&gt; int [1:3] 1 2 3 str(a[[4]]) #&gt; List of 2 #&gt; $ : num -1 #&gt; $ : num -5 El símbolo $ es un atajo para extraer elementos con nombres de una lista. Este funciona de modo similar al doble corchete´[[´ excepto que en el primer caso no es necesario el uso de comillas dobles. a$a #&gt; [1] 1 2 3 a[[&quot;a&quot;]] #&gt; [1] 1 2 3 La diferencia entre el corchete simple [ y el doble [[ es realmente importante para las listas, porque el doble [[ profundiza en una lista mientras que el simple [ retorna una nueva, lista más pequeña. Compara el código y el output de arriba con la representacion visual de la Figura @ref(fig:lists-subsetting). lists-subsetting, echo = FALSE, out.width = &quot;75%&quot;, fig.cap = &quot;Subdividir una lista, de manera visual.&quot;} knitr::include_graphics(&quot;diagrams/lists-subsetting.png&quot;) 20.8 Listas de Condimentos La diferencia entre ambos [ y [[ es muy importante, pero es muy fácil confundirlos. Para ayudarte a recordar, permiteme mostrarte un pimientero inusual Si este pimientero es tu lista x, entonces, `x[1] es un pimientero que contiene un simple paquete de pimienta: La expresión x[2] luciría del mismo modo, pero podría contener el segundo paquete. La expresión x[1:2] sería un pimientero que contiente dos paquetes de pimienta. La expresión x[[1]] es: Si quisieras obtener el contenido del paquete de pimiento, necesitarías utilizar la siguiente expresión `x[[1]][[1]: 20.8.1 Ejercicios 1.Dibuja las listas siguientes como sets anidados: 1. `list(a, b, list(c, d), list(e, f))` 1. `list(list(list(list(list(list(a))))))` 1.¿Qué pasaría si subdividieras un tibble como si fuera una lista? ¿Cuáles son las principales diferencias entre una lista y un tibble? 20.9 Atributos Cualquier vector puede contener metadata arbitraria adicional mediante sus atributos. Puedes pensar en los atributos como una lista de vectores que pueden ser adjuntadas a cualquier otro objeto. Puedes obtener y setear valores de atributos individuales con attr() o verlos todos al mismo tiempo con attributes(). x &lt;- 1:10 attr(x, &quot;saludo&quot;) #&gt; NULL attr(x, &quot;saludo&quot;) &lt;- &quot;Hola!&quot; attr(x, &quot; despedida&quot;) &lt;- &quot;Adiós!&quot; attributes(x) #&gt; $saludo #&gt; [1] &quot;Hola!&quot; #&gt; #&gt; $` despedida` #&gt; [1] &quot;Adiós!&quot; Existen tres atributos muy importantes que son utilizados para implementar partes fundamentals de R: 1. Los Nombres son utilizados para nombrar los elementos de un vector. 2. Las Dimensiones (o dims, denominación más corta) hacen que un vector se comporte como una matriz o arreglo. 3. Una Clase es utilizada para implementar el sistema S3 orientado a objetos. A los atributos nombres los vimos arriba, y no cubriremos las dimensiones porque no se usan matrices en este libro. Resta describir el atributo clase, el cual controla como las funciones genéricas funcionan. Las funciones genéricas son clave para la programacion orientada a objetos en R, porque ellas hacen que las funciones se comporten de manera diferente de acuerdo a las diferentes clases de inputs. Una discusión más profunda sobre programacion orientada a objetos no está contemplada en el ámbito de este libro, pero puedes leer más al respecto en el documento R Avanzado en: http://adv-r.had.co.nz/OO-essentials.html#s3. Así es como una función genérica típica luce: as.Date #&gt; function (x, ...) #&gt; UseMethod(&quot;as.Date&quot;) #&gt; &lt;bytecode: 0x6818258&gt; #&gt; &lt;environment: namespace:base&gt; La llamada al método “UseMethod” significa que esta es una función genérica, y llamará a un metódo específico, una función, basada en la clase del primer argumento. (Todos los métodos son funciones; no todas las funciones son métodos). Puedes listar todos los métodos existentes para una función genérica utilizando la función: methods(): methods(&quot;as.Date&quot;) #&gt; [1] as.Date.character as.Date.default as.Date.factor as.Date.numeric #&gt; [5] as.Date.POSIXct as.Date.POSIXlt #&gt; see &#39;?methods&#39; for accessing help and source code Por ejemplo, si x es un vector de caracteres, as.Date() llamará a as.Date.character(); si es un factor, llamará a as.Date.factor(). Puedes ver la implementación específica de un método con: getS3method(): getS3method(&quot;as.Date&quot;, &quot;default&quot;) #&gt; function (x, ...) #&gt; { #&gt; if (inherits(x, &quot;Date&quot;)) #&gt; x #&gt; else if (is.logical(x) &amp;&amp; all(is.na(x))) #&gt; .Date(as.numeric(x)) #&gt; else stop(gettextf(&quot;do not know how to convert &#39;%s&#39; to class %s&quot;, #&gt; deparse(substitute(x)), dQuote(&quot;Date&quot;)), domain = NA) #&gt; } #&gt; &lt;bytecode: 0x276a988&gt; #&gt; &lt;environment: namespace:base&gt; getS3method(&quot;as.Date&quot;, &quot;numeric&quot;) #&gt; function (x, origin, ...) #&gt; { #&gt; if (missing(origin)) #&gt; stop(&quot;&#39;origin&#39; must be supplied&quot;) #&gt; as.Date(origin, ...) + x #&gt; } #&gt; &lt;bytecode: 0x27687c8&gt; #&gt; &lt;environment: namespace:base&gt; Lo mas importante del S3 genérico; sistema OO, es decir, orientado a objetos; es la función print(): el cual controla como el objeto es impreso cuando tipeas su nombre en la consola. Otras funciones genéricas importantes son las funciones de subdivisión [, [[, and $. 20.10 Vectores Aumentados Los vectores atómicos y las listas son los bloques sobre los que se construyen otros tipos importantes de vectores como los tipos factores y fechas (dates). A estos vectores , los llamo __ vectores aumentados __, porque son vectores con attributos adicionales, incluyendo la clase. Los vectores aumentados tienen una clase, por ello se comportan de manera diferente a los vectores atómicos sobre los cuales son construidos. En este libro, hacemos uso de cuatro importantes vectores aumentados: • Los Factores • Las Dates (Fechas) • Los Date-times (Fecha-tiempo) • Los Tibbles Estos son descriptos a continuación: Factores Los factores son diseñados para representar datos categoricos que pueden tomar un set fijo de valores posibles, son construidos sobre los enteros, y tienen x &lt;- factor(c(&quot;ab&quot;, &quot;cd&quot;, &quot;ab&quot;), levels = c(&quot;ab&quot;, &quot;cd&quot;, &quot;ef&quot;)) typeof(x) #&gt; [1] &quot;integer&quot; attributes(x) #&gt; $levels #&gt; [1] &quot;ab&quot; &quot;cd&quot; &quot;ef&quot; #&gt; #&gt; $class #&gt; [1] &quot;factor&quot; 20.10.1 Dates and date-times (Fechas y Fecha – Hora) Las vectores del tipo date en R son vectores numéricos que representan el número de días desde el 1° de enero de 1970. x &lt;- as.Date(&quot;1971-01-01&quot;) unclass(x) #&gt; [1] 365 typeof(x) #&gt; [1] &quot;double&quot; attributes(x) #&gt; $class #&gt; [1] &quot;Date&quot; Los vectores date-time son vectores numéricos de clase POSIXct que representan el número de segundos desde el 1° de enero de 1970. (En caso de que te preguntes sobre “POSIXct”; “POSIXct” significa “Portable Operating System Interface, lo que significa “Interfaz portable de sistema operativo”; tiempo de calendario.) x &lt;- lubridate::ymd_hm(&quot;1970-01-01 01:00&quot;) unclass(x) #&gt; [1] 3600 #&gt; attr(,&quot;tzone&quot;) #&gt; [1] &quot;UTC&quot; typeof(x) #&gt; [1] &quot;double&quot; attributes(x) #&gt; $class #&gt; [1] &quot;POSIXct&quot; &quot;POSIXt&quot; #&gt; #&gt; $tzone #&gt; [1] &quot;UTC&quot; El atributo tzone es opcional. Este controla como se muestra la hora, y no hace referencia al tiempo en términos absolutos. attr(x, &quot;tzone&quot;) &lt;- &quot;US/Pacifico&quot; x #&gt; [1] &quot;1970-01-01 01:00:00&quot; attr(x, &quot;tzone&quot;) &lt;- &quot;US/ Oriental&quot; x #&gt; [1] &quot;1970-01-01 01:00:00&quot; Existe otro tipo de vector date-time llamado POSIXlt. Éstos son construidos en base a listas con nombres (named lists). y &lt;- as.POSIXlt(x) typeof(y) #&gt; [1] &quot;list&quot; attributes(y) #&gt; $names #&gt; [1] &quot;sec&quot; &quot;min&quot; &quot;hour&quot; &quot;mday&quot; &quot;mon&quot; &quot;year&quot; &quot;wday&quot; #&gt; [8] &quot;yday&quot; &quot;isdst&quot; &quot;zone&quot; &quot;gmtoff&quot; #&gt; #&gt; $class #&gt; [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; #&gt; #&gt; $tzone #&gt; [1] &quot;US/ Oriental&quot; &quot;&quot; &quot;&quot; Los vectores POSIXlts son pocos comunes dentro del paquete tidyverse. Surgen en base a R, porque son necesarios para extraer components específicos de una fecha, como el año o el mes. Desde que el paquete lubridate te provee helpers para efectuar dicha extracción, los vectores POSIXlts no son necesarios. Siempre es más sencillo trabajar con loa vectores POSIXct’s, por lo tanto si tenés un vector POSIXlt, deberías convertirlo a un vector regular del tipo data-time con lubridate::as_date_time(). 20.10.2 Tibbles Los Tibbles son listas aumentadas: los cuales tienen las clases “tbl_df”, “tbl” y “data.frame”, y los atributos names (para nombrar una columna) y row.names (para nombrar una fila): tb &lt;- tibble::tibble(x = 1:5, y = 5:1) typeof(tb) #&gt; [1] &quot;list&quot; attributes(tb) #&gt; $names #&gt; [1] &quot;x&quot; &quot;y&quot; #&gt; #&gt; $row.names #&gt; [1] 1 2 3 4 5 #&gt; #&gt; $class #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; La diferencia entre un tibble y una lista, consiste en que todos los elementos de un data frame deben ser vectores con la misma longitud. Por lo tanto, todas las funciones que utilizan tibbles refuerzan esta condición. Además, los data.frames tradicionales tienen una estructura muy similar a los tibbles: df &lt;- data.frame(x = 1:5, y = 5:1) typeof(df) #&gt; [1] &quot;list&quot; attributes(df) #&gt; $names #&gt; [1] &quot;x&quot; &quot;y&quot; #&gt; #&gt; $class #&gt; [1] &quot;data.frame&quot; #&gt; #&gt; $row.names #&gt; [1] 1 2 3 4 5 Se puede decir, que la diferencia principal entre ambos es la clase. Debido a que un tibble incluye el tipo “data.frame” lo que significa que los tibbles heredan el comportamiento regular de un data frame por defecto. Ejecicios: 1. ¿Qué valor retorna la siguiente expresión hms::hms(3600)? ¿Cómo se muestra? ¿Cuál es la tipo primario sobre el cual se basa el vector aumentado? ¿Qué atributos utiliza el mismo? 2. Intenta y crea un tibble que tenga columnas con diferentes longitudes ¿Qué es lo que ocurre? 3. Teniendo en cuenta "],
["iteration.html", "21 Iteration 21.1 Introduction 21.2 For loops 21.3 For loop variations 21.4 For loops vs. functionals 21.5 The map functions 21.6 Dealing with failure 21.7 Mapping over multiple arguments 21.8 Walk 21.9 Other patterns of for loops", " 21 Iteration 21.1 Introduction In functions, we talked about how important it is to reduce duplication in your code by creating functions instead of copying-and-pasting. Reducing code duplication has three main benefits: It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in more places. One tool for reducing duplication is functions, which reduce duplication by identifying repeated patterns of code and extract them out into independent pieces that can be easily reused and updated. Another tool for reducing duplication is iteration, which helps you when you need to do the same thing to multiple inputs: repeating the same operation on different columns, or on different datasets. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors. 21.1.1 Prerequisites Once you’ve mastered the for loops provided by base R, you’ll learn some of the powerful programming tools provided by purrr, one of the tidyverse core packages. library(tidyverse) 21.2 For loops Imagine we have this simple tibble: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) We want to compute the median of each column. You could do with copy-and-paste: median(df$a) #&gt; [1] -0.246 median(df$b) #&gt; [1] -0.287 median(df$c) #&gt; [1] -0.0567 median(df$d) #&gt; [1] 0.144 But that breaks our rule of thumb: never copy and paste more than twice. Instead, we could use a for loop: output &lt;- vector(&quot;double&quot;, ncol(df)) # 1. output for (i in seq_along(df)) { # 2. sequence output[[i]] &lt;- median(df[[i]]) # 3. body } output #&gt; [1] -0.2458 -0.2873 -0.0567 0.1443 Every for loop has three components: The output: output &lt;- vector(&quot;double&quot;, length(x)). Before you start the loop, you must always allocate sufficient space for the output. This is very important for efficiency: if you grow the for loop at each iteration using c() (for example), your for loop will be very slow. A general way of creating an empty vector of given length is the vector() function. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc) and the length of the vector. The sequence: i in seq_along(df). This determines what to loop over: each run of the for loop will assign i to a different value from seq_along(df). It’s useful to think of i as a pronoun, like “it”. You might not have seen seq_along() before. It’s a safe version of the familiar 1:length(l), with an important difference: if you have a zero-length vector, seq_along() does the right thing: y &lt;- vector(&quot;double&quot;, 0) seq_along(y) #&gt; integer(0) 1:length(y) #&gt; [1] 1 0 You probably won’t create a zero-length vector deliberately, but it’s easy to create them accidentally. If you use 1:length(x) instead of seq_along(x), you’re likely to get a confusing error message. The body: output[[i]] &lt;- median(df[[i]]). This is the code that does the work. It’s run repeatedly, each time with a different value for i. The first iteration will run output[[1]] &lt;- median(df[[1]]), the second will run output[[2]] &lt;- median(df[[2]]), and so on. That’s all there is to the for loop! Now is a good time to practice creating some basic (and not so basic) for loops using the exercises below. Then we’ll move on some variations of the for loop that help you solve other problems that will crop up in practice. 21.2.1 Exercises Write for loops to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). Think about the output, sequence, and body before you start writing the loop. Eliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors: out &lt;- &quot;&quot; for (x in letters) { out &lt;- stringr::str_c(out, x) } x &lt;- sample(100) sd &lt;- 0 for (i in seq_along(x)) { sd &lt;- sd + (x[i] - mean(x))^2 } sd &lt;- sqrt(sd / (length(x) - 1)) x &lt;- runif(100) out &lt;- vector(&quot;numeric&quot;, length(x)) out[1] &lt;- x[1] for (i in 2:length(x)) { out[i] &lt;- out[i - 1] + x[i] } Combine your function writing and for loop skills: Write a for loop that prints() the lyrics to the children’s song “Alice the camel”. Convert the nursery rhyme “ten in the bed” to a function. Generalise it to any number of people in any sleeping structure. Convert the song “99 bottles of beer on the wall” to a function. Generalise to any number of any vessel containing any liquid on any surface. It’s common to see for loops that don’t preallocate the output and instead increase the length of a vector at each step: output &lt;- vector(&quot;integer&quot;, 0) for (i in seq_along(x)) { output &lt;- c(output, lengths(x[[i]])) } output How does this affect performance? Design and execute an experiment. 21.3 For loop variations Once you have the basic for loop under your belt, there are some variations that you should be aware of. These variations are important regardless of how you do iteration, so don’t forget about them once you’ve mastered the FP techniques you’ll learn about in the next section. There are four variations on the basic theme of the for loop: Modifying an existing object, instead of creating a new object. Looping over names or values, instead of indices. Handling outputs of unknown length. Handling sequences of unknown length. 21.3.1 Modifying an existing object Sometimes you want to use a for loop to modify an existing object. For example, remember our challenge from functions. We wanted to rescale every column in a data frame: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) rescale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } df$a &lt;- rescale01(df$a) df$b &lt;- rescale01(df$b) df$c &lt;- rescale01(df$c) df$d &lt;- rescale01(df$d) To solve this with a for loop we again think about the three components: Output: we already have the output — it’s the same as the input! Sequence: we can think about a data frame as a list of columns, so we can iterate over each column with seq_along(df). Body: apply rescale01(). This gives us: for (i in seq_along(df)) { df[[i]] &lt;- rescale01(df[[i]]) } Typically you’ll be modifying a list or data frame with this sort of loop, so remember to use [[, not [. You might have spotted that I used [[ in all my for loops: I think it’s better to use [[ even for atomic vectors because it makes it clear that I want to work with a single element. 21.3.2 Looping patterns There are three basic ways to loop over a vector. So far I’ve shown you the most general: looping over the numeric indices with for (i in seq_along(xs)), and extracting the value with x[[i]]. There are two other forms: Loop over the elements: for (x in xs). This is most useful if you only care about side-effects, like plotting or saving a file, because it’s difficult to save the output efficiently. Loop over the names: for (nm in names(xs)). This gives you name, which you can use to access the value with x[[nm]]. This is useful if you want to use the name in a plot title or a file name. If you’re creating named output, make sure to name the results vector like so: results &lt;- vector(&quot;list&quot;, length(x)) names(results) &lt;- names(x) Iteration over the numeric indices is the most general form, because given the position you can extract both the name and the value: for (i in seq_along(x)) { name &lt;- names(x)[[i]] value &lt;- x[[i]] } 21.3.3 Unknown output length Sometimes you might not know how long the output will be. For example, imagine you want to simulate some random vectors of random lengths. You might be tempted to solve this problem by progressively growing the vector: means &lt;- c(0, 1, 2) output &lt;- double() for (i in seq_along(means)) { n &lt;- sample(100, 1) output &lt;- c(output, rnorm(n, means[[i]])) } str(output) #&gt; num [1:202] 0.912 0.205 2.584 -0.789 0.588 ... But this is not very efficient because in each iteration, R has to copy all the data from the previous iterations. In technical terms you get “quadratic” (\\(O(n^2)\\)) behaviour which means that a loop with three times as many elements would take nine (\\(3^2\\)) times as long to run. A better solution to save the results in a list, and then combine into a single vector after the loop is done: out &lt;- vector(&quot;list&quot;, length(means)) for (i in seq_along(means)) { n &lt;- sample(100, 1) out[[i]] &lt;- rnorm(n, means[[i]]) } str(out) #&gt; List of 3 #&gt; $ : num [1:83] 0.367 1.13 -0.941 0.218 1.415 ... #&gt; $ : num [1:21] -0.485 -0.425 2.937 1.688 1.324 ... #&gt; $ : num [1:40] 2.34 1.59 2.93 3.84 1.3 ... str(unlist(out)) #&gt; num [1:144] 0.367 1.13 -0.941 0.218 1.415 ... Here I’ve used unlist() to flatten a list of vectors into a single vector. A stricter option is to use purrr::flatten_dbl() — it will throw an error if the input isn’t a list of doubles. This pattern occurs in other places too: You might be generating a long string. Instead of paste()ing together each iteration with the previous, save the output in a character vector and then combine that vector into a single string with paste(output, collapse = &quot;&quot;). You might be generating a big data frame. Instead of sequentially rbind()ing in each iteration, save the output in a list, then use dplyr::bind_rows(output) to combine the output into a single data frame. Watch out for this pattern. Whenever you see it, switch to a more complex result object, and then combine in one step at the end. 21.3.4 Unknown sequence length Sometimes you don’t even know how long the input sequence should run for. This is common when doing simulations. For example, you might want to loop until you get three heads in a row. You can’t do that sort of iteration with the for loop. Instead, you can use a while loop. A while loop is simpler than for loop because it only has two components, a condition and a body: while (condition) { # body } A while loop is also more general than a for loop, because you can rewrite any for loop as a while loop, but you can’t rewrite every while loop as a for loop: for (i in seq_along(x)) { # body } # Equivalent to i &lt;- 1 while (i &lt;= length(x)) { # body i &lt;- i + 1 } Here’s how we could use a while loop to find how many tries it takes to get three heads in a row: flip &lt;- function() sample(c(&quot;T&quot;, &quot;H&quot;), 1) flips &lt;- 0 nheads &lt;- 0 while (nheads &lt; 3) { if (flip() == &quot;H&quot;) { nheads &lt;- nheads + 1 } else { nheads &lt;- 0 } flips &lt;- flips + 1 } flips #&gt; [1] 3 I mention while loops only briefly, because I hardly ever use them. They’re most often used for simulation, which is outside the scope of this book. However, it is good to know they exist so that you’re prepared for problems where the number of iterations is not known in advance. 21.3.5 Exercises Imagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files &lt;- dir(&quot;data/&quot;, pattern = &quot;\\\\.csv$&quot;, full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame. What happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique? Write a function that prints the mean of each numeric column in a data frame, along with its name. For example, show_mean(iris) would print: show_mean(iris) # &gt; Sepal.Length: 5.84 # &gt; Sepal.Width: 3.06 # &gt; Petal.Length: 3.76 # &gt; Petal.Width: 1.20 (Extra challenge: what function did I use to make sure that the numbers lined up nicely, even though the variable names had different lengths?) What does this code do? How does it work? trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) { factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) } ) for (var in names(trans)) { mtcars[[var]] &lt;- trans[[var]](mtcars[[var]]) } 21.4 For loops vs. functionals For loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. To see why this is important, consider (again) this simple data frame: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) Imagine you want to compute the mean of every column. You could do that with a for loop: output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[[i]] &lt;- mean(df[[i]]) } output #&gt; [1] 0.2026 -0.2068 0.1275 -0.0917 You realise that you’re going to want to compute the means of every column pretty frequently, so you extract it out into a function: col_mean &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- mean(df[[i]]) } output } But then you think it’d also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your col_mean() function and replace the mean() with median() and sd(): col_median &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- median(df[[i]]) } output } col_sd &lt;- function(df) { output &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { output[i] &lt;- sd(df[[i]]) } output } Uh oh! You’ve copied-and-pasted this code twice, so it’s time to think about how to generalise it. Notice that most of this code is for-loop boilerplate and it’s hard to see the one thing (mean(), median(), sd()) that is different between the functions. What would you do if you saw a set of functions like this: f1 &lt;- function(x) abs(x - mean(x))^1 f2 &lt;- function(x) abs(x - mean(x))^2 f3 &lt;- function(x) abs(x - mean(x))^3 Hopefully, you’d notice that there’s a lot of duplication, and extract it out into an additional argument: f &lt;- function(x, i) abs(x - mean(x))^i You’ve reduced the chance of bugs (because you now have 1/3 less code), and made it easy to generalise to new situations. We can do exactly the same thing with col_mean(), col_median() and col_sd() by adding an argument that supplies the function to apply to each column: col_summary &lt;- function(df, fun) { out &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { out[i] &lt;- fun(df[[i]]) } out } col_summary(df, median) #&gt; [1] 0.237 -0.218 0.254 -0.133 col_summary(df, mean) #&gt; [1] 0.2026 -0.2068 0.1275 -0.0917 The idea of passing a function to another function is extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. It might take you a while to wrap your head around the idea, but it’s worth the investment. In the rest of the chapter, you’ll learn about and use the purrr package, which provides functions that eliminate the need for many common for loops. The apply family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but purrr is more consistent and thus is easier to learn. The goal of using purrr functions instead of for loops is to allow you break common list manipulation challenges into independent pieces: How can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list. If you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe. This structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code. 21.4.1 Exercises Read the documentation for apply(). In the 2d case, what two for loops does it generalise? Adapt col_summary() so that it only applies to numeric columns You might want to start with an is_numeric() function that returns a logical vector that has a TRUE corresponding to each numeric column. 21.5 The map functions The pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output: map() makes a list. map_lgl() makes a logical vector. map_int() makes an integer vector. map_dbl() makes a double vector. map_chr() makes a character vector. Each function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function. Once you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!). Some people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years). The chief benefits of using functions like map() is not speed, but clarity: they make your code easier to write and to read. We can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use map_dbl(): map_dbl(df, mean) #&gt; a b c d #&gt; 0.2026 -0.2068 0.1275 -0.0917 map_dbl(df, median) #&gt; a b c d #&gt; 0.237 -0.218 0.254 -0.133 map_dbl(df, sd) #&gt; a b c d #&gt; 0.796 0.759 1.164 1.062 Compared to using a for loop, focus is on the operation being performed (i.e. mean(), median(), sd()), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe: df %&gt;% map_dbl(mean) #&gt; a b c d #&gt; 0.2026 -0.2068 0.1275 -0.0917 df %&gt;% map_dbl(median) #&gt; a b c d #&gt; 0.237 -0.218 0.254 -0.133 df %&gt;% map_dbl(sd) #&gt; a b c d #&gt; 0.796 0.759 1.164 1.062 There are a few differences between map_*() and col_summary(): All purrr functions are implemented in C. This makes them a little faster at the expense of readability. The second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section. map_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called: map_dbl(df, mean, trim = 0.5) #&gt; a b c d #&gt; 0.237 -0.218 0.254 -0.133 The map functions also preserve names: z &lt;- list(x = 1:3, y = 4:5) map_int(z, length) #&gt; x y #&gt; 3 2 21.5.1 Shortcuts There are a few shortcuts that you can use with .f in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits the up the mtcars dataset in to three pieces (one for each value of cylinder) and fits the same linear model to each piece: models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(function(df) lm(mpg ~ wt, data = df)) The syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula. models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .)) Here I’ve used . as a pronoun: it refers to the current list element (in the same way that i referred to the current index in the for loop). When you’re looking at many models, you might want to extract a summary statistic like the \\(R^2\\). To do that we need to first run summary() and then extract the component called r.squared. We could do that using the shorthand for anonymous functions: models %&gt;% map(summary) %&gt;% map_dbl(~ .$r.squared) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 But extracting named components is a common operation, so purrr provides an even shorter shortcut: you can use a string. models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) #&gt; 4 6 8 #&gt; 0.509 0.465 0.423 You can also use an integer to select elements by position: x &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9)) x %&gt;% map_dbl(2) #&gt; [1] 2 5 8 21.5.2 Base R If you’re familiar with the apply family of functions in base R, you might have noticed some similarities with the purrr functions: lapply() is basically identical to map(), except that map() is consistent with all the other functions in purrr, and you can use the shortcuts for .f. Base sapply() is a wrapper around lapply() that automatically simplifies the output. This is useful for interactive work but is problematic in a function because you never know what sort of output you’ll get: x1 &lt;- list( c(0.27, 0.37, 0.57, 0.91, 0.20), c(0.90, 0.94, 0.66, 0.63, 0.06), c(0.21, 0.18, 0.69, 0.38, 0.77) ) x2 &lt;- list( c(0.50, 0.72, 0.99, 0.38, 0.78), c(0.93, 0.21, 0.65, 0.13, 0.27), c(0.39, 0.01, 0.38, 0.87, 0.34) ) threshold &lt;- function(x, cutoff = 0.8) x[x &gt; cutoff] x1 %&gt;% sapply(threshold) %&gt;% str() #&gt; List of 3 #&gt; $ : num 0.91 #&gt; $ : num [1:2] 0.9 0.94 #&gt; $ : num(0) x2 %&gt;% sapply(threshold) %&gt;% str() #&gt; num [1:3] 0.99 0.93 0.87 vapply() is a safe alternative to sapply() because you supply an additional argument that defines the type. The only problem with vapply() is that it’s a lot of typing: vapply(df, is.numeric, logical(1)) is equivalent to map_lgl(df, is.numeric). One advantage of vapply() over purrr’s map functions is that it can also produce matrices — the map functions only ever produce vectors. I focus on purrr functions here because they have more consistent names and arguments, helpful shortcuts, and in the future will provide easy parallelism and progress bars. 21.5.3 Exercises Write code that uses one of the map functions to: Compute the mean of every column in mtcars. Determine the type of each column in nycflights13::flights. Compute the number of unique values in each column of iris. Generate 10 random normals for each of \\(\\mu = -10\\), \\(0\\), \\(10\\), and \\(100\\). How can you create a single vector that for each column in a data frame indicates whether or not it’s a factor? What happens when you use the map functions on vectors that aren’t lists? What does map(1:5, runif) do? Why? What does map(-2:2, rnorm, n = 5) do? Why? What does map_dbl(-2:2, rnorm, n = 5) do? Why? Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) to eliminate the anonymous function. 21.6 Dealing with failure When you use the map functions to repeat many operations, the chances are much higher that one of those operations will fail. When this happens, you’ll get an error message, and no output. This is annoying: why does one failure prevent you from accessing all the other successes? How do you ensure that one bad apple doesn’t ruin the whole barrel? In this section you’ll learn how to deal this situation with a new function: safely(). safely() is an adverb: it takes a function (a verb) and returns a modified version. In this case, the modified function will never throw an error. Instead, it always returns a list with two elements: result is the original result. If there was an error, this will be NULL. error is an error object. If the operation was successful, this will be NULL. (You might be familiar with the try() function in base R. It’s similar, but because it sometimes returns the original result and it sometimes returns an error object it’s more difficult to work with.) Let’s illustrate this with a simple example: log(): safe_log &lt;- safely(log) str(safe_log(10)) #&gt; List of 2 #&gt; $ result: num 2.3 #&gt; $ error : NULL str(safe_log(&quot;a&quot;)) #&gt; List of 2 #&gt; $ result: NULL #&gt; $ error :List of 2 #&gt; ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; When the function succeeds, the result element contains the result and the error element is NULL. When the function fails, the result element is NULL and the error element contains an error object. safely() is designed to work with map: x &lt;- list(1, 10, &quot;a&quot;) y &lt;- x %&gt;% map(safely(log)) str(y) #&gt; List of 3 #&gt; $ :List of 2 #&gt; ..$ result: num 0 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: num 2.3 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: NULL #&gt; ..$ error :List of 2 #&gt; .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; This would be easier to work with if we had two lists: one of all the errors and one of all the output. That’s easy to get with purrr::transpose(): y &lt;- y %&gt;% transpose() str(y) #&gt; List of 2 #&gt; $ result:List of 3 #&gt; ..$ : num 0 #&gt; ..$ : num 2.3 #&gt; ..$ : NULL #&gt; $ error :List of 3 #&gt; ..$ : NULL #&gt; ..$ : NULL #&gt; ..$ :List of 2 #&gt; .. ..$ message: chr &quot;non-numeric argument to mathematical function&quot; #&gt; .. ..$ call : language .Primitive(&quot;log&quot;)(x, base) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; It’s up to you how to deal with the errors, but typically you’ll either look at the values of x where y is an error, or work with the values of y that are ok: is_ok &lt;- y$error %&gt;% map_lgl(is_null) x[!is_ok] #&gt; [[1]] #&gt; [1] &quot;a&quot; y$result[is_ok] %&gt;% flatten_dbl() #&gt; [1] 0.0 2.3 Purrr provides two other useful adverbs: Like safely(), possibly() always succeeds. It’s simpler than safely(), because you give it a default value to return when there is an error. x &lt;- list(1, 10, &quot;a&quot;) x %&gt;% map_dbl(possibly(log, NA_real_)) #&gt; [1] 0.0 2.3 NA quietly() performs a similar role to safely(), but instead of capturing errors, it captures printed output, messages, and warnings: x &lt;- list(1, -1) x %&gt;% map(quietly(log)) %&gt;% str() #&gt; List of 2 #&gt; $ :List of 4 #&gt; ..$ result : num 0 #&gt; ..$ output : chr &quot;&quot; #&gt; ..$ warnings: chr(0) #&gt; ..$ messages: chr(0) #&gt; $ :List of 4 #&gt; ..$ result : num NaN #&gt; ..$ output : chr &quot;&quot; #&gt; ..$ warnings: chr &quot;NaNs produced&quot; #&gt; ..$ messages: chr(0) 21.7 Mapping over multiple arguments So far we’ve mapped along a single input. But often you have multiple related inputs that you need iterate along in parallel. That’s the job of the map2() and pmap() functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with map(): mu &lt;- list(5, 10, -3) mu %&gt;% map(rnorm, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 5.45 5.5 5.78 6.51 3.18 #&gt; $ : num [1:5] 10.79 9.03 10.89 10.76 10.65 #&gt; $ : num [1:5] -3.54 -3.08 -5.01 -3.51 -2.9 What if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds: sigma &lt;- list(1, 5, 10) seq_along(mu) %&gt;% map(~ rnorm(5, mu[[.]], sigma[[.]])) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 4.94 2.57 4.37 4.12 5.29 #&gt; $ : num [1:5] 11.72 5.32 11.46 10.24 12.22 #&gt; $ : num [1:5] 3.68 -6.12 22.24 -7.2 10.37 But that obfuscates the intent of the code. Instead we could use map2() which iterates over two vectors in parallel: map2(mu, sigma, rnorm, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 4.78 5.59 4.93 4.3 4.47 #&gt; $ : num [1:5] 10.85 10.57 6.02 8.82 15.93 #&gt; $ : num [1:5] -1.12 7.39 -7.5 -10.09 -2.7 map2() generates this series of function calls: Note that the arguments that vary for each call come before the function; arguments that are the same for every call come after. Like map(), map2() is just a wrapper around a for loop: map2 &lt;- function(x, y, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], y[[i]], ...) } out } You could also imagine map3(), map4(), map5(), map6() etc, but that would get tedious quickly. Instead, purrr provides pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples: n &lt;- list(1, 3, 5) args1 &lt;- list(n, mu, sigma) args1 %&gt;% pmap(rnorm) %&gt;% str() #&gt; List of 3 #&gt; $ : num 4.55 #&gt; $ : num [1:3] 13.4 18.8 13.2 #&gt; $ : num [1:5] 0.685 10.801 -11.671 21.363 -2.562 That looks like: If you don’t name the elements of list, pmap() will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments: args2 &lt;- list(mean = mu, sd = sigma, n = n) args2 %&gt;% pmap(rnorm) %&gt;% str() That generates longer, but safer, calls: Since the arguments are all the same length, it makes sense to store them in a data frame: params &lt;- tribble( ~ mean, ~ sd, ~ n, 5, 1, 1, 10, 5, 3, -3, 10, 5 ) params %&gt;% pmap(rnorm) #&gt; [[1]] #&gt; [1] 4.68 #&gt; #&gt; [[2]] #&gt; [1] 23.44 12.85 7.28 #&gt; #&gt; [[3]] #&gt; [1] -5.34 -17.66 0.92 6.06 9.02 As soon as your code gets complicated, I think a data frame is a good approach because it ensures that each column has a name and is the same length as all the other columns. 21.7.1 Invoking different functions There’s one more step up in complexity - as well as varying the arguments to the function you might also vary the function itself: f &lt;- c(&quot;runif&quot;, &quot;rnorm&quot;, &quot;rpois&quot;) param &lt;- list( list(min = -1, max = 1), list(sd = 5), list(lambda = 10) ) To handle this case, you can use invoke_map(): invoke_map(f, param, n = 5) %&gt;% str() #&gt; List of 3 #&gt; $ : num [1:5] 0.762 0.36 -0.714 0.531 0.254 #&gt; $ : num [1:5] 3.07 -3.09 1.1 5.64 9.07 #&gt; $ : int [1:5] 9 14 8 9 7 The first argument is a list of functions or character vector of function names. The second argument is a list of lists giving the arguments that vary for each function. The subsequent arguments are passed on to every function. And again, you can use tribble() to make creating these matching pairs a little easier: sim &lt;- tribble( ~ f, ~ params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sim = invoke_map(f, params, n = 10)) 21.8 Walk Walk is an alternative to map that you use when you want to call a function for its side effects, rather than for its return value. You typically do this because you want to render output to the screen or save files to disk - the important thing is the action, not the return value. Here’s a very simple example: x &lt;- list(1, &quot;a&quot;, 3) x %&gt;% walk(print) #&gt; [1] 1 #&gt; [1] &quot;a&quot; #&gt; [1] 3 walk() is generally not that useful compared to walk2() or pwalk(). For example, if you had a list of plots and a vector of file names, you could use pwalk() to save each file to the corresponding location on disk: library(ggplot2) plots &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~ ggplot(., aes(mpg, wt)) + geom_point()) paths &lt;- stringr::str_c(names(plots), &quot;.pdf&quot;) pwalk(list(paths, plots), ggsave, path = tempdir()) walk(), walk2() and pwalk() all invisibly return .x, the first argument. This makes them suitable for use in the middle of pipelines. 21.9 Other patterns of for loops Purrr provides a number of other functions that abstract over other types of for loops. You’ll use them less frequently than the map functions, but they’re useful to know about. The goal here is to briefly illustrate each function, so hopefully it will come to mind if you see a similar problem in the future. Then you can go look up the documentation for more details. 21.9.1 Predicate functions A number of functions work with predicate functions that return either a single TRUE or FALSE. keep() and discard() keep elements of the input where the predicate is TRUE or FALSE respectively: iris %&gt;% keep(is.factor) %&gt;% str() #&gt; &#39;data.frame&#39;: 150 obs. of 1 variable: #&gt; $ Species: Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris %&gt;% discard(is.factor) %&gt;% str() #&gt; &#39;data.frame&#39;: 150 obs. of 4 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... some() and every() determine if the predicate is true for any or for all of the elements. x &lt;- list(1:5, letters, list(10)) x %&gt;% some(is_character) #&gt; [1] TRUE x %&gt;% every(is_vector) #&gt; [1] TRUE detect() finds the first element where the predicate is true; detect_index() returns its position. x &lt;- sample(10) x #&gt; [1] 8 7 5 6 9 2 10 1 3 4 x %&gt;% detect(~ . &gt; 5) #&gt; [1] 8 x %&gt;% detect_index(~ . &gt; 5) #&gt; [1] 1 head_while() and tail_while() take elements from the start or end of a vector while a predicate is true: x %&gt;% head_while(~ . &gt; 5) #&gt; [1] 8 7 x %&gt;% tail_while(~ . &gt; 5) #&gt; integer(0) 21.9.2 Reduce and accumulate Sometimes you have a complex list that you want to reduce to a simple list by repeatedly applying a function that reduces a pair to a singleton. This is useful if you want to apply a two-table dplyr verb to multiple tables. For example, you might have a list of data frames, and you want to reduce to a single data frame by joining the elements together: dfs &lt;- list( age = tibble(name = &quot;John&quot;, age = 30), sex = tibble(name = c(&quot;John&quot;, &quot;Mary&quot;), sex = c(&quot;M&quot;, &quot;F&quot;)), trt = tibble(name = &quot;Mary&quot;, treatment = &quot;A&quot;) ) dfs %&gt;% reduce(full_join) #&gt; Joining, by = &quot;name&quot; #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 2 x 4 #&gt; name age sex treatment #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John 30 M &lt;NA&gt; #&gt; 2 Mary NA F A Or maybe you have a list of vectors, and want to find the intersection: vs &lt;- list( c(1, 3, 5, 6, 10), c(1, 2, 3, 7, 8, 10), c(1, 2, 3, 4, 8, 9, 10) ) vs %&gt;% reduce(intersect) #&gt; [1] 1 3 10 The reduce function takes a “binary” function (i.e. a function with two primary inputs), and applies it repeatedly to a list until there is only a single element left. Accumulate is similar but it keeps all the interim results. You could use it to implement a cumulative sum: x &lt;- sample(10) x #&gt; [1] 6 9 8 5 2 4 7 1 10 3 x %&gt;% accumulate(`+`) #&gt; [1] 6 15 23 28 30 34 41 42 52 55 21.9.3 Exercises Implement your own version of every() using a for loop. Compare it with purrr::every(). What does purrr’s version do that your version doesn’t? Create an enhanced col_sum() that applies a summary function to every numeric column in a data frame. A possible base R equivalent of col_sum() is: col_sum3 &lt;- function(df, f) { is_num &lt;- sapply(df, is.numeric) df_num &lt;- df[, is_num] sapply(df_num, f) } But it has a number of bugs as illustrated with the following inputs: df &lt;- tibble( x = 1:3, y = 3:1, z = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) # OK col_sum3(df, mean) # Has problems: don&#39;t always return numeric vector col_sum3(df[1:2], mean) col_sum3(df[1], mean) col_sum3(df[0], mean) What causes the bugs? "],
["model-intro.html", "22 Introducción 22.1 Generación de hipótesis vs. confirmación de hipótesis", " 22 Introducción Ahora que estás equipado con herramientas de programación poderosas, finalmente podemos regresar a modelar. Utilizarás tus nuevas herramientas de domador de datos y de programación para ajustar muchos modelos y entender como funcionan. El foco de este libro es la exploración, no la confirmación o la inferencia formal. Pero aprenderás un par de herramientas básicas que te ayudarán a entender la variación en tus modelos. El objetivo de un modelo es proveer un resumen simple y de baja dimensionalidad sobre un conjunto de datos. Idealmente, el modelo capturará “señales” verdaderas (por ejemplo patrones generados por el fenómeno de interés) e ignorará el “ruido” (es decir, variaciones aleatorias que no nos interesan). Sólo vamos a cubrir modelos “predictivos”, los cuales, como su nombre sugiere, generan predicciones. Hay otro tipo de modelo que no vamos a discutir: los modelos de “descubrimiento de datos”. Estos modelos no hacen predicciones, si no que ayudan a descubrir relaciones interesantes entre tus datos. (Estos dos tipos de modelos suelen ser llamados supervisados y no supervisados, pero no creo que esa terminología sea particularmente esclarecedora.) Este libro no te dará un entendimiento profundo de la teoría matemática que subyace a los modelos. Sin embargo, construirá tu intuición sobre cómo funcionan los modelos estadísticos y te proporcionará una familia de herramientas útiles que te permitirán utilizar modelos para comprender mejor tus datos: En [conceptos básicos], aprenderás cómo los modelos funcionan mecánicamente, centrándonos en la importante familia de modelos lineales. Aprenderás herramientas generales para obtener información sobre lo que un modelo predictivo te dice sobre tus datos, centrándonos en conjuntos de datos simples simulados. En [construcción de modelos], aprenderás cómo usar modelos para extraer patrones conocidos en datos reales. Una vez que reconozcas un patrón importante, es útil hacerlo explícito en un modelo, porque entonces podrás ver más fácilmente aquellas señales sutiles que quedan en tus datos. En muchos modelos, aprenderás cómo usar muchos modelos simples para ayudar a comprender conjuntos de datos complejos. Esta es una técnica poderosa, pero para acceder necesitarás combinar herramientas de modelado y de programación. Deliberadamente dejamos fuera del capítulo las explicaciones de herramientas para evaluar cuantitativamente los modelos porque dicha evaluación precisa requiere conocer un par de grandes ideas que simplemente no tenemos espacio para cubrir aquí. Por ahora, dependerás de la evaluación cualitativa y de tu escepticismo natural. En [Aprender más sobre los modelos], te indicaremos otros recursos donde podrás seguir aprendiendo. 22.1 Generación de hipótesis vs. confirmación de hipótesis En este libro vamos a usar los modelos como una herramienta para la exploración, completando la tríada de las herramientas para EDA que se introdujeron en la Parte 1. Los modelos no se suelen enseñar de esta manera, pero como verás, son herramientas importantes para la exploración. Tradicionalmente, el enfoque del modelado está en la inferencia, es decir, para confirmar que una hipótesis es verdadera; hacerlo correctamente no es complicado, pero si difícil. Hay un par de ideas que debes comprender para poder hacer la inferencia correctamente: Cada observación puede ser utilizada para exploración o para confirmación, pero no para ambas. Puedes usar una observación tantas veces como quieras para la exploración, pero solo una vez para confirmación. En el instante que usaste una observación dos veces, pasaste de confirmar a explorar. Esto es necesario porque, para confirmar una hipótesis, debes usar datos independientes de los datos utilizados para generarla. De lo contrario, serás demasiado optimista. No hay absolutamente nada de malo en la exploración, pero nunca debes vender un análisis exploratorio como análisis confirmatorio porque es fundamentalmente erróneo. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: Si realmente quieres realizar un análisis confirmatorio, un enfoque es dividir los datos en tres partes antes de comenzar el análisis: El 60% de los datos van a un conjunto de entrenamiento (del inglés, training) o exploración. Puedes hacer lo que quieras con estos datos, desde visualizarlo a ajustar toneladas de modelos. 20% va a un conjunto de consulta (del inglés, query). Se puede usar esta información para comparar modelos o hacer visualizaciones a mano, pero no está permitido usarlo como parte de un proceso automatizado. El otro 20% se reserva para un conjunto de validación (del inglés, test). Sólo se pueden usar estos datos UNA VEZ, para probar tu modelo final. Esta partición de los datos, te permite explorar con los datos de entrenamiento, generando ocasionalmente hipótesis candidatas que se verifican con el conjunto de consultas. Cuando estés seguro de tener el modelo correcto, se verifica una vez con los datos del conjunto de validación. (Se debe tener en cuenta que, incluso cuando realice modelado confirmatorio, se necesita hacer EDA. Si no se realiza ninguna EDA, los problemas de calidad que tengan los datos quedarán ocultos). "],
["model-basics.html", "23 Model basics 23.1 Introduction 23.2 A simple model 23.3 Visualising models 23.4 Formulas and model families 23.5 Missing values 23.6 Other model families", " 23 Model basics 23.1 Introduction The goal of a model is to provide a simple low-dimensional summary of a dataset. In the context of this book we’re going to use models to partition data into patterns and residuals. Strong patterns will hide subtler trends, so we’ll use models to help peel back layers of structure as we explore a dataset. However, before we can start using models on interesting, real, datasets, you need to understand the basics of how models work. For that reason, this chapter of the book is unique because it uses only simulated datasets. These datasets are very simple, and not at all interesting, but they will help you understand the essence of modelling before you apply the same techniques to real data in the next chapter. There are two parts to a model: First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like y = a_1 * x + a_2 or y = a_1 * x ^ a_2. Here, x and y are known variables from your data, and a_1 and a_2 are parameters that can vary to capture different patterns. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like y = 3 * x + 7 or y = 9 * x ^ 2. It’s important to understand that a fitted model is just the closest model from a family of models. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. George Box puts this well in his famous aphorism: All models are wrong, but some are useful. It’s worth reading the fuller context of the quote: Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?”. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful. 23.1.1 Prerequisites In this chapter we’ll use the modelr package which wraps around base R’s modelling functions to make them work naturally in a pipe. library(tidyverse) library(modelr) options(na.action = na.warn) 23.2 A simple model Lets take a look at the simulated dataset sim1, included with the modelr package. It contains two continuous variables, x and y. Let’s plot them to see how they’re related: ggplot(sim1, aes(x, y)) + geom_point() You can see a strong pattern in the data. Let’s use a model to capture that pattern and make it explicit. It’s our job to supply the basic form of the model. In this case, the relationship looks linear, i.e. y = a_0 + a_1 * x. Let’s start by getting a feel for what models from that family look like by randomly generating a few and overlaying them on the data. For this simple case, we can use geom_abline() which takes a slope and intercept as parameters. Later on we’ll learn more general techniques that work with any model. models &lt;- tibble( a1 = runif(250, -20, 40), a2 = runif(250, -5, 5) ) ggplot(sim1, aes(x, y)) + geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1 / 4) + geom_point() There are 250 models on this plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is “close” to the data. We need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of a_0 and a_1 that generate the model with the smallest distance from this data. One easy place to start is to find the vertical distance between each point and the model, as in the following diagram. (Note that I’ve shifted the x values slightly so you can see the individual distances.) This distance is just the difference between the y value given by the model (the prediction), and the actual y value in the data (the response). To compute this distance, we first turn our model family into an R function. This takes the model parameters and the data as inputs, and gives values predicted by the model as output: model1 &lt;- function(a, data) { a[1] + data$x * a[2] } model1(c(7, 1.5), sim1) #&gt; [1] 8.5 8.5 8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 #&gt; [15] 14.5 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 #&gt; [29] 22.0 22.0 Next, we need some way to compute an overall distance between the predicted and actual values. In other words, the plot above shows 30 distances: how do we collapse that into a single number? One common way to do this in statistics to use the “root-mean-squared deviation”. We compute the difference between actual and predicted, square them, average them, and the take the square root. This distance has lots of appealing mathematical properties, which we’re not going to talk about here. You’ll just have to take my word for it! measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) sqrt(mean(diff^2)) } measure_distance(c(7, 1.5), sim1) #&gt; [1] 2.67 Now we can use purrr to compute the distance for all the models defined above. We need a helper function because our distance function expects the model as a numeric vector of length 2. sim1_dist &lt;- function(a1, a2) { measure_distance(c(a1, a2), sim1) } models &lt;- models %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) models #&gt; # A tibble: 250 x 3 #&gt; a1 a2 dist #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -15.2 0.0889 30.8 #&gt; 2 30.1 -0.827 13.2 #&gt; 3 16.0 2.27 13.2 #&gt; 4 -10.6 1.38 18.7 #&gt; 5 -19.6 -1.04 41.8 #&gt; 6 7.98 4.59 19.3 #&gt; # … with 244 more rows Next, let’s overlay the 10 best models on to the data. I’ve coloured the models by -dist: this is an easy way to make sure that the best models (i.e. the ones with the smallest distance) get the brighest colours. ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(models, rank(dist) &lt;= 10) ) We can also think about these models as observations, and visualising with a scatterplot of a1 vs a2, again coloured by -dist. We can no longer directly see how the model compares to the data, but we can see many models at once. Again, I’ve highlighted the 10 best models, this time by drawing red circles underneath them. ggplot(models, aes(a1, a2)) + geom_point(data = filter(models, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) Instead of trying lots of random models, we could be more systematic and generate an evenly spaced grid of points (this is called a grid search). I picked the parameters of the grid roughly by looking at where the best models were in the plot above. grid &lt;- expand.grid( a1 = seq(-5, 20, length = 25), a2 = seq(1, 3, length = 25) ) %&gt;% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) grid %&gt;% ggplot(aes(a1, a2)) + geom_point(data = filter(grid, rank(dist) &lt;= 10), size = 4, colour = &quot;red&quot;) + geom_point(aes(colour = -dist)) When you overlay the best 10 models back on the original data, they all look pretty good: ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline( aes(intercept = a1, slope = a2, colour = -dist), data = filter(grid, rank(dist) &lt;= 10) ) You could imagine iteratively making the grid finer and finer until you narrowed in on the best model. But there’s a better way to tackle that problem: a numerical minimisation tool called Newton-Raphson search. The intuition of Newton-Raphson is pretty simple: you pick a starting point and look around for the steepest slope. You then ski down that slope a little way, and then repeat again and again, until you can’t go any lower. In R, we can do that with optim(): best &lt;- optim(c(0, 0), measure_distance, data = sim1) best$par #&gt; [1] 4.22 2.05 ggplot(sim1, aes(x, y)) + geom_point(size = 2, colour = &quot;grey30&quot;) + geom_abline(intercept = best$par[1], slope = best$par[2]) Don’t worry too much about the details of how optim() works. It’s the intuition that’s important here. If you have a function that defines the distance between a model and a dataset, an algorithm that can minimise that distance by modifying the parameters of the model, you can find the best model. The neat thing about this approach is that it will work for any family of models that you can write an equation for. There’s one more approach that we can use for this model, because it’s a special case of a broader family: linear models. A linear model has the general form y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1). So this simple model is equivalent to a general linear model where n is 2 and x_1 is x. R has a tool specifically designed for fitting linear models called lm(). lm() has a special way to specify the model family: formulas. Formulas look like y ~ x, which lm() will translate to a function like y = a_1 + a_2 * x. We can fit the model and look at the output: sim1_mod &lt;- lm(y ~ x, data = sim1) coef(sim1_mod) #&gt; (Intercept) x #&gt; 4.22 2.05 These are exactly the same values we got with optim()! Behind the scenes lm() doesn’t use optim() but instead takes advantage of the mathematical structure of linear models. Using some connections between geometry, calculus, and linear algebra, lm() actually finds the closest model in a single step, using a sophisticated algorithm. This approach is both faster, and guarantees that there is a global minimum. 23.2.1 Exercises One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model? sim1a &lt;- tibble( x = rep(1:10, each = 3), y = x * 1.5 + 6 + rt(length(x), df = 2) ) One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance: measure_distance &lt;- function(mod, data) { diff &lt;- data$y - model1(mod, data) mean(abs(diff)) } Use optim() to fit this model to the simulated data above and compare it to the linear model. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this? model1 &lt;- function(a, data) { a[1] + data$x * a[2] + a[3] } 23.3 Visualising models For simple models, like the one above, you can figure out what pattern the model captures by carefully studying the model family and the fitted coefficients. And if you ever take a statistics course on modelling, you’re likely to spend a lot of time doing just that. Here, however, we’re going to take a different tack. We’re going to focus on understanding a model by looking at its predictions. This has a big advantage: every type of predictive model makes predictions (otherwise what use would it be?) so we can use the same set of techniques to understand any type of predictive model. It’s also useful to see what the model doesn’t capture, the so-called residuals which are left after subtracting the predictions from the data. Residuals are powerful because they allow us to use models to remove striking patterns so we can study the subtler trends that remain. 23.3.1 Predictions To visualise the predictions from a model, we start by generating an evenly spaced grid of values that covers the region where our data lies. The easiest way to do that is to use modelr::data_grid(). Its first argument is a data frame, and for each subsequent argument it finds the unique variables and then generates all combinations: grid &lt;- sim1 %&gt;% data_grid(x) grid #&gt; # A tibble: 10 x 1 #&gt; x #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 #&gt; 4 4 #&gt; 5 5 #&gt; 6 6 #&gt; # … with 4 more rows (This will get more interesting when we start to add more variables to our model.) Next we add predictions. We’ll use modelr::add_predictions() which takes a data frame and a model. It adds the predictions from the model to a new column in the data frame: grid &lt;- grid %&gt;% add_predictions(sim1_mod) grid #&gt; # A tibble: 10 x 2 #&gt; x pred #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 6.27 #&gt; 2 2 8.32 #&gt; 3 3 10.4 #&gt; 4 4 12.4 #&gt; 5 5 14.5 #&gt; 6 6 16.5 #&gt; # … with 4 more rows (You can also use this function to add predictions to your original dataset.) Next, we plot the predictions. You might wonder about all this extra work compared to just using geom_abline(). But the advantage of this approach is that it will work with any model in R, from the simplest to the most complex. You’re only limited by your visualisation skills. For more ideas about how to visualise more complex model types, you might try http://vita.had.co.nz/papers/model-vis.html. ggplot(sim1, aes(x)) + geom_point(aes(y = y)) + geom_line(aes(y = pred), data = grid, colour = &quot;red&quot;, size = 1) 23.3.2 Residuals The flip-side of predictions are residuals. The predictions tells you the pattern that the model has captured, and the residuals tell you what the model has missed. The residuals are just the distances between the observed and predicted values that we computed above. We add residuals to the data with add_residuals(), which works much like add_predictions(). Note, however, that we use the original dataset, not a manufactured grid. This is because to compute residuals we need actual y values. sim1 &lt;- sim1 %&gt;% add_residuals(sim1_mod) sim1 #&gt; # A tibble: 30 x 3 #&gt; x y resid #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 4.20 -2.07 #&gt; 2 1 7.51 1.24 #&gt; 3 1 2.13 -4.15 #&gt; 4 2 8.99 0.665 #&gt; 5 2 10.2 1.92 #&gt; 6 2 11.3 2.97 #&gt; # … with 24 more rows There are a few different ways to understand what the residuals tell us about the model. One way is to simply draw a frequency polygon to help us understand the spread of the residuals: ggplot(sim1, aes(resid)) + geom_freqpoly(binwidth = 0.5) This helps you calibrate the quality of the model: how far away are the predictions from the observed values? Note that the average of the residual will always be 0. You’ll often want to recreate plots using the residuals instead of the original predictor. You’ll see a lot of that in the next chapter. ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0) + geom_point() This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset. 23.3.3 Exercises Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()? add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ? What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important? Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals? 23.4 Formulas and model families You’ve seen formulas before when using facet_wrap() and facet_grid(). In R, formulas provide a general way of getting “special behaviour”. Rather than evaluating the values of the variables right away, they capture them so they can be interpreted by the function. The majority of modelling functions in R use a standard conversion from formulas to functions. You’ve seen one simple conversion already: y ~ x is translated to y = a_1 + a_2 * x. If you want to see what R actually does, you can use the model_matrix() function. It takes a data frame and a formula and returns a tibble that defines the model equation: each column in the output is associated with one coefficient in the model, the function is always y = a_1 * out1 + a_2 * out_2. For the simplest case of y ~ x1 this shows us something interesting: df &lt;- tribble( ~ y, ~ x1, ~ x2, 4, 2, 5, 5, 1, 6 ) model_matrix(df, y ~ x1) #&gt; # A tibble: 2 x 2 #&gt; `(Intercept)` x1 #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 #&gt; 2 1 1 The way that R adds the intercept to the model is just by having a column that is full of ones. By default, R will always add this column. If you don’t want, you need to explicitly drop it with -1: model_matrix(df, y ~ x1 - 1) #&gt; # A tibble: 2 x 1 #&gt; x1 #&gt; &lt;dbl&gt; #&gt; 1 2 #&gt; 2 1 The model matrix grows in an unsurprising way when you add more variables to the the model: model_matrix(df, y ~ x1 + x2) #&gt; # A tibble: 2 x 3 #&gt; `(Intercept)` x1 x2 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2 5 #&gt; 2 1 1 6 This formula notation is sometimes called “Wilkinson-Rogers notation”, and was initially described in Symbolic Description of Factorial Models for Analysis of Variance, by G. N. Wilkinson and C. E. Rogers https://www.jstor.org/stable/2346786. It’s worth digging up and reading the original paper if you’d like to understand the full details of the modelling algebra. The following sections expand on how this formula notation works for categorical variables, interactions, and transformation. 23.4.1 Categorical variables Generating a function from a formula is straight forward when the predictor is continuous, but things get a bit more complicated when the predictor is categorical. Imagine you have a formula like y ~ sex, where sex could either be male or female. It doesn’t make sense to convert that to a formula like y = x_0 + x_1 * sex because sex isn’t a number - you can’t multiply it! Instead what R does is convert it to y = x_0 + x_1 * sex_male where sex_male is one if sex is male and zero otherwise: df &lt;- tribble( ~ sex, ~ response, &quot;male&quot;, 1, &quot;female&quot;, 2, &quot;male&quot;, 1 ) model_matrix(df, response ~ sex) #&gt; # A tibble: 3 x 2 #&gt; `(Intercept)` sexmale #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 0 #&gt; 3 1 1 You might wonder why R also doesn’t create a sexfemale column. The problem is that would create a column that is perfectly predictable based on the other columns (i.e. sexfemale = 1 - sexmale). Unfortunately the exact details of why this is a problem is beyond the scope of this book, but basically it creates a model family that is too flexible, and will have infinitely many models that are equally close to the data. Fortunately, however, if you focus on visualising predictions you don’t need to worry about the exact parameterisation. Let’s look at some data and models to make that concrete. Here’s the sim2 dataset from modelr: ggplot(sim2) + geom_point(aes(x, y)) We can fit a model to it, and generate predictions: mod2 &lt;- lm(y ~ x, data = sim2) grid &lt;- sim2 %&gt;% data_grid(x) %&gt;% add_predictions(mod2) grid #&gt; # A tibble: 4 x 2 #&gt; x pred #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1.15 #&gt; 2 b 8.12 #&gt; 3 c 6.13 #&gt; 4 d 1.91 Effectively, a model with a categorical x will predict the mean value for each category. (Why? Because the mean minimises the root-mean-squared distance.) That’s easy to see if we overlay the predictions on top of the original data: ggplot(sim2, aes(x)) + geom_point(aes(y = y)) + geom_point(data = grid, aes(y = pred), colour = &quot;red&quot;, size = 4) You can’t make predictions about levels that you didn’t observe. Sometimes you’ll do this by accident so it’s good to recognise this error message: tibble(x = &quot;e&quot;) %&gt;% add_predictions(mod2) #&gt; Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor x has new level e 23.4.2 Interactions (continuous and categorical) What happens when you combine a continuous and a categorical variable? sim3 contains a categorical predictor and a continuous predictor. We can visualise it with a simple plot: ggplot(sim3, aes(x1, y)) + geom_point(aes(colour = x2)) There are two possible models you could fit to this data: mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) When you add variables with +, the model will estimate each effect independent of all the others. It’s possible to fit the so-called interaction by using *. For example, y ~ x1 * x2 is translated to y = a_0 + a_1 * x1 + a_2 * x2 + a_12 * x1 * x2. Note that whenever you use *, both the interaction and the individual components are included in the model. To visualise these models we need two new tricks: We have two predictors, so we need to give data_grid() both variables. It finds all the unique values of x1 and x2 and then generates all combinations. To generate predictions from both models simultaneously, we can use gather_predictions() which adds each prediction as a row. The complement of gather_predictions() is spread_predictions() which adds each prediction to a new column. Together this gives us: grid &lt;- sim3 %&gt;% data_grid(x1, x2) %&gt;% gather_predictions(mod1, mod2) grid #&gt; # A tibble: 80 x 4 #&gt; model x1 x2 pred #&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 mod1 1 a 1.67 #&gt; 2 mod1 1 b 4.56 #&gt; 3 mod1 1 c 6.48 #&gt; 4 mod1 1 d 4.03 #&gt; 5 mod1 2 a 1.48 #&gt; 6 mod1 2 b 4.37 #&gt; # … with 74 more rows We can visualise the results for both models on one plot using facetting: ggplot(sim3, aes(x1, y, colour = x2)) + geom_point() + geom_line(data = grid, aes(y = pred)) + facet_wrap(~ model) Note that the model that uses + has the same slope for each line, but different intercepts. The model that uses * has a different slope and intercept for each line. Which model is better for this data? We can take look at the residuals. Here I’ve facetted by both model and x2 because it makes it easier to see the pattern within each group. sim3 &lt;- sim3 %&gt;% gather_residuals(mod1, mod2) ggplot(sim3, aes(x1, resid, colour = x2)) + geom_point() + facet_grid(model ~ x2) There is little obvious pattern in the residuals for mod2. The residuals for mod1 show that the model has clearly missed some pattern in b, and less so, but still present is pattern in c, and d. You might wonder if there’s a precise way to tell which of mod1 or mod2 is better. There is, but it requires a lot of mathematical background, and we don’t really care. Here, we’re interested in a qualitative assessment of whether or not the model has captured the pattern that we’re interested in. 23.4.3 Interactions (two continuous) Let’s take a look at the equivalent model for two continuous variables. Initially things proceed almost identically to the previous example: mod1 &lt;- lm(y ~ x1 + x2, data = sim4) mod2 &lt;- lm(y ~ x1 * x2, data = sim4) grid &lt;- sim4 %&gt;% data_grid( x1 = seq_range(x1, 5), x2 = seq_range(x2, 5) ) %&gt;% gather_predictions(mod1, mod2) grid #&gt; # A tibble: 50 x 4 #&gt; model x1 x2 pred #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mod1 -1 -1 0.996 #&gt; 2 mod1 -1 -0.5 -0.395 #&gt; 3 mod1 -1 0 -1.79 #&gt; 4 mod1 -1 0.5 -3.18 #&gt; 5 mod1 -1 1 -4.57 #&gt; 6 mod1 -0.5 -1 1.91 #&gt; # … with 44 more rows Note my use of seq_range() inside data_grid(). Instead of using every unique value of x, I’m going to use a regularly spaced grid of five values between the minimum and maximum numbers. It’s probably not super important here, but it’s a useful technique in general. There are two other useful arguments to seq_range(): pretty = TRUE will generate a “pretty” sequence, i.e. something that looks nice to the human eye. This is useful if you want to produce tables of output: seq_range(c(0.0123, 0.923423), n = 5) #&gt; [1] 0.0123 0.2401 0.4679 0.6956 0.9234 seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE) #&gt; [1] 0.0 0.2 0.4 0.6 0.8 1.0 trim = 0.1 will trim off 10% of the tail values. This is useful if the variables have a long tailed distribution and you want to focus on generating values near the center: x1 &lt;- rcauchy(100) seq_range(x1, n = 5) #&gt; [1] -115.9 -83.5 -51.2 -18.8 13.5 seq_range(x1, n = 5, trim = 0.10) #&gt; [1] -13.84 -8.71 -3.58 1.55 6.68 seq_range(x1, n = 5, trim = 0.25) #&gt; [1] -2.1735 -1.0594 0.0547 1.1687 2.2828 seq_range(x1, n = 5, trim = 0.50) #&gt; [1] -0.725 -0.268 0.189 0.647 1.104 expand = 0.1 is in some sense the opposite of trim() it expands the range by 10%. x2 &lt;- c(0, 1) seq_range(x2, n = 5) #&gt; [1] 0.00 0.25 0.50 0.75 1.00 seq_range(x2, n = 5, expand = 0.10) #&gt; [1] -0.050 0.225 0.500 0.775 1.050 seq_range(x2, n = 5, expand = 0.25) #&gt; [1] -0.125 0.188 0.500 0.812 1.125 seq_range(x2, n = 5, expand = 0.50) #&gt; [1] -0.250 0.125 0.500 0.875 1.250 Next let’s try and visualise that model. We have two continuous predictors, so you can imagine the model like a 3d surface. We could display that using geom_tile(): ggplot(grid, aes(x1, x2)) + geom_tile(aes(fill = pred)) + facet_wrap(~ model) That doesn’t suggest that the models are very different! But that’s partly an illusion: our eyes and brains are not very good at accurately comparing shades of colour. Instead of looking at the surface from the top, we could look at it from either side, showing multiple slices: ggplot(grid, aes(x1, pred, colour = x2, group = x2)) + geom_line() + facet_wrap(~ model) ggplot(grid, aes(x2, pred, colour = x1, group = x1)) + geom_line() + facet_wrap(~ model) This shows you that interaction between two continuous variables works basically the same way as for a categorical and continuous variable. An interaction says that there’s not a fixed offset: you need to consider both values of x1 and x2 simultaneously in order to predict y. You can see that even with just two continuous variables, coming up with good visualisations are hard. But that’s reasonable: you shouldn’t expect it will be easy to understand how three or more variables simultaneously interact! But again, we’re saved a little because we’re using models for exploration, and you can gradually build up your model over time. The model doesn’t have to be perfect, it just has to help you reveal a little more about your data. I spent some time looking at the residuals to see if I could figure if mod2 did better than mod1. I think it does, but it’s pretty subtle. You’ll have a chance to work on it in the exercises. 23.4.4 Transformations You can also perform transformations inside the model formula. For example, log(y) ~ sqrt(x1) + x2 is transformed to log(y) = a_1 + a_2 * sqrt(x1) + a_3 * x2. If your transformation involves +, *, ^, or -, you’ll need to wrap it in I() so R doesn’t treat it like part of the model specification. For example, y ~ x + I(x ^ 2) is translated to y = a_1 + a_2 * x + a_3 * x^2. If you forget the I() and specify y ~ x ^ 2 + x, R will compute y ~ x * x + x. x * x means the interaction of x with itself, which is the same as x. R automatically drops redundant variables so x + x become x, meaning that y ~ x ^ 2 + x specifies the function y = a_1 + a_2 * x. That’s probably not what you intended! Again, if you get confused about what your model is doing, you can always use model_matrix() to see exactly what equation lm() is fitting: df &lt;- tribble( ~ y, ~ x, 1, 1, 2, 2, 3, 3 ) model_matrix(df, y ~ x^2 + x) #&gt; # A tibble: 3 x 2 #&gt; `(Intercept)` x #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 2 #&gt; 3 1 3 model_matrix(df, y ~ I(x^2) + x) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `I(x^2)` x #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 1 #&gt; 2 1 4 2 #&gt; 3 1 9 3 Transformations are useful because you can use them to approximate non-linear functions. If you’ve taken a calculus class, you may have heard of Taylor’s theorem which says you can approximate any smooth function with an infinite sum of polynomials. That means you can use a polynomial function to get arbitrarily close to a smooth function by fitting an equation like y = a_1 + a_2 * x + a_3 * x^2 + a_4 * x ^ 3. Typing that sequence by hand is tedious, so R provides a helper function: poly(): model_matrix(df, y ~ poly(x, 2)) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `poly(x, 2)1` `poly(x, 2)2` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 -7.07e- 1 0.408 #&gt; 2 1 -7.85e-17 -0.816 #&gt; 3 1 7.07e- 1 0.408 However there’s one major problem with using poly(): outside the range of the data, polynomials rapidly shoot off to positive or negative infinity. One safer alternative is to use the natural spline, splines::ns(). library(splines) model_matrix(df, y ~ ns(x, 2)) #&gt; # A tibble: 3 x 3 #&gt; `(Intercept)` `ns(x, 2)1` `ns(x, 2)2` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0 0 #&gt; 2 1 0.566 -0.211 #&gt; 3 1 0.344 0.771 Let’s see what that looks like when we try and approximate a non-linear function: sim5 &lt;- tibble( x = seq(0, 3.5 * pi, length = 50), y = 4 * sin(x) + rnorm(length(x)) ) ggplot(sim5, aes(x, y)) + geom_point() I’m going to fit five models to this data. mod1 &lt;- lm(y ~ ns(x, 1), data = sim5) mod2 &lt;- lm(y ~ ns(x, 2), data = sim5) mod3 &lt;- lm(y ~ ns(x, 3), data = sim5) mod4 &lt;- lm(y ~ ns(x, 4), data = sim5) mod5 &lt;- lm(y ~ ns(x, 5), data = sim5) grid &lt;- sim5 %&gt;% data_grid(x = seq_range(x, n = 50, expand = 0.1)) %&gt;% gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = &quot;y&quot;) ggplot(sim5, aes(x, y)) + geom_point() + geom_line(data = grid, colour = &quot;red&quot;) + facet_wrap(~ model) Notice that the extrapolation outside the range of the data is clearly bad. This is the downside to approximating a function with a polynomial. But this is a very real problem with every model: the model can never tell you if the behaviour is true when you start extrapolating outside the range of the data that you have seen. You must rely on theory and science. 23.4.5 Exercises What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions? Use model_matrix() to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good shorthand for interaction? Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.) mod1 &lt;- lm(y ~ x1 + x2, data = sim3) mod2 &lt;- lm(y ~ x1 * x2, data = sim3) For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim? 23.5 Missing values Missing values obviously can not convey any information about the relationship between the variables, so modelling functions will drop any rows that contain missing values. R’s default behaviour is to silently drop them, but options(na.action = na.warn) (run in the prerequisites), makes sure you get a warning. df &lt;- tribble( ~ x, ~ y, 1, 2.2, 2, NA, 3, 3.5, 4, 8.3, NA, 10 ) mod &lt;- lm(y ~ x, data = df) #&gt; Warning: Dropping 2 rows with missing values To suppress the warning, set na.action = na.exclude: mod &lt;- lm(y ~ x, data = df, na.action = na.exclude) You can always see exactly how many observations were used with nobs(): nobs(mod) #&gt; [1] 3 23.6 Other model families This chapter has focussed exclusively on the class of linear models, which assume a relationship of the form y = a_1 * x1 + a_2 * x2 + ... + a_n * xn. Linear models additionally assume that the residuals have a normal distribution, which we haven’t talked about. There are a large set of model classes that extend the linear model in various interesting ways. Some of them are: Generalised linear models, e.g. stats::glm(). Linear models assume that the response is continuous and the error has a normal distribution. Generalised linear models extend linear models to include non-continuous responses (e.g. binary data or counts). They work by defining a distance metric based on the statistical idea of likelihood. Generalised additive models, e.g. mgcv::gam(), extend generalised linear models to incorporate arbitrary smooth functions. That means you can write a formula like y ~ s(x) which becomes an equation like y = f(x) and let gam() estimate what that function is (subject to some smoothness constraints to make the problem tractable). Penalised linear models, e.g. glmnet::glmnet(), add a penalty term to the distance that penalises complex models (as defined by the distance between the parameter vector and the origin). This tends to make models that generalise better to new datasets from the same population. Robust linear models, e.g. MASS:rlm(), tweak the distance to downweight points that are very far away. This makes them less sensitive to the presence of outliers, at the cost of being not quite as good when there are no outliers. Trees, e.g. rpart::rpart(), attack the problem in a completely different way than linear models. They fit a piece-wise constant model, splitting the data into progressively smaller and smaller pieces. Trees aren’t terribly effective by themselves, but they are very powerful when used in aggregate by models like random forests (e.g. randomForest::randomForest()) or gradient boosting machines (e.g. xgboost::xgboost.) These models all work similarly from a programming perspective. Once you’ve mastered linear models, you should find it easy to master the mechanics of these other model classes. Being a skilled modeller is a mixture of some good general principles and having a big toolbox of techniques. Now that you’ve learned some general tools and one useful class of models, you can go on and learn more classes from other sources. "],
["model-building.html", "24 Model building 24.1 Introduction 24.2 Why are low quality diamonds more expensive? 24.3 What affects the number of daily flights? 24.4 Learning more about models", " 24 Model building 24.1 Introduction In the previous chapter you learned how linear models work, and learned some basic tools for understanding what a model is telling you about your data. The previous chapter focussed on simulated datasets. This chapter will focus on real data, showing you how you can progressively build up a model to aid your understanding of the data. We will take advantage of the fact that you can think about a model partitioning your data into pattern and residuals. We’ll find patterns with visualisation, then make them concrete and precise with a model. We’ll then repeat the process, but replace the old response variable with the residuals from the model. The goal is to transition from implicit knowledge in the data and your head to explicit knowledge in a quantitative model. This makes it easier to apply to new domains, and easier for others to use. For very large and complex datasets this will be a lot of work. There are certainly alternative approaches - a more machine learning approach is simply to focus on the predictive ability of the model. These approaches tend to produce black boxes: the model does a really good job at generating predictions, but you don’t know why. This is a totally reasonable approach, but it does make it hard to apply your real world knowledge to the model. That, in turn, makes it difficult to assess whether or not the model will continue to work in the long-term, as fundamentals change. For most real models, I’d expect you to use some combination of this approach and a more classic automated approach. It’s a challenge to know when to stop. You need to figure out when your model is good enough, and when additional investment is unlikely to pay off. I particularly like this quote from reddit user Broseidon241: A long time ago in art class, my teacher told me “An artist needs to know when a piece is done. You can’t tweak something into perfection - wrap it up. If you don’t like it, do it over again. Otherwise begin something new”. Later in life, I heard “A poor seamstress makes many mistakes. A good seamstress works hard to correct those mistakes. A great seamstress isn’t afraid to throw out the garment and start over.” – Broseidon241, https://www.reddit.com/r/datascience/comments/4irajq 24.1.1 Prerequisites We’ll use the same tools as in the previous chapter, but add in some real datasets: diamonds from ggplot2, and flights from nycflights13. We’ll also need lubridate in order to work with the date/times in flights. library(tidyverse) library(modelr) options(na.action = na.warn) library(nycflights13) library(lubridate) 24.2 Why are low quality diamonds more expensive? In previous chapters we’ve seen a surprising relationship between the quality of diamonds and their price: low quality diamonds (poor cuts, bad colours, and inferior clarity) have higher prices. ggplot(diamonds, aes(cut, price)) + geom_boxplot() ggplot(diamonds, aes(color, price)) + geom_boxplot() ggplot(diamonds, aes(clarity, price)) + geom_boxplot() Note that the worst diamond color is J (slightly yellow), and the worst clarity is I1 (inclusions visible to the naked eye). 24.2.1 Price and carat It looks like lower quality diamonds have higher prices because there is an important confounding variable: the weight (carat) of the diamond. The weight of the diamond is the single most important factor for determining the price of the diamond, and lower quality diamonds tend to be larger. ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 50) We can make it easier to see how the other attributes of a diamond affect its relative price by fitting a model to separate out the effect of carat. But first, lets make a couple of tweaks to the diamonds dataset to make it easier to work with: Focus on diamonds smaller than 2.5 carats (99.7% of the data) Log-transform the carat and price variables. diamonds2 &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% mutate(lprice = log2(price), lcarat = log2(carat)) Together, these changes make it easier to see the relationship between carat and price: ggplot(diamonds2, aes(lcarat, lprice)) + geom_hex(bins = 50) The log-transformation is particularly useful here because it makes the pattern linear, and linear patterns are the easiest to work with. Let’s take the next step and remove that strong linear pattern. We first make the pattern explicit by fitting a model: mod_diamond &lt;- lm(lprice ~ lcarat, data = diamonds2) Then we look at what the model tells us about the data. Note that I back transform the predictions, undoing the log transformation, so I can overlay the predictions on the raw data: grid &lt;- diamonds2 %&gt;% data_grid(carat = seq_range(carat, 20)) %&gt;% mutate(lcarat = log2(carat)) %&gt;% add_predictions(mod_diamond, &quot;lprice&quot;) %&gt;% mutate(price = 2^lprice) ggplot(diamonds2, aes(carat, price)) + geom_hex(bins = 50) + geom_line(data = grid, colour = &quot;red&quot;, size = 1) That tells us something interesting about our data. If we believe our model, then the large diamonds are much cheaper than expected. This is probably because no diamond in this dataset costs more than $19,000. Now we can look at the residuals, which verifies that we’ve successfully removed the strong linear pattern: diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond, &quot;lresid&quot;) ggplot(diamonds2, aes(lcarat, lresid)) + geom_hex(bins = 50) Importantly, we can now re-do our motivating plots using those residuals instead of price. ggplot(diamonds2, aes(cut, lresid)) + geom_boxplot() ggplot(diamonds2, aes(color, lresid)) + geom_boxplot() ggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot() Now we see the relationship we expect: as the quality of the diamond increases, so too does its relative price. To interpret the y axis, we need to think about what the residuals are telling us, and what scale they are on. A residual of -1 indicates that lprice was 1 unit lower than a prediction based solely on its weight. \\(2^{-1}\\) is 1/2, points with a value of -1 are half the expected price, and residuals with value 1 are twice the predicted price. 24.2.2 A more complicated model If we wanted to, we could continue to build up our model, moving the effects we’ve observed into the model to make them explicit. For example, we could include color, cut, and clarity into the model so that we also make explicit the effect of these three categorical variables: mod_diamond2 &lt;- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2) This model now includes four predictors, so it’s getting harder to visualise. Fortunately, they’re currently all independent which means that we can plot them individually in four plots. To make the process a little easier, we’re going to use the .model argument to data_grid: grid &lt;- diamonds2 %&gt;% data_grid(cut, .model = mod_diamond2) %&gt;% add_predictions(mod_diamond2) grid #&gt; # A tibble: 5 x 5 #&gt; cut lcarat color clarity pred #&gt; &lt;ord&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Fair -0.515 G VS2 11.2 #&gt; 2 Good -0.515 G VS2 11.3 #&gt; 3 Very Good -0.515 G VS2 11.4 #&gt; 4 Premium -0.515 G VS2 11.4 #&gt; 5 Ideal -0.515 G VS2 11.4 ggplot(grid, aes(cut, pred)) + geom_point() If the model needs variables that you haven’t explicitly supplied, data_grid() will automatically fill them in with “typical” value. For continuous variables, it uses the median, and categorical variables it uses the most common value (or values, if there’s a tie). diamonds2 &lt;- diamonds2 %&gt;% add_residuals(mod_diamond2, &quot;lresid2&quot;) ggplot(diamonds2, aes(lcarat, lresid2)) + geom_hex(bins = 50) This plot indicates that there are some diamonds with quite large residuals - remember a residual of 2 indicates that the diamond is 4x the price that we expected. It’s often useful to look at unusual values individually: diamonds2 %&gt;% filter(abs(lresid2) &gt; 1) %&gt;% add_predictions(mod_diamond2) %&gt;% mutate(pred = round(2^pred)) %&gt;% select(price, pred, carat:table, x:z) %&gt;% arrange(price) #&gt; # A tibble: 16 x 11 #&gt; price pred carat cut color clarity depth table x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1013 264 0.25 Fair F SI2 54.4 64 4.3 4.23 2.32 #&gt; 2 1186 284 0.25 Premium G SI2 59 60 5.33 5.28 3.12 #&gt; 3 1186 284 0.25 Premium G SI2 58.8 60 5.33 5.28 3.12 #&gt; 4 1262 2644 1.03 Fair E I1 78.2 54 5.72 5.59 4.42 #&gt; 5 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; 6 1415 639 0.35 Fair G VS2 65.9 54 5.57 5.53 3.66 #&gt; # … with 10 more rows Nothing really jumps out at me here, but it’s probably worth spending time considering if this indicates a problem with our model, or if there are errors in the data. If there are mistakes in the data, this could be an opportunity to buy diamonds that have been priced low incorrectly. 24.2.3 Exercises In the plot of lcarat vs. lprice, there are some bright vertical strips. What do they represent? If log(price) = a_0 + a_1 * log(carat), what does that say about the relationship between price and carat? Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors? Does the final model, mod_diamond2, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond? 24.3 What affects the number of daily flights? Let’s work through a similar process for a dataset that seems even simpler at first glance: the number of flights that leave NYC per day. This is a really small dataset — only 365 rows and 2 columns — and we’re not going to end up with a fully realised model, but as you’ll see, the steps along the way will help us better understand the data. Let’s get started by counting the number of flights per day and visualising it with ggplot2. daily &lt;- flights %&gt;% mutate(date = make_date(year, month, day)) %&gt;% group_by(date) %&gt;% summarise(n = n()) daily #&gt; # A tibble: 365 x 2 #&gt; date n #&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 2013-01-01 842 #&gt; 2 2013-01-02 943 #&gt; 3 2013-01-03 914 #&gt; 4 2013-01-04 915 #&gt; 5 2013-01-05 720 #&gt; 6 2013-01-06 832 #&gt; # … with 359 more rows ggplot(daily, aes(date, n)) + geom_line() 24.3.1 Day of week Understanding the long-term trend is challenging because there’s a very strong day-of-week effect that dominates the subtler patterns. Let’s start by looking at the distribution of flight numbers by day-of-week: daily &lt;- daily %&gt;% mutate(wday = wday(date, label = TRUE)) ggplot(daily, aes(wday, n)) + geom_boxplot() There are fewer flights on weekends because most travel is for business. The effect is particularly pronounced on Saturday: you might sometimes leave on Sunday for a Monday morning meeting, but it’s very rare that you’d leave on Saturday as you’d much rather be at home with your family. One way to remove this strong pattern is to use a model. First, we fit the model, and display its predictions overlaid on the original data: mod &lt;- lm(n ~ wday, data = daily) grid &lt;- daily %&gt;% data_grid(wday) %&gt;% add_predictions(mod, &quot;n&quot;) ggplot(daily, aes(wday, n)) + geom_boxplot() + geom_point(data = grid, colour = &quot;red&quot;, size = 4) Next we compute and visualise the residuals: daily &lt;- daily %&gt;% add_residuals(mod) daily %&gt;% ggplot(aes(date, resid)) + geom_ref_line(h = 0) + geom_line() Note the change in the y-axis: now we are seeing the deviation from the expected number of flights, given the day of week. This plot is useful because now that we’ve removed much of the large day-of-week effect, we can see some of the subtler patterns that remain: Our model seems to fail starting in June: you can still see a strong regular pattern that our model hasn’t captured. Drawing a plot with one line for each day of the week makes the cause easier to see: ggplot(daily, aes(date, resid, colour = wday)) + geom_ref_line(h = 0) + geom_line() Our model fails to accurately predict the number of flights on Saturday: during summer there are more flights than we expect, and during Fall there are fewer. We’ll see how we can do better to capture this pattern in the next section. There are some days with far fewer flights than expected: daily %&gt;% filter(resid &lt; -100) #&gt; # A tibble: 11 x 4 #&gt; date n wday resid #&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; #&gt; 1 2013-01-01 842 Tue -109. #&gt; 2 2013-01-20 786 Sun -105. #&gt; 3 2013-05-26 729 Sun -162. #&gt; 4 2013-07-04 737 Thu -229. #&gt; 5 2013-07-05 822 Fri -145. #&gt; 6 2013-09-01 718 Sun -173. #&gt; # … with 5 more rows If you’re familiar with American public holidays, you might spot New Year’s day, July 4th, Thanksgiving and Christmas. There are some others that don’t seem to correspond to public holidays. You’ll work on those in one of the exercises. There seems to be some smoother long term trend over the course of a year. We can highlight that trend with geom_smooth(): daily %&gt;% ggplot(aes(date, resid)) + geom_ref_line(h = 0) + geom_line(colour = &quot;grey50&quot;) + geom_smooth(se = FALSE, span = 0.20) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; There are fewer flights in January (and December), and more in summer (May-Sep). We can’t do much with this pattern quantitatively, because we only have a single year of data. But we can use our domain knowledge to brainstorm potential explanations. 24.3.2 Seasonal Saturday effect Let’s first tackle our failure to accurately predict the number of flights on Saturday. A good place to start is to go back to the raw numbers, focussing on Saturdays: daily %&gt;% filter(wday == &quot;Sat&quot;) %&gt;% ggplot(aes(date, n)) + geom_point() + geom_line() + scale_x_date(NULL, date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) (I’ve used both points and lines to make it more clear what is data and what is interpolation.) I suspect this pattern is caused by summer holidays: many people go on holiday in the summer, and people don’t mind travelling on Saturdays for vacation. Looking at this plot, we might guess that summer holidays are from early June to late August. That seems to line up fairly well with the state’s school terms: summer break in 2013 was Jun 26–Sep 9. Why are there more Saturday flights in the Spring than the Fall? I asked some American friends and they suggested that it’s less common to plan family vacations during the Fall because of the big Thanksgiving and Christmas holidays. We don’t have the data to know for sure, but it seems like a plausible working hypothesis. Lets create a “term” variable that roughly captures the three school terms, and check our work with a plot: term &lt;- function(date) { cut(date, breaks = ymd(20130101, 20130605, 20130825, 20140101), labels = c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;) ) } daily &lt;- daily %&gt;% mutate(term = term(date)) daily %&gt;% filter(wday == &quot;Sat&quot;) %&gt;% ggplot(aes(date, n, colour = term)) + geom_point(alpha = 1 / 3) + geom_line() + scale_x_date(NULL, date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) (I manually tweaked the dates to get nice breaks in the plot. Using a visualisation to help you understand what your function is doing is a really powerful and general technique.) It’s useful to see how this new variable affects the other days of the week: daily %&gt;% ggplot(aes(wday, n, colour = term)) + geom_boxplot() It looks like there is significant variation across the terms, so fitting a separate day of week effect for each term is reasonable. This improves our model, but not as much as we might hope: mod1 &lt;- lm(n ~ wday, data = daily) mod2 &lt;- lm(n ~ wday * term, data = daily) daily %&gt;% gather_residuals(without_term = mod1, with_term = mod2) %&gt;% ggplot(aes(date, resid, colour = model)) + geom_line(alpha = 0.75) We can see the problem by overlaying the predictions from the model on to the raw data: grid &lt;- daily %&gt;% data_grid(wday, term) %&gt;% add_predictions(mod2, &quot;n&quot;) ggplot(daily, aes(wday, n)) + geom_boxplot() + geom_point(data = grid, colour = &quot;red&quot;) + facet_wrap(~ term) Our model is finding the mean effect, but we have a lot of big outliers, so mean tends to be far away from the typical value. We can alleviate this problem by using a model that is robust to the effect of outliers: MASS::rlm(). This greatly reduces the impact of the outliers on our estimates, and gives a model that does a good job of removing the day of week pattern: mod3 &lt;- MASS::rlm(n ~ wday * term, data = daily) daily %&gt;% add_residuals(mod3, &quot;resid&quot;) %&gt;% ggplot(aes(date, resid)) + geom_hline(yintercept = 0, size = 2, colour = &quot;white&quot;) + geom_line() It’s now much easier to see the long-term trend, and the positive and negative outliers. 24.3.3 Computed variables If you’re experimenting with many models and many visualisations, it’s a good idea to bundle the creation of variables up into a function so there’s no chance of accidentally applying a different transformation in different places. For example, we could write: compute_vars &lt;- function(data) { data %&gt;% mutate( term = term(date), wday = wday(date, label = TRUE) ) } Another option is to put the transformations directly in the model formula: wday2 &lt;- function(x) wday(x, label = TRUE) mod3 &lt;- lm(n ~ wday2(date) * term(date), data = daily) Either approach is reasonable. Making the transformed variable explicit is useful if you want to check your work, or use them in a visualisation. But you can’t easily use transformations (like splines) that return multiple columns. Including the transformations in the model function makes life a little easier when you’re working with many different datasets because the model is self contained. 24.3.4 Time of year: an alternative approach In the previous section we used our domain knowledge (how the US school term affects travel) to improve the model. An alternative to using our knowledge explicitly in the model is to give the data more room to speak. We could use a more flexible model and allow that to capture the pattern we’re interested in. A simple linear trend isn’t adequate, so we could try using a natural spline to fit a smooth curve across the year: library(splines) mod &lt;- MASS::rlm(n ~ wday * ns(date, 5), data = daily) daily %&gt;% data_grid(wday, date = seq_range(date, n = 13)) %&gt;% add_predictions(mod) %&gt;% ggplot(aes(date, pred, colour = wday)) + geom_line() + geom_point() We see a strong pattern in the numbers of Saturday flights. This is reassuring, because we also saw that pattern in the raw data. It’s a good sign when you get the same signal from different approaches. 24.3.5 Exercises Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year? What do the three days with high positive residuals represent? How would these days generalise to another year? daily %&gt;% top_n(3, resid) #&gt; # A tibble: 3 x 5 #&gt; date n wday resid term #&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 2013-11-30 857 Sat 112. fall #&gt; 2 2013-12-01 987 Sun 95.5 fall #&gt; 3 2013-12-28 814 Sat 69.4 fall Create a new variable that splits the wday variable into terms, but only for Saturdays, i.e. it should have Thurs, Fri, but Sat-summer, Sat-spring, Sat-fall. How does this model compare with the model with every combination of wday and term? Create a new wday variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like? What happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful? What would you expect the model n ~ wday + ns(date, 5) to look like? Knowing what you know about the data, why would you expect it to be not particularly effective? We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday. 24.4 Learning more about models We have only scratched the absolute surface of modelling, but you have hopefully gained some simple, but general-purpose tools that you can use to improve your own data analyses. It’s OK to start simple! As you’ve seen, even very simple models can make a dramatic difference in your ability to tease out interactions between variables. These modelling chapters are even more opinionated than the rest of the book. I approach modelling from a somewhat different perspective to most others, and there is relatively little space devoted to it. Modelling really deserves a book on its own, so I’d highly recommend that you read at least one of these three books: Statistical Modeling: A Fresh Approach by Danny Kaplan, http://www.mosaic-web.org/go/StatisticalModeling/. This book provides a gentle introduction to modelling, where you build your intuition, mathematical tools, and R skills in parallel. The book replaces a traditional “introduction to statistics” course, providing a curriculum that is up-to-date and relevant to data science. An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, http://www-bcf.usc.edu/~gareth/ISL/ (available online for free). This book presents a family of modern modelling techniques collectively known as statistical learning. For an even deeper understanding of the math behind the models, read the classic Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, http://statweb.stanford.edu/~tibs/ElemStatLearn/ (also available online for free). Applied Predictive Modeling by Max Kuhn and Kjell Johnson, http://appliedpredictivemodeling.com. This book is a companion to the caret package and provides practical tools for dealing with real-life predictive modelling challenges. "],
["muchos-modelos.html", "25 Muchos modelos 25.1 Introducción 25.2 gapminder 25.3 Columnas-lista 25.4 Creando columnas-lista 25.5 Simplificando columnas-lista 25.6 Haciendo datos ordenados con broom", " 25 Muchos modelos 25.1 Introducción En este capítulo vas a aprender tres ideas poderosas que te van a ayudar a trabajar fácilmente con un gran número de modelos: Usar muchos modelos simples para entender mejor conjuntos de datos complejos. Usar columnas-lista (list-columns) para almacenar estructuras de datos arbitrarias en un data frame. Ésto, por ejemplo, te permitirá tener una columna que contenga modelos lineales. Usar el paquete broom, de David Robinson, para transformar modelos en datos ordenados. Ésta es una técnica poderosa para trabajar con un gran número de modelos porque una vez que tienes datos ordenados, puedes aplicar todas las técnicas que has aprendido anteriormente en el libro. Empezaremos entrando de lleno en un ejemplo motivador usando datos sobre la esperanza de vida alrededor del mundo. Es un conjunto de datos pequeño pero que ilustra cuán importante puede ser modelar para mejorar tus visualizaciones. Utilizaremos un número grande de modelos simples para extraer algunas de las señales más fuertes y que podamos ver las sutiles que permanecen. También veremos cómo las medidas de resumen de los modelos nos pueden ayudar a encontrar datos atípicos y tendencias inusuales. Las siguientes secciones ahondarán en más detalles acerca de las técnicas: En columnas-lista, aprenderás más acerca de la estructura de datos columna-lista, y por qué es válido poner listas en data frames. En creando columnas-lista, aprenderás las tres maneras principales en las que crearás columnas-lista. En simplificando columnas-lista aprenderás cómo convertir columnas-lista de vuelta a vectores atómicos regulares (o conjuntos de vectores atómicos) para que puedas trabajar con ellos más fácilmente. En haciendo datos ordenados con broom, aprenderás sobre el conjunto de herramientas completo provisto por broom (del inglés escoba), y verás cómo puede ser aplicado a otros tipos de estructuras de datos. Este capítulo es aspiracional en cierta medida: si este libro es tu primera introducción a R, este capítulo es probable que sea una lucha para ti. Requiere que tengas profundamente internalizadas ideas acerca de modelamiento, estructuras de datos, e iteración. Así que no te preocupes si no lo entiendes — solo aparta este capítulo por un par de meses, y vuelve cuando quieras ejercitar tu cerebro. 25.1.1 Prerrequisitos Trabajar con muchos modelos requiere muchos de los paquetes del tidyverse (para exploración de datos, doma y programación) y modelr para facilitar el modelamiento. library(modelr) library(tidyverse) 25.2 gapminder Para motivar el poder de muchos modelos simples, vamos a mirar los datos de “gapminder”. Estos datos fueron popularizados por Hans Rosling, un doctor y estadístico sueco. Si nunca has escuchado de él, para de leer este capítulo ahora mismo y ve a mirar uno de sus videos! Él es un presentador de datos fantástico e ilustra cómo puedes usar datos para presentar una historia convincente. Un buen lugar para empezar es este video corto filmado en conjunto con la BBC: https://www.youtube.com/watch?v=jbkSRLYSojo. Los datos de gapminder resumen la progresión de países a través del tiempo, mirando estadísticos como esperanza de vida y PIB. Los datos son de fácil acceso en R, gracias a Jenny Bryan que creó el paquete gapminder. Utilizaremos la versión en español contenida en el paquete datos (TODO: ver cómo poner esto): library(datos) paises #&gt; # A tibble: 1,704 x 6 #&gt; pais continente anio esperanza_de_vida poblacion pib_per_capita #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afganistán Asia 1952 28.8 8425333 779. #&gt; 2 Afganistán Asia 1957 30.3 9240934 821. #&gt; 3 Afganistán Asia 1962 32.0 10267083 853. #&gt; 4 Afganistán Asia 1967 34.0 11537966 836. #&gt; 5 Afganistán Asia 1972 36.1 13079460 740. #&gt; 6 Afganistán Asia 1977 38.4 14880372 786. #&gt; # … with 1,698 more rows En este caso de estudio, nos enfocaremos en solo tres variables para responder la pregunta “¿Cómo la esperanza de vida (esperanza_de_vida) cambia a través del tiempo (anio) para cada país (pais)?”. Un buen lugar para empezar es con un gráfico: paises %&gt;% ggplot(aes(anio, esperanza_de_vida, group = pais)) + geom_line(alpha = 1 / 3) Es un conjunto de datos pequeño: solo tiene ~1,700 observaciones y 3 variables. Pero aún así es difícil ver qué está pasando! En general, parece que la esperanza de vida ha estado mejorando en forma constante. Sin embargo, si miras de cerca, puedes notar algunos países que no siguen este patrón. ¿Cómo podemos hacer que esos países se vean más fácilmente? Una forma es usar el mismo enfoque que en el último capítulo: hay una señal fuerte (en general crecimiento lineal) que hace difícil ver tendencias más sutiles. Separaremos estos factores estimando un modelo con una tendencia lineal. El modelo captura el crecimiento estable en el tiempo, y los residuos mostrarán lo que queda fuera. Ya sabes cómo hacer eso si tenemos un solo país: nz &lt;- filter(paises, pais == &quot;Nueva Zelandia&quot;) nz %&gt;% ggplot(aes(anio, esperanza_de_vida)) + geom_line() + ggtitle(&quot;Datos completos = &quot;) nz_mod &lt;- lm(esperanza_de_vida ~ anio, data = nz) nz %&gt;% add_predictions(nz_mod) %&gt;% ggplot(aes(anio, pred)) + geom_line() + ggtitle(&quot;Tendencia lineal + &quot;) nz %&gt;% add_residuals(nz_mod) %&gt;% ggplot(aes(anio, resid)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 3) + geom_line() + ggtitle(&quot;Patrón restante&quot;) ¿Cómo podemos ajustar fácilmente ese modelo para cada país? 25.2.1 Datos anidados Te puedes imaginar copiando y pegando ese código múltiples veces; pero ya has aprendido una mejor forma! Extrae el código en común con una función y repítelo usando una función map (TODO: no me queda claro que tenga sentido poner que el nombre de esta función venga del inglés, y no sé si ponerla en cursiva) de purrr. Este problema se estructura un poco diferente respecto a lo que has visto antes. En lugar de repetir una acción por cada variable, queremos repetirla para cada país, un subconjunto de filas. Para hacer eso, necesitamos una nueva estructura de datos: el data frame anidado (nested data frame). Para crear un data frame anidado empezamos con un data frame agrupado, y lo “anidamos”: por_pais &lt;- paises %&gt;% group_by(pais, continente) %&gt;% nest() por_pais #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistán Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows (Estoy haciendo un poco de trampa agrupando por continente y pais al mismo tiempo. Dado el pais, continente es fijo, así que no agrega ningún grupo más, pero es una forma fácil de llevarnos una variable adicional para el camino.) Ésto crea un data frame que tiene una fila por grupo (por país), y una columna bastante inusual: data. data es una lista de data frames (o tibbles, para ser precisos). Esto parece una idea un poco loca: tenemos un data frame con una columna que es una lista de otros data frames! Explicaré brevemente por qué pienso que es una buena idea. La columna data es un poco difícil de examinar porque es una lista moderadamente complicada, y todavía estamos trabajando para tener buenas herramientas para explorar estos objetos. Desafortunadamente usar str() no es recomendable porque usualmente producirá un output (salida de código) muy extenso. Pero si extraes un solo elemento de la columna data verás que contiene todos los datos para ese país (en este caso, Afganistán). por_pais$data[[1]] #&gt; # A tibble: 12 x 4 #&gt; anio esperanza_de_vida poblacion pib_per_capita #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1952 28.8 8425333 779. #&gt; 2 1957 30.3 9240934 821. #&gt; 3 1962 32.0 10267083 853. #&gt; 4 1967 34.0 11537966 836. #&gt; 5 1972 36.1 13079460 740. #&gt; 6 1977 38.4 14880372 786. #&gt; # … with 6 more rows Nota la diferencia entre un data frame agrupado estándar y un data frame anidado: en un data frame agrupado, cada fila es una observación; en un data frame anidado, cada fila es un grupo. Otra forma de pensar en un conjunto de datos anidado es que ahora tenemos una meta-observación: una fila que representa todo el transcurso de tiempo para un país, en lugar de solo un punto en el tiempo. 25.2.2 Columnas-lista Ahora que tenemos nuestro data frame anidado, estamos en una buena posición para ajustar algunos modelos. Tenemos una función para ajustar modelos: modelo_pais &lt;- function(df) { lm(esperanza_de_vida ~ anio, data = df) } Y queremos aplicarlo a cada data frame. Los data frames están en una lista, así que podemos usar purrr::map() para aplicar modelo_pais a cada elemento: modelos &lt;- map(por_pais$data, modelo_pais) Sin embargo, en lugar de dejar la lista de modelos como un objeto suelto, creo que es mejor almacenarlo como una columna en el data frame por_pais. Almacenar objetos relacionados en columnas es una parte clave del valor de los data frames, y por eso pienso que las columnas-lista son tan buena idea. En el transcurso de nuetro trabajo con estos países, vamos a tener muchas listas donde tenemos un elemento por país. ¿Por qué no almacenarlos todos juntos en un data frame? En otras palabras, en lugar de crear un nuevo objeto en el entorno global, vamos a crear una nueva variable en el data frame por_pais. Ese es un trabajo para dplyr::mutate(): por_pais &lt;- por_pais %&gt;% mutate(modelo = map(data, modelo_pais)) por_pais #&gt; # A tibble: 142 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Afganistán Asia &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; # … with 136 more rows Esto tiene una gran ventaja: como todos los objetos relacionados están almacenados juntos, no necesitas manualmente mantenerlos sincronizados cuando filtras o reordenas. La semántica del data frame se ocupa de esto por ti: por_pais %&gt;% filter(continente == &quot;Europa&quot;) #&gt; # A tibble: 30 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Albania Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 2 Austria Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 3 Bélgica Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 4 Bosnia y Herzegovina Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 5 Bulgaria Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 6 Croacia Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; # … with 24 more rows por_pais %&gt;% arrange(continente, pais) #&gt; # A tibble: 142 x 4 #&gt; pais continente data modelo #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Argelia África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 2 Angola África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 3 Benin África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 4 Botswana África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 5 Burkina Faso África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; 6 Burundi África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; #&gt; # … with 136 more rows Si tu lista de data frames y lista de modelos fueran objetos separados, tienes (TODO: acá usaría “tendrías” pero no es lo que usa el original) que acordarte de que cuando reordenas o seleccionas un subconjunto de un vector, necesitas reordenar o seleccionar el subconjunto de todos los demás para mantenerlos sincronizados. Si te olvidas, tu código va a seguir funcionando, pero va a devolver la respuesta equivocada! 25.2.3 Desanidando Previamente calculamos los residuos de un único modelo con un conjunto de datos también único. Ahora tenemos 142 data frames y 142 modelos. Para calcular los residuos, necesitamos llamar a la función add_residuals() (del inglés adicionar residuos) con cada par modelo-datos: por_pais &lt;- por_pais %&gt;% mutate( residuos = map2(data, modelo, add_residuals) ) por_pais #&gt; # A tibble: 142 x 5 #&gt; pais continente data modelo residuos #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Afganistán Asia &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt; &lt;tibble [12 × 5]&gt; #&gt; # … with 136 more rows ¿Pero cómo puedes graficar una lista de data frames? En lugar de luchar para contestar esa pregunta, transformemos la lista de data frames de vuelta en un data frame regular. Previamente usamos nest() (del inglés anidar) para transformar un data frame regular en uno anidado, y ahora desanidamos con unnest(): residuos &lt;- unnest(por_pais, residuos) residuos #&gt; # A tibble: 1,704 x 7 #&gt; pais continente anio esperanza_de_vi… poblacion pib_per_capita resid #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afgan… Asia 1952 28.8 8425333 779. -1.11 #&gt; 2 Afgan… Asia 1957 30.3 9240934 821. -0.952 #&gt; 3 Afgan… Asia 1962 32.0 10267083 853. -0.664 #&gt; 4 Afgan… Asia 1967 34.0 11537966 836. -0.0172 #&gt; 5 Afgan… Asia 1972 36.1 13079460 740. 0.674 #&gt; 6 Afgan… Asia 1977 38.4 14880372 786. 1.65 #&gt; # … with 1,698 more rows Nota que cada columna regular está repetida una vez por cada fila en la columna anidada. Ahora tenemos un data frame regular, podemos graficar los residuos: residuos %&gt;% ggplot(aes(anio, resid)) + geom_line(aes(group = pais), alpha = 1 / 3) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Separar facetas por continente es particularmente revelador: residuos %&gt;% ggplot(aes(anio, resid, group = pais)) + geom_line(alpha = 1 / 3) + facet_wrap(~ continente) Parece que hemos perdido algunos patrones suaves. También hay algo interesante pasando en África: vemos algunos residuos muy grandes lo que sugiere que nuestro modelo no está ajustando muy bien. Exploraremos más eso en la próxima sección, atacando el problema desde un ángulo un poco diferente. 25.2.4 Calidad del modelo En lugar de examinar los residuos del modelo, podríamos examinar algunas medidas generales de la calidad del modelo. Aprendiste cómo calcular algunas medidas específicas en el capítulo anterior. Aquí mostraremos un enfoque diferente usando el paquete broom. El paquete broom provee un conjunto de funciones generales para transformar modelos en datos ordenados. Aquí utilizaremos broom::glance() (del inglés vistazo) para extraer algunas métricas de la calidad del modelo. Si lo aplicamos a un modelo, obtenemos un data frame con una única fila: broom::glance(nz_mod) #&gt; # A tibble: 1 x 11 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.954 0.949 0.804 205. 5.41e-8 2 -13.3 32.6 34.1 #&gt; # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Podemos usar mutate() y unnest() para crear un data frame con una fila por cada país: por_pais %&gt;% mutate(glance = map(modelo, broom::glance)) %&gt;% unnest(glance) #&gt; # A tibble: 142 x 16 #&gt; pais continente data modelo residuos r.squared adj.r.squared sigma #&gt; &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afga… Asia &lt;tib… &lt;S3: … &lt;tibble… 0.948 0.942 1.22 #&gt; 2 Alba… Europa &lt;tib… &lt;S3: … &lt;tibble… 0.911 0.902 1.98 #&gt; 3 Arge… África &lt;tib… &lt;S3: … &lt;tibble… 0.985 0.984 1.32 #&gt; 4 Ango… África &lt;tib… &lt;S3: … &lt;tibble… 0.888 0.877 1.41 #&gt; 5 Arge… Américas &lt;tib… &lt;S3: … &lt;tibble… 0.996 0.995 0.292 #&gt; 6 Aust… Oceanía &lt;tib… &lt;S3: … &lt;tibble… 0.980 0.978 0.621 #&gt; # … with 136 more rows, and 8 more variables: statistic &lt;dbl&gt;, #&gt; # p.value &lt;dbl&gt;, df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, #&gt; # deviance &lt;dbl&gt;, df.residual &lt;int&gt; Este no es exactamente el output que queremos, porque aún incluye todas las columnas que son una lista. Éste es el comportamiento por defecto cuando unnest() trabaja sobre data frames con una única fila. Para suprimir esas columnas usamos .drop = TRUE (drop — del inglés descartar): glance &lt;- por_pais %&gt;% mutate(glance = map(modelo, broom::glance)) %&gt;% unnest(glance, .drop = TRUE) glance #&gt; # A tibble: 142 x 13 #&gt; pais continente r.squared adj.r.squared sigma statistic p.value df #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Afga… Asia 0.948 0.942 1.22 181. 9.84e- 8 2 #&gt; 2 Alba… Europa 0.911 0.902 1.98 102. 1.46e- 6 2 #&gt; 3 Arge… África 0.985 0.984 1.32 662. 1.81e-10 2 #&gt; 4 Ango… África 0.888 0.877 1.41 79.1 4.59e- 6 2 #&gt; 5 Arge… Américas 0.996 0.995 0.292 2246. 4.22e-13 2 #&gt; 6 Aust… Oceanía 0.980 0.978 0.621 481. 8.67e-10 2 #&gt; # … with 136 more rows, and 5 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, #&gt; # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; (Presta atención a las variables que no se imprimieron: hay mucha información útil allí.) Con este data frame, podemos empezar a buscar modelos que no se ajustan bien: glance %&gt;% arrange(r.squared) #&gt; # A tibble: 142 x 13 #&gt; pais continente r.squared adj.r.squared sigma statistic p.value df #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Ruan… África 0.0172 -0.0811 6.56 0.175 0.685 2 #&gt; 2 Bots… África 0.0340 -0.0626 6.11 0.352 0.566 2 #&gt; 3 Zimb… África 0.0562 -0.0381 7.21 0.596 0.458 2 #&gt; 4 Zamb… África 0.0598 -0.0342 4.53 0.636 0.444 2 #&gt; 5 Swaz… África 0.0682 -0.0250 6.64 0.732 0.412 2 #&gt; 6 Leso… África 0.0849 -0.00666 5.93 0.927 0.358 2 #&gt; # … with 136 more rows, and 5 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, #&gt; # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; Los peores modelos parecen estar todos en África. Vamos a chequear ésto con un gráfico. Tenemos un número relativamente chico de observaciones y una variable discreta, así que geom_jitter() (TODO: en inglés jitter es temblar o algo similar, pero no sé si estaría bien poner acá eso porque no es exactamente eso…) es efectiva: glance %&gt;% ggplot(aes(continente, r.squared)) + geom_jitter(width = 0.5) Podríamos quitar los países con un \\(R^2\\) particularmente malo y graficar los datos: mal_ajuste &lt;- filter(glance, r.squared &lt; 0.25) paises %&gt;% semi_join(mal_ajuste, by = &quot;pais&quot;) %&gt;% ggplot(aes(anio, esperanza_de_vida, colour = pais)) + geom_line() Vemos dos efectos principales aquí: las tragedias de la epidemia de VIH/SIDA y el genocidio de Ruanda. 25.2.5 Ejercicios Una tendencia lineal parece ser demasiado simple para la tendencia general. ¿Puedes hacerlo mejor con un polinomio cuadrático? ¿Cómo puedes interpretar el coeficiente del término cuadrático? (Pista: puedes querer transformar year para que tenga media cero.) Explora otros métodos para visualizar la distribución del \\(R^2\\) por continente. Puedes querer probar el paquete ggbeeswarm, que provee métodos similares para evitar superposiciones como jitter, pero usa métodos determinísticos. Para crear el último gráfico (mostrando los datos para los países con los peores ajustes del modelo), precisamos dos pasos: creamos un data frame con una fila por país y después hicimos un semi-join (del inglés semi juntar) (TODO: deberíamos aclarar algo?) al conjunto de datos original. Es posible evitar este join si usamos unnest() en lugar de unnest(.drop = TRUE). ¿Cómo? 25.3 Columnas-lista Ahora que has visto un flujo de trabajo básico para manejar muchos modelos, vamos a sumergirnos en algunos detalles. En esta sección, exploraremos en más detalle la estructura de datos columna-lista. Solo recientemente es que he comenzado a apreciar realmente la idea de la columna-lista. Esta estructura está implícita en la definición de data frame: un data frame es una lista nombrada de vectores de igual largo. Una lista es un vector, así que siempre ha sido legítimo usar una lista como una columna de un data frame. Sin embargo, R base no hace las cosas fáciles para crear columnas-lista, y data.frame() trata a la lista como una lista de columnas: data.frame(x = list(1:3, 3:5)) #&gt; x.1.3 x.3.5 #&gt; 1 1 3 #&gt; 2 2 4 #&gt; 3 3 5 Puedes prevenir que data.frame() haga esto con I(), pero el resultado no se imprime particularmente bien: data.frame( x = I(list(1:3, 3:5)), y = c(&quot;1, 2&quot;, &quot;3, 4, 5&quot;) ) #&gt; x y #&gt; 1 1, 2, 3 1, 2 #&gt; 2 3, 4, 5 3, 4, 5 Tibble mitiga este problema siendo más perezoso (TODO: lazier) (tibble() no modifica sus inputs) y proporcionando un mejor método de impresión: tibble( x = list(1:3, 3:5), y = c(&quot;1, 2&quot;, &quot;3, 4, 5&quot;) ) #&gt; # A tibble: 2 x 2 #&gt; x y #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;int [3]&gt; 1, 2 #&gt; 2 &lt;int [3]&gt; 3, 4, 5 Es incluso más fácil con tribble() ya que automáticamente puede interpretar que necesitas una lista: tribble( ~ x, ~ y, 1:3, &quot;1, 2&quot;, 3:5, &quot;3, 4, 5&quot; ) #&gt; # A tibble: 2 x 2 #&gt; x y #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;int [3]&gt; 1, 2 #&gt; 2 &lt;int [3]&gt; 3, 4, 5 Columnas-lista son usualmente más útiles como estructuras de datos intermedias. Es difícil trabajar con ellas directamente, porque la mayoría de las funciones de R trabajan con vectores atómicos o data frames, pero la ventaja de mantener ítems relacionados juntos en un data frame hace que valga la pena un poco de molestia. Generalmente hay tres partes de un pipeline (del inglés tubería) efectivo de columnas-lista: Creas la columna-lista usando uno entre nest(), summarise() + list(), o mutate() + una función map, como se describió en Creando columnas-lista. Creas otra columna-lista intermedia transformando columnas lista existentes con map(), map2() o pmap(). Por ejemplo, en el caso de estudio de arriba, creamos una columna-lista de modelos transformando una columna-lista de data frames. Simplificas la columna-lista de vuelta en un data frame o vector atómico, como se describió en Simplificando columnas-lista. 25.4 Creando columnas-lista Típicamente, no crearás columnas-lista con tibble(). En cambio, las crearás a partir de columnas regulares, usando uno de tres métodos: Con tidyr::nest() para convertir un data frame agrupado en uno anidado donde tengas columnas-lista de data frames. Con mutate() y funciones vectorizadas que retornan una lista. Con summarise() y funciones de resumen que retornan múltiples resultados. Alternativamente, podrías crearlas a partir de una lista nombrada, usando tibble::enframe(). Generalmente, cuando creas columnas-lista, debes asegurarte de que sean homogéneas: cada elemento debe contener el mismo tipo de cosa. No hay chequeos para asegurarte de que sea así, pero si usas purrr y recuerdas lo que aprendiste sobre funciones de tipo estable (TODO: type-stable functions), encontrarás que eso pasa naturalmente. 25.4.1 Con anidación nest() crea un data frame anidado, que es un data frame con una columna-lista de data frames. En un data frame anidado cada fila es una meta-observación: las otras columnas son variables que definen la observación (como país y continente arriba), y la columna-lista de data frames tiene las observaciones individuales que construyen la meta-observación. Hay dos formas de usar nest(). Hasta ahora has visto cómo usarlo con un data frame agrupado. Cuando se aplica a un data frame agrupado, nest() mantiene las columnas que agrupan tal cual, y envuelve todo lo demás en la columna-lista: paises %&gt;% group_by(pais, continente) %&gt;% nest() #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistán Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows También lo puedes usar en un data frame no agrupado, especificando cuáles columnas quieres anidar: paises %&gt;% nest(anio:pib_per_capita) #&gt; # A tibble: 142 x 3 #&gt; pais continente data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Afganistán Asia &lt;tibble [12 × 4]&gt; #&gt; 2 Albania Europa &lt;tibble [12 × 4]&gt; #&gt; 3 Argelia África &lt;tibble [12 × 4]&gt; #&gt; 4 Angola África &lt;tibble [12 × 4]&gt; #&gt; 5 Argentina Américas &lt;tibble [12 × 4]&gt; #&gt; 6 Australia Oceanía &lt;tibble [12 × 4]&gt; #&gt; # … with 136 more rows 25.4.2 A partir de funciones vectorizadas Algunas funciones útiles toman un vector atómico y retornan una lista. Por ejemplo, en strings (TODO: chequear cómo se llama el capítulo en español) aprendiste stringr::str_split() que toma un vector de caracteres y retorna una lista de vectores de caracteres. Si lo usas dentro de mutate (TODO: no sé si ponerlo en cursiva o dejarlo como el original), obtendrás una columna-lista: df &lt;- tribble( ~ x1, &quot;a,b,c&quot;, &quot;d,e,f,g&quot; ) df %&gt;% mutate(x2 = stringr::str_split(x1, &quot;,&quot;)) #&gt; # A tibble: 2 x 2 #&gt; x1 x2 #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a,b,c &lt;chr [3]&gt; #&gt; 2 d,e,f,g &lt;chr [4]&gt; unnest() sabe cómo manejar estas listas de vectores: df %&gt;% mutate(x2 = stringr::str_split(x1, &quot;,&quot;)) %&gt;% unnest() #&gt; # A tibble: 7 x 2 #&gt; x1 x2 #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a,b,c a #&gt; 2 a,b,c b #&gt; 3 a,b,c c #&gt; 4 d,e,f,g d #&gt; 5 d,e,f,g e #&gt; 6 d,e,f,g f #&gt; # … with 1 more row (Si usas mucho este patrón, asegúrate de chequear tidyr::separate_rows() (del inglés separar filas) que es un wrapper (TODO: cuando esté el capítulo de Joshua poner explicación) alrededor de este patrón común). Otro ejemplo de este patrón es usar map(), map2(), pmap() de purrr. Por ejemplo, podríamos tomar el ejemplo final de Invoking different functions (TODO: chequear nombre en español) y reescribirlo usando mutate(): sim &lt;- tribble( ~ f, ~ params, &quot;runif&quot;, list(min = -1, max = 1), &quot;rnorm&quot;, list(sd = 5), &quot;rpois&quot;, list(lambda = 10) ) sim %&gt;% mutate(sims = invoke_map(f, params, n = 10)) #&gt; # A tibble: 3 x 3 #&gt; f params sims #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 runif &lt;list [2]&gt; &lt;dbl [10]&gt; #&gt; 2 rnorm &lt;list [1]&gt; &lt;dbl [10]&gt; #&gt; 3 rpois &lt;list [1]&gt; &lt;int [10]&gt; Nota que técnicamente sim no es homogénea porque contiene vectores con tipo de datos dobles así como enteros (TODO: both double and integer vectors). Sin embargo, es probable que ésto no cause muchos problemas porque ambos vectores son numéricos. 25.4.3 A partir de medidas de resumen con más de un valor Una restricción de summarise() es que solo funciona con funciones de resumen que retornan un único valor. Eso significa que no puedes usarlo con funciones como quantile() que retorna un vector de largo arbitrario: mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = quantile(millas)) #&gt; Error in summarise_impl(.data, dots): Column `q` must be length 1 (a summary value), not 5 Sin embargo, puedes envolver el resultado en una lista! Esto obedece el contrato de summarise(), porque cada resumen ahora es una lista (un vector) de largo 1. mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = list(quantile(millas))) #&gt; # A tibble: 3 x 2 #&gt; cilindros q #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 4 &lt;dbl [5]&gt; #&gt; 2 6 &lt;dbl [5]&gt; #&gt; 3 8 &lt;dbl [5]&gt; Para producir resultados útiles con unnest, también necesitarás capturar las probabilidades: probs &lt;- c(0.01, 0.25, 0.5, 0.75, 0.99) mtautos %&gt;% group_by(cilindros) %&gt;% summarise(p = list(probs), q = list(quantile(millas, probs))) %&gt;% unnest() #&gt; # A tibble: 15 x 3 #&gt; cilindros p q #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 0.01 21.4 #&gt; 2 4 0.25 22.8 #&gt; 3 4 0.5 26 #&gt; 4 4 0.75 30.4 #&gt; 5 4 0.99 33.8 #&gt; 6 6 0.01 17.8 #&gt; # … with 9 more rows 25.4.4 A partir de una lista nombrada Data frames con columnas-lista proveen una solución a un problema común: ¿qué haces si quieres iterar sobre el contenido de una lista y también sobre sus elementos? En lugar de tratar de juntar todo en un único objeto, usualmente es más fácil hacer un data frame: una columna puede contener los elementos y otra columna la lista. Una forma fácil de crear un data frame como éste desde una lista es tibble::enframe(). x &lt;- list( a = 1:5, b = 3:4, c = 5:6 ) df &lt;- enframe(x) df #&gt; # A tibble: 3 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 a &lt;int [5]&gt; #&gt; 2 b &lt;int [2]&gt; #&gt; 3 c &lt;int [2]&gt; La ventaja de esta estructura es que se generaliza de una forma relativamente sencilla - los nombres son útiles si tienes como metadata vectores de caracteres, pero no ayudan para otros tipos de datos o para múltiples vectores. Ahora, si quieres iterar sobre los nombres y valores en paralelo, puedes usar map2(): df %&gt;% mutate( smry = map2_chr(name, value, ~ stringr::str_c(.x, &quot;: &quot;, .y[1])) ) #&gt; # A tibble: 3 x 3 #&gt; name value smry #&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 a &lt;int [5]&gt; a: 1 #&gt; 2 b &lt;int [2]&gt; b: 3 #&gt; 3 c &lt;int [2]&gt; c: 5 25.4.5 Ejercicios Lista todas las funciones en las que puedas pensar que tomen como input un vector atómico y retornen una lista. Piensa en funciones de resumen útiles que, como quantile(), retornen múltiples valores. ¿Qué es lo que falta en el siguiente data frame? ¿Cómo quantile() retorna eso que falta? ¿Por qué eso no es tan útil aquí? mtautos %&gt;% group_by(cilindros) %&gt;% summarise(q = list(quantile(millas))) %&gt;% unnest() #&gt; # A tibble: 15 x 2 #&gt; cilindros q #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 4 21.4 #&gt; 2 4 22.8 #&gt; 3 4 26 #&gt; 4 4 30.4 #&gt; 5 4 33.9 #&gt; 6 6 17.8 #&gt; # … with 9 more rows ¿Qué hace este código? ¿Por qué podría ser útil? mtautos %&gt;% group_by(cilindros) %&gt;% summarise_each(funs(list)) 25.5 Simplificando columnas-lista Para aplicar las técnicas de manipulación de datos y visualización que has aprendido en este libro, necesitarás simplificar la columna-lista de vuelta a una columna regular (un vector atómico), o conjunto de columnas. La técnica que usarás para volver a una estructura más sencilla depende de si quieres un único valor por elemento, o múltiples valores. Si quieres un único valor, usa mutate() con map_lgl(), map_int(), map_dbl(), y map_chr() para crear un vector atómico. Si quieres varios valores, usa unnest() para convertir columnas-lista de vuelta a columnas regulares, repitiendo las filas tantas veces como sea necesario. Estas técnicas están descritas con más detalle abajo. 25.5.1 Lista a vector Si puedes reducir tu columna lista a un vector atómico entonces será una columna regular. Por ejemplo, siempre puedes resumir un objeto con su tipo y largo, por lo que este código funcionará sin importar cuál tipo de columna-lista tengas: df &lt;- tribble( ~ x, letters[1:5], 1:3, runif(5) ) df %&gt;% mutate( tipo = map_chr(x, typeof), largo = map_int(x, length) ) #&gt; # A tibble: 3 x 3 #&gt; x tipo largo #&gt; &lt;list&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;chr [5]&gt; character 5 #&gt; 2 &lt;int [3]&gt; integer 3 #&gt; 3 &lt;dbl [5]&gt; double 5 Ésta es la misma información básica que obtienes del método por defecto de imprimir tbl (TODO: esto se traduce?), pero ahora lo puedes usar para filtrar. Es una técnica útil si tienes listas heterogéneas, y quieres remover las partes que no te sirven. No te olvides de los atajos de map_*() - puedes usar map_chr(x, &quot;manzana&quot;) para extraer la cadena de caracteres almacenada en manzana para cada elemento de x. Ésto es útil para separar listas anidadas en columnas regulares. Usa el argumento .null para proveer un valor para usar si el elemento es un valor perdido (missing) (TODO: traducimos missing?) (en lugar de retornar NULL): df &lt;- tribble( ~ x, list(a = 1, b = 2), list(a = 2, c = 4) ) df %&gt;% mutate( a = map_dbl(x, &quot;a&quot;), b = map_dbl(x, &quot;b&quot;, .null = NA_real_) ) #&gt; # A tibble: 2 x 3 #&gt; x a b #&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &lt;list [2]&gt; 1 2 #&gt; 2 &lt;list [2]&gt; 2 NA 25.5.2 Desanidando unnest() trabaja repitiendo la columna regular una vez para cada elemento de la columna-lista. Por ejemplo, en el siguiente ejemplo sencillo repetimos la primera fila 4 veces (porque el primer elemento de y tiene largo cuatro), y la segunda fila una vez: tibble(x = 1:2, y = list(1:4, 1)) %&gt;% unnest(y) #&gt; # A tibble: 5 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 1 2 #&gt; 3 1 3 #&gt; 4 1 4 #&gt; 5 2 1 Esto significa que no puedes simultáneamente desanidar dos columnas que contengan un número diferente de elementos: # Funciona, porque y y z tienen el mismo número de elementos en # cada fila df1 &lt;- tribble( ~ x, ~ y, ~ z, 1, c(&quot;a&quot;, &quot;b&quot;), 1:2, 2, &quot;c&quot;, 3 ) df1 #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [2]&gt; &lt;int [2]&gt; #&gt; 2 2 &lt;chr [1]&gt; &lt;dbl [1]&gt; df1 %&gt;% unnest(y, z) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 1 #&gt; 2 1 b 2 #&gt; 3 2 c 3 # No funciona porque y y z tienen un número diferente de elementos df2 &lt;- tribble( ~ x, ~ y, ~ z, 1, &quot;a&quot;, 1:2, 2, c(&quot;b&quot;, &quot;c&quot;), 3 ) df2 #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; &lt;int [2]&gt; #&gt; 2 2 &lt;chr [2]&gt; &lt;dbl [1]&gt; df2 %&gt;% unnest(y, z) #&gt; Error: All nested columns must have the same number of elements. El mismo principio aplica al desanidar columnas-lista de data frames. Puedes desanidar múltiples columnas-lista siempre que todos los data frames de cada fila tengan la misma cantidad de filas. 25.5.3 Ejercicios ¿Por qué podría ser útil la función lengths() para crear columnas de vectores atómicos a partir de columnas-lista? Lista los tipos de vectores más comúnes que se encuentran en un data frame. ¿Qué hace que las listas sean diferentes? 25.6 Haciendo datos ordenados con broom El paquete broom provee tres herramientas generales para transformar modelos en en data frames ordenados: broom::glance(modelo) retorna una fila para cada modelo. Cada columna tiene una medida de resumen del modelo: o bien una medida de la calidad del modelo, o complejidad, o una combinación de ambos. broom::tidy(modelo) retorna una fila por cada coeficiente en el modelo. Cada columna brinda información acerca de la estimación o su variabilidad. broom::augment(modelo, datos) retorna una fila por cada fila en datos, agregando valores adicionales como residuos, y estadísticos de influencia. Comunicando Introducción Hasta aquí, hemos aprendido a usar las herramientas para importar tus datos en R, ordenarlos de una manera conveniente para el análisis, y luego interpretarlos a través de su transformación, visualización y modelado. Sin embargo, no importa lo bien que esté hecho tu análisis si no puedes explicarlo de manera sencilla a otros: es decir, que es necesario comunicar tus resultados. La comunicación de resultados es el tema de los siguientes cuatro capítulos. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ======= * En [el capítulo: R Markdown], aprenderás sobre dicho paquete, el cual es una herramienta para integrar texto, código y resultados. Puedes usarlo en modo notebook, es decir, en un entorno interactivo de ejecución de código para la comunicación de analista-a-analista, y en modo reporte para la comunicación de analista-a-tomadores-de-decisión. Gracias al potencial de los formatos de R Markdown, incluso puedes usar el mismo documento para ambos propósitos. &gt;&gt;&gt;&gt;&gt;&gt;&gt; upstream/traduccion En [el capítulo: Gráficos para la comunicación], aprenderás cómo convertir tus gráficos exploratorios en gráficos explicativos, los cuales ayudarán a quien ve tu análisis por primera vez a comprender de qué se trata de manera fácil y sencilla. En [el capítulo: Formatos de R Markdown], aprenderás un poco sobre la gran variedad de salidas que puedes generar usando la librería R Markdown, incluyendo dashboards (tableros de control), sitios web, y libros. Terminaremos con [el flujo de trabajo de R Markdown], donde aprenderás sobre “analysis notebook”, en otras palabras, aprenderás sobre el modo notebook para realizar el análisis y registrar sistemáticamente tus logros y fallas para que puedas aprender de ellos. Desafortunadamente, estos capítulos se enfocan principalmente en la parte técnica de la comunicación, y no en los verdaderos grandes problemas de comunicar tus pensamientos a otras personas. Sin embargo, existe una gran cantidad de excelentes libros que abordan esta problemática, cuyas referencias estarán disponibles al final de cada capítulo. "],
["r-markdown.html", "26 R Markdown 26.1 Introduction 26.2 R Markdown basics 26.3 Text formatting with Markdown 26.4 Code chunks 26.5 Troubleshooting 26.6 YAML header 26.7 Learning more", " 26 R Markdown 26.1 Introduction R Markdown provides an unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. R Markdown files are designed to be used in three ways: For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis. For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code). As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking. R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at http://rstudio.com/cheatsheets. 26.1.1 Prerequisites You need the rmarkdown package, but you don’t need to explicitly install it or load it, as RStudio automatically does both when needed. 26.2 R Markdown basics This is an R Markdown file, a plain text file that has the extension .Rmd: --- title: &quot;Diamond sizes&quot; date: 2016-08-25 output: html_document --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) smaller &lt;- diamonds %&gt;% filter(carat &lt;= 2.5) ``` We have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below: ```{r, echo = FALSE} smaller %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.01) ``` It contains three important types of content: An (optional) YAML header surrounded by ---s. Chunks of R code surrounded by ```. Text mixed with simple text formatting like # heading and _italics_. When you open an .Rmd, you get a notebook interface where code and output are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and displays the results inline with the code: To produce a complete report containing all text, code, and results, click “Knit” or press Cmd/Ctrl + Shift + K. You can also do this programmatically with rmarkdown::render(&quot;1-example.Rmd&quot;). This will display the report in the viewer pane, and create a self-contained HTML file that you can share with others. When you knit the document, R Markdown sends the .Rmd file to knitr, http://yihui.name/knitr/, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output. The markdown file generated by knitr is then processed by pandoc, http://pandoc.org/, which is responsible for creating the finished file. The advantage of this two step workflow is that you can create a very wide range of output formats, as you’ll learn about in R markdown formats. To get started with your own .Rmd file, select File &gt; New File &gt; R Markdown… in the menubar. RStudio will launch a wizard that you can use to pre-populate your file with useful content that reminds you how the key features of R Markdown work. The following sections dive into the three components of an R Markdown document in more details: the markdown text, the code chunks, and the YAML header. 26.2.1 Exercises Create a new notebook using File &gt; New File &gt; R Notebook. Read the instructions. Practice running the chunks. Verify that you can modify the code, re-run it, and see modified output. Create a new R Markdown document with File &gt; New File &gt; R Markdown… Knit it by clicking the appropriate button. Knit it by using the appropriate keyboard short cut. Verify that you can modify the input and see the output update. Compare and contrast the R notebook and R markdown files you created above. How are the outputs similar? How are they different? How are the inputs similar? How are they different? What happens if you copy the YAML header from one to the other? Create one new R Markdown document for each of the three built-in formats: HTML, PDF and Word. Knit each of the three documents. How does the output differ? How does the input differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.) 26.3 Text formatting with Markdown Prose in .Rmd files is written in Markdown, a lightweight set of conventions for formatting plain text files. Markdown is designed to be easy to read and easy to write. It is also very easy to learn. The guide below shows how to use Pandoc’s Markdown, a slightly extended version of Markdown that R Markdown understands. Text formatting ------------------------------------------------------------ *italic* or _italic_ **bold** __bold__ `code` superscript^2^ and subscript~2~ Headings ------------------------------------------------------------ # 1st Level Header ## 2nd Level Header ### 3rd Level Header Lists ------------------------------------------------------------ * Bulleted list item 1 * Item 2 * Item 2a * Item 2b 1. Numbered list item 1 1. Item 2. The numbers are incremented automatically in the output. Links and images ------------------------------------------------------------ &lt;http://example.com&gt; [linked phrase](http://example.com) ![optional caption text](path/to/img.png) Tables ------------------------------------------------------------ First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell The best way to learn these is simply to try them out. It will take a few days, but soon they will become second nature, and you won’t need to think about them. If you forget, you can get to a handy reference sheet with Help &gt; Markdown Quick Reference. 26.3.1 Exercises Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. Using the R Markdown quick reference, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. Copy and paste the contents of diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown in to a local R markdown document. Check that you can run it, then add text after the frequency polygon that describes its most striking features. 26.4 Code chunks To run code inside an R Markdown document, you need to insert a chunk. There are three ways to do so: The keyboard shortcut Cmd/Ctrl + Alt + I The “Insert” button icon in the editor toolbar. By manually typing the chunk delimiters ```{r} and ```. Obviously, I’d recommend you learn the keyboard shortcut. It will save you a lot of time in the long run! You can continue to run the code using the keyboard shortcut that by now (I hope!) you know and love: Cmd/Ctrl + Enter. However, chunks get a new keyboard shortcut: Cmd/Ctrl + Shift + Enter, which runs all the code in the chunk. Think of a chunk like a function. A chunk should be relatively self-contained, and focussed around a single task. The following sections describe the chunk header which consists of ```{r, followed by an optional chunk name, followed by comma separated options, followed by }. Next comes your R code and the chunk end is indicated by a final ```. 26.4.1 Chunk name Chunks can be given an optional name: ```{r by-name}. This has three advantages: You can more easily navigate to specific chunks using the drop-down code navigator in the bottom-left of the script editor: Graphics produced by the chunks will have useful names that make them easier to use elsewhere. More on that in [other important options]. You can set up networks of cached chunks to avoid re-performing expensive computations on every run. More on that below. There is one chunk name that imbues special behaviour: setup. When you’re in a notebook mode, the chunk named setup will be run automatically once, before any other code is run. 26.4.2 Chunk options Chunk output can be customised with options, arguments supplied to chunk header. Knitr provides almost 60 options that you can use to customize your code chunks. Here we’ll cover the most important chunk options that you’ll use frequently. You can see the full list at http://yihui.name/knitr/options/. The most important set of options controls if your code block is executed and what results are inserted in the finished report: eval = FALSE prevents code from being evaluated. (And obviously if the code is not run, no results will be generated). This is useful for displaying example code, or for disabling a large block of code without commenting each line. include = FALSE runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report. echo = FALSE prevents code, but not the results from appearing in the finished file. Use this when writing reports aimed at people who don’t want to see the underlying R code. message = FALSE or warning = FALSE prevents messages or warnings from appearing in the finished file. results = 'hide' hides printed output; fig.show = 'hide' hides plots. error = TRUE causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your .Rmd. It’s also useful if you’re teaching R and want to deliberately include an error. The default, error = FALSE causes knitting to fail if there is a single error in the document. The following table summarises which types of output each option supressess: Option Run code Show code Output Plots Messages Warnings eval = FALSE - - - - - include = FALSE - - - - - echo = FALSE - results = &quot;hide&quot; - fig.show = &quot;hide&quot; - message = FALSE - warning = FALSE - 26.4.3 Table By default, R Markdown prints data frames and matrices as you’d see them in the console: mtcars[1:5, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 If you prefer that data be displayed with additional formatting you can use the knitr::kable function. The code below generates Table 26.1. knitr::kable( mtcars[1:5, ], caption = &quot;A knitr kable.&quot; ) Table 26.1: A knitr kable. mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.62 16.5 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.88 17.0 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.21 19.4 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 Read the documentation for ?knitr::kable to see the other ways in which you can customise the table. For even deeper customisation, consider the xtable, stargazer, pander, tables, and ascii packages. Each provides a set of tools for returning formatted tables from R code. There is also a rich set of options for controlling how figures are embedded. You’ll learn about these in [saving your plots]. 26.4.4 Caching Normally, each knit of a document starts from a completely clean slate. This is great for reproducibility, because it ensures that you’ve captured every important computation in code. However, it can be painful if you have some computations that take a long time. The solution is cache = TRUE. When set, this will save the output of the chunk to a specially named file on disk. On subsequent runs, knitr will check to see if the code has changed, and if it hasn’t, it will reuse the cached results. The caching system must be used with care, because by default it is based on the code only, not its dependencies. For example, here the processed_data chunk depends on the raw_data chunk: ```{r raw_data} rawdata &lt;- readr::read_csv(&quot;a_very_large_file.csv&quot;) ``` ```{r processed_data, cache = TRUE} processed_data &lt;- rawdata %&gt;% filter(!is.na(import_var)) %&gt;% mutate(new_variable = complicated_transformation(x, y, z)) ``` Caching the processed_data chunk means that it will get re-run if the dplyr pipeline is changed, but it won’t get rerun if the read_csv() call changes. You can avoid that problem with the dependson chunk option: ```{r processed_data, cache = TRUE, dependson = &quot;raw_data&quot;} processed_data &lt;- rawdata %&gt;% filter(!is.na(import_var)) %&gt;% mutate(new_variable = complicated_transformation(x, y, z)) ``` dependson should contain a character vector of every chunk that the cached chunk depends on. Knitr will update the results for the cached chunk whenever it detects that one of its dependencies have changed. Note that the chunks won’t update if a_very_large_file.csv changes, because knitr caching only tracks changes within the .Rmd file. If you want to also track changes to that file you can use the cache.extra option. This is an arbitrary R expression that will invalidate the cache whenever it changes. A good function to use is file.info(): it returns a bunch of information about the file including when it was last modified. Then you can write: ```{r raw_data, cache.extra = file.info(&quot;a_very_large_file.csv&quot;)} rawdata &lt;- readr::read_csv(&quot;a_very_large_file.csv&quot;) ``` As your caching strategies get progressively more complicated, it’s a good idea to regularly clear out all your caches with knitr::clean_cache(). I’ve used the advice of David Robinson to name these chunks: each chunk is named after the primary object that it creates. This makes it easier to understand the dependson specification. 26.4.5 Global options As you work more with knitr, you will discover that some of the default chunk options don’t fit your needs and you want to change them. You can do this by calling knitr::opts_chunk$set() in a code chunk. For example, when writing books and tutorials I set: knitr::opts_chunk$set( comment = &quot;#&gt;&quot;, collapse = TRUE ) This uses my preferred comment formatting, and ensures that the code and output are kept closely entwined. On the other hand, if you were preparing a report, you might set: knitr::opts_chunk$set( echo = FALSE ) That will hide the code by default, so only showing the chunks you deliberately choose to show (with echo = TRUE). You might consider setting message = FALSE and warning = FALSE, but that would make it harder to debug problems because you wouldn’t see any messages in the final document. 26.4.6 Inline code There is one other way to embed R code into an R Markdown document: directly into the text, with: `r `. This can be very useful if you mention properties of your data in the text. For example, in the example document I used at the start of the chapter I had: We have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below: When the report is knit, the results of these computations are inserted into the text: We have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below: When inserting numbers into text, format() is your friend. It allows you to set the number of digits so you don’t print to a ridiculous degree of accuracy, and a big.mark to make numbers easier to read. I’ll often combine these into a helper function: comma &lt;- function(x) format(x, digits = 2, big.mark = &quot;,&quot;) comma(3452345) #&gt; [1] &quot;3,452,345&quot; comma(.12358124331) #&gt; [1] &quot;0.12&quot; 26.4.7 Exercises Add a section that explores how diamond sizes vary by cut, colour, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo = FALSE on each chunk, set a global option. Download diamond-sizes.Rmd from https://github.com/hadley/r4ds/tree/master/rmarkdown. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes. Modify diamonds-sizes.Rmd to use comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats. Set up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache = TRUE, then verify your understanding of caching. 26.5 Troubleshooting Troubleshooting R Markdown documents can be challenging because you are no longer in an interactive R environment, and you will need to learn some new tricks. The first thing you should always try is to recreate the problem in an interactive session. Restart R, then “Run all chunks” (either from Code menu, under Run region), or with the keyboard shortcut Ctrl + Alt + R. If you’re lucky, that will recreate the problem, and you can figure out what’s going on interactively. If that doesn’t help, there must be something different between your interactive environment and the R markdown environment. You’re going to need to systematically explore the options. The most common difference is the working directory: the working directory of an R Markdown is the directory in which it lives. Check the working directory is what you expect by including getwd() in a chunk. Next, brainstorm all the things that might cause the bug. You’ll need to systematically check that they’re the same in your R session and your R markdown session. The easiest way to do that is to set error = TRUE on the chunk causing the problem, then use print() and str() to check that settings are as you expect. 26.6 YAML header You can control many other “whole document” settings by tweaking the parameters of the YAML header. You might wonder what YAML stands for: it’s “yet another markup language”, which is designed for representing hierarchical data in a way that’s easy for humans to read and write. R Markdown uses it to control many details of the output. Here we’ll discuss two: document parameters and bibliographies. 26.6.1 Parameters R Markdown documents can include one or more parameters whose values can be set when you render the report. Parameters are useful when you want to re-render the same report with distinct values for various key inputs. For example, you might be producing sales reports per branch, exam results by student, or demographic summaries by country. To declare one or more parameters, use the params field. This example uses a my_class parameter to determine which class of cars to display: --- output: html_document params: my_class: &quot;suv&quot; --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) class &lt;- mpg %&gt;% filter(class == params$my_class) ``` # Fuel economy for `r params$my_class`s ```{r, message = FALSE} ggplot(class, aes(displ, hwy)) + geom_point() + geom_smooth(se = FALSE) ``` As you can see, parameters are available within the code chunks as a read-only list named params. You can write atomic vectors directly into the YAML header. You can also run arbitrary R expressions by prefacing the parameter value with !r. This is a good way to specify date/time parameters. params: start: !r lubridate::ymd(&quot;2015-01-01&quot;) snapshot: !r lubridate::ymd_hms(&quot;2015-01-01 12:30:00&quot;) In RStudio, you can click the “Knit with Parameters” option in the Knit dropdown menu to set parameters, render, and preview the report in a single user friendly step. You can customise the dialog by setting other options in the header. See http://rmarkdown.rstudio.com/developer_parameterized_reports.html#parameter_user_interfaces for more details. Alternatively, if you need to produce many such paramterised reports, you can call rmarkdown::render() with a list of params: rmarkdown::render(&quot;fuel-economy.Rmd&quot;, params = list(my_class = &quot;suv&quot;)) This is particularly powerful in conjunction with purrr:pwalk(). The following example creates a report for each value of class found in mpg. First we create a data frame that has one row for each class, giving the filename of the report and the params: reports &lt;- tibble( class = unique(mpg$class), filename = stringr::str_c(&quot;fuel-economy-&quot;, class, &quot;.html&quot;), params = purrr::map(class, ~ list(my_class = .)) ) reports #&gt; # A tibble: 7 x 3 #&gt; class filename params #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 compact fuel-economy-compact.html &lt;list [1]&gt; #&gt; 2 midsize fuel-economy-midsize.html &lt;list [1]&gt; #&gt; 3 suv fuel-economy-suv.html &lt;list [1]&gt; #&gt; 4 2seater fuel-economy-2seater.html &lt;list [1]&gt; #&gt; 5 minivan fuel-economy-minivan.html &lt;list [1]&gt; #&gt; 6 pickup fuel-economy-pickup.html &lt;list [1]&gt; #&gt; # … with 1 more row Then we match the column names to the argument names of render(), and use purrr’s parallel walk to call render() once for each row: reports %&gt;% select(output_file = filename, params) %&gt;% purrr::pwalk(rmarkdown::render, input = &quot;fuel-economy.Rmd&quot;) 26.6.2 Bibliographies and Citations Pandoc can automatically generate citations and a bibliography in a number of styles. To use this feature, specify a bibliography file using the bibliography field in your file’s header. The field should contain a path from the directory that contains your .Rmd file to the file that contains the bibliography file: bibliography: rmarkdown.bib You can use many common bibliography formats including BibLaTeX, BibTeX, endnote, medline. To create a citation within your .Rmd file, use a key composed of ‘@’ + the citation identifier from the bibliography file. Then place the citation in square brackets. Here are some examples: Separate multiple citations with a `;`: Blah blah [@smith04; @doe99]. You can add arbitrary comments inside the square brackets: Blah blah [see @doe99, pp. 33-35; also @smith04, ch. 1]. Remove the square brackets to create an in-text citation: @smith04 says blah, or @smith04 [p. 33] says blah. Add a `-` before the citation to suppress the author&#39;s name: Smith says blah [-@smith04]. When R Markdown renders your file, it will build and append a bibliography to the end of your document. The bibliography will contain each of the cited references from your bibliography file, but it will not contain a section heading. As a result it is common practice to end your file with a section header for the bibliography, such as # References or # Bibliography. You can change the style of your citations and bibliography by referencing a CSL (citation style language) file in the csl field: bibliography: rmarkdown.bib csl: apa.csl As with the bibliography field, your csl file should contain a path to the file. Here I assume that the csl file is in the same directory as the .Rmd file. A good place to find CSL style files for common bibliography styles is http://github.com/citation-style-language/styles. 26.7 Learning more R Markdown is still relatively young, and is still growing rapidly. The best place to stay on top of innovations is the official R Markdown website: http://rmarkdown.rstudio.com. There are two important topics that we haven’t covered here: collaboration, and the details of accurately communicating your ideas to other humans. Collaboration is a vital part of modern data science, and you can make your life much easier by using version control tools, like Git and GitHub. We recommend two free resources that will teach you about Git: “Happy Git with R”: a user friendly introduction to Git and GitHub from R users, by Jenny Bryan. The book is freely available online: http://happygitwithr.com The “Git and GitHub” chapter of R Packages, by Hadley. You can also read it for free online: http://r-pkgs.had.co.nz/git.html. I have also not touched on what you should actually write in order to clearly communicate the results of your analysis. To improve your writing, I highly recommend reading either Style: Lessons in Clarity and Grace by Joseph M. Williams &amp; Joseph Bizup, or The Sense of Structure: Writing from the Reader’s Perspective by George Gopen. Both books will help you understand the structure of sentences and paragraphs, and give you the tools to make your writing more clear. (These books are rather expensive if purchased new, but they’re used by many English classes so there are plenty of cheap second-hand copies). George Gopen also has a number of short articles on writing at https://www.georgegopen.com/the-litigation-articles.html. They are aimed at lawyers, but almost everything applies to data scientists too. "],
["comunicar-con-graficos.html", "27 Comunicar con gráficos 27.1 Introducción 27.2 Etiquetas 27.3 Anotaciones 27.4 Escalas 27.5 Haciendo Zoom 27.6 Temas 27.7 Guardando tus gráficos 27.8 Aprendiendo más", " 27 Comunicar con gráficos 27.1 Introducción En [análisis de datos exploratorios] aprendistes a usar gráficos como herramientas de exploración. Cuando haces gráficos exploratorios, sabes incluso antes de mirar, qué variables mostrará el gráfico. Hicistes cada gráfico con un propósito, lo miraste rápidamente y luego pasaste al siguiente. En el transcurso de la mayoría de los análisis, producirás decenas o cientos de gráficos, muchos de los cuales se desecharán inmediatamente. Ahora que comprendes tus datos, debes comunicar tu conocimiento a los demás. Es probable que tu audiencia no comparta tus conocimientos previos y no esté profundamente involucrada en los datos. Para ayudar a otros a construir rápidamente un buen modelo mental de los datos, deberás invertir un esfuerzo considerable para que tus gráficos se expliquen por sí solos . En este capítulo, aprenderás algunas de las herramientas que proporciona ggplot2 para hacerlo. Este capítulo se centra en las herramientas necesarias para crear buenos gráficos. Supongo que sabes lo que quieres y solo te falta saber cómo hacerlo. Por esa razón, recomiendo combinar este capítulo con un buen libro de visualización general. Me gusta especialmente The Truthful Art, de Albert Cairo. No enseña la mecánica de crear visualizaciones, sino que se enfoca en lo que necesitas pensar para crear gráficos efectivos. 27.1.1 Prerrequisitos En este capítulo, nos centraremos una vez más en ggplot2. También usaremos un poco el paquete dplyr para la manipulación de datos y algunos paquetes como ggrepel y viridis que extienden las funciones de ggplot2. En lugar de cargar esas extensiones aquí, nos referiremos a sus funciones de forma explícita, utilizando la notación :: . Esto ayudará a aclarar qué funciones están integradas en ggplot2 y cuáles vienen de otros paquetes. No olvides que deberás instalar esos paquetes con install.packages() si aún no los tienes. library(tidyverse) library(datos) 27.2 Etiquetas El punto de inicio más sencillo para convertir un gráfico exploratorio en un gráfico expositivo es con buenas etiquetas. Agrega etiquetas con la función labs(). Este ejemplo agrega un título al gráfico: ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + labs(title = &quot;La eficiencia en el uso de combustible disminuye con el tamaño del motor&quot;) El propósito del título de un gráfico es resumir el hallazgo principal. Evita títulos que simplemente describen el gráfico, por ejemplo “Diagrama de dispersión del desplazamiento del motor frente al ahorro de combustible”. Si necesitas agregar más texto, hay otras dos etiquetas útiles que puedes usar en ggplot2 versión 2.2.0 y superiores (que deberían estar disponibles para cuando estés leyendo este libro): el subtítulo, del inglés subtitle, agrega detalles adicionales en una fuente más pequeña debajo del título. la leyenda, del inglés caption, agrega texto en la parte inferior derecha del gráfico, suele usarse para describir la fuente de los datos. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + labs( title = &quot;La eficiencia en el uso de combustible disminuye con el tamaño del motor&quot;, subtitle = &quot;Los automóviles deportivos de dos asientos son la excepción debido a su peso liviano&quot;, caption = &quot;Datos de fueleconomy.gov&quot; ) También puedes usar labs() para reemplazar los títulos de ejes y leyendas. Por lo general, es una buena idea reemplazar los nombres cortos de las variables con descripciones más detalladas e incluir las unidades. ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_smooth(se = FALSE) + labs( x = &quot;Desplazamiento del motor (L)&quot;, y = &quot;Economía de combustible de carretera (millas)&quot;, colour = &quot;Tipo de automóvil&quot; ) Es posible usar ecuaciones matemáticas en lugar de cadenas de texto. Simplemente cambia &quot;&quot; por quote() y lee acerca de las opciones disponibles en ?plotmath: df &lt;- tibble( x = runif(10), y = runif(10) ) ggplot(df, aes(x, y)) + geom_point() + labs( x = quote(sum(x[i]^2, i == 1, n)), y = quote(alpha + beta + frac(delta, theta)) ) 27.2.1 Ejercicios Crea un gráfico partiendo de los datos de economía de combustible con etiquetas para title , subtitle, caption, x, y y color personalizadas. La función geom_smooth() es un poco engañosa porque autopista está sesgada positivamente para motores grandes, debido a la inclusión de autos deportivos livianos con motores grandes. Usa tus herramientas de modelado para ajustar y mostrar un modelo mejor. Elige un gráfico exploratorio que hayas creado en el último mes y agrégale títulos informativos para volverlo más fácil de comprender para otros. 27.3 Anotaciones Además de etiquetar las partes principales de tu gráfico, a menudo es útil etiquetar observaciones individuales o grupos de observaciones. La primera herramienta que tienes a tu disposición es geom_text(). La función geom_text() es similar a geom_point(), pero tiene una estética adicional: label. Esto hace posible agregar etiquetas textuales a tus gráficos. Hay dos posibles fuentes de etiquetas. En primer lugar, es posible tener un tibble que proporcione las etiquetas. El siguiente gráfico no es en sí terriblemente útil, pero si lo es su enfoque: filtrar el auto más eficiente de cada clase con dplyr, y luego etiquetarlo en el gráfico: mejor_de_su_clase &lt;- millas %&gt;% group_by(clase) %&gt;% filter(row_number(desc(autopista)) == 1) ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_text(aes(label = modelo), data = mejor_de_su_clase) Esto es difícil de leer porque las etiquetas se superponen entre sí y con los puntos. Podemos mejorar ligeramente las cosas cambiando por geom_label(), que dibuja un rectángulo detrás del texto. También usamos el parámetro nudge_y para mover las etiquetas ligeramente por encima de los puntos correspondientes: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_label(aes(label = modelo), data = mejor_de_su_clase, nudge_y = 2, alpha = 0.5) Esto ayuda un poco, pero si te fijas bien en la esquina superior izquierda verás que hay dos etiquetas prácticamente una encima de la otra. Esto sucede porque el kilometraje y el desplazamiento para los mejores automóviles en las categorías de compactos y subcompactos son exactamente los mismos. No hay forma de que podamos solucionar esto aplicando la misma transformación para cada etiqueta. En cambio, podemos usar el paquete ggrepel de Kamil Slowikowski. Este paquete es muy útil ya que ajusta automáticamente las etiquetas para que no se superpongan: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_point(size = 3, shape = 1, data = mejor_de_su_clase) + ggrepel::geom_label_repel(aes(label = modelo), data = mejor_de_su_clase) Ten en cuenta otra técnica muy práctica utilizada aquí: agregué una segunda capa de puntos grandes y huecos para resaltar los puntos que etiqueté. A veces puedes usar la misma idea para reemplazar la leyenda con etiquetas colocadas directamente en tu gráfico. Los resultados no son maravillosos para este gráfico en particular, pero tampoco son tan malos. (theme(legend.position = &quot;none&quot;) desactiva la leyenda — hablaremos de ello en breve). clase_promedio &lt;- millas %&gt;% group_by(clase) %&gt;% summarise( motor = median(motor), autopista = median(autopista) ) ggplot(millas, aes(motor, autopista, colour = clase)) + ggrepel::geom_label_repel(aes(label = clase), data = clase_promedio, size = 6, label.size = 0, segment.color = NA ) + geom_point() + theme(legend.position = &quot;none&quot;) Alternativamente puede que quieras agregar una única etiqueta al gráfico, pero de todas formas necesitarás generar un conjunto de datos. Puede ocurrir que desees ubicar la etiqueta en la esquina del gráfico, en ese caso es conveniente crear un nuevo marco de datos usando summarise() para calcular los valores máximos de x e y. etiqueta &lt;- millas %&gt;% summarise( motor = max(motor), autopista = max(autopista), etiqueta = &quot;El aumento del tamaño del motor está \\nrelacionado con la disminución en el gasto de combustible.&quot; ) ggplot(millas, aes(motor, autopista)) + geom_point() + geom_text(aes(label = etiqueta), data = etiqueta, vjust = &quot;top&quot;, hjust = &quot;right&quot;) Si deseas colocar el texto exactamente en los bordes del gráfico puedes usar +Inf y -Inf. Como ya no estamos calculando las posiciones de millas, podemos usar tibble() para crear el conjunto de datos: etiqueta &lt;- millas %&gt;% summarise( motor = Inf, autopista = Inf, etiqueta = &quot;El aumento del tamaño del motor está \\nrelacionado con la disminución en el gasto de combustible.&quot; ) ggplot(millas, aes(motor, autopista)) + geom_point() + geom_text(aes(label = etiqueta), data = etiqueta, vjust = &quot;top&quot;, hjust = &quot;right&quot;) En estos ejemplos, separé manualmente la etiqueta en líneas usando “”. Otra posibilidad es usar stringr::str_wrap() para agregar saltos de línea automáticamente, dado el número de caracteres que deseas por línea: &quot;El aumento del tamaño del motor está relacionado con la disminución en el gasto de combustible.&quot; %&gt;% stringr::str_wrap(width = 40) %&gt;% writeLines() #&gt; El aumento del tamaño del motor está #&gt; relacionado con la disminución en el #&gt; gasto de combustible. Ten en cuenta el uso de hjust y vjust para controlar la alineación de la etiqueta. La figura 27.1 muestra las nueve combinaciones posibles. Figure 27.1: Las nueve combinaciones posibles con hjust y vjust. Recuerda que además de geom_text(), en ggplot2 tienes muchos otros geoms disponibles para ayudar a agregar notas a tu gráfico . Algunas ideas: Emplea geom_hline() y geom_vline() para agregar líneas de referencia. A menudo las hago gruesas (size = 2) y blancas (color = white), y las dibujo debajo de la primera capa de datos. Eso las hace fáciles de ver, sin distraer la atención de los datos. Emplea geom_rect() para dibujar un rectángulo alrededor de los puntos de interés. Los límites del rectángulo están definidos por las estéticas xmin,xmax, ymin,ymax. Emplea geom_segment() con el argumento arrow para destacar un punto en particular con una flecha. Usa la estética x e y para definir la ubicación inicial, y xend y yend para definir la ubicación final. ¡El único límite es tu imaginación! (y tu paciencia para posicionar las anotaciones de forma estéticamente agradable) 27.3.1 Ejercicios Usa las infinitas posiciones que permite geom_text() para colocar texto en cada una de las cuatro esquinas del gráfico. Lee la documentación de la función annotate(). ¿Cómo puedes usarla para agregar una etiqueta de texto a un gráfico sin tener que crear un tibble? ¿Cómo interactúan las etiquetas producidas por geom_text() con la separación en facetas? ¿Cómo puedes agregar una etiqueta a una sola faceta? ¿Cómo puedes poner una etiqueta diferente en cada faceta? (Sugerencia: piensa en los datos subyacentes). ¿Qué argumentos para geom_label() controlan la apariencia de la caja que se ve atrás? ¿Cuáles son los cuatro argumentos de arrow()? ¿Cómo funcionan? Crea una serie de gráficos que demuestren las opciones más importantes. 27.4 Escalas La tercera forma en que puedes mejorar tu gráfico para comunicar es ajustar las escalas. Las escalas controla el mapeo de los valores de los datos a cosas que puedes percibir. Normalmente, ggplot2 agrega escalas automáticamente. Por ejemplo, cuando tipeas: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) ggplot2 agrega automáticamente escalas predeterminadas detrás de escena: ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + scale_x_continuous() + scale_y_continuous() + scale_colour_discrete() Ten en cuenta que los nombres de las escalas comienzan siempre igual: scale_ seguido del nombre de la estética, luego _ y finalmente el nombre de la escala. Las escalas predeterminadas se nombran según el tipo de variable con la que se alinean: continua, discreta, fecha y hora (datetime) o fecha. Hay muchas escalas no predeterminadas que aprenderás a continuación. Las escalas predeterminadas se han elegido cuidadosamente para ser adecuadas para una gama amplia de valores. Sin embargo, es posible que desees sobrescribir los valores predeterminados por dos razones: Es posible que desees modificar algunos de los parámetros de la escala predeterminada. Esto te permite hacer cosas como cambiar los intervalos de valores en los ejes o las etiquetas de cada valor visible. Es posible que desees reemplazar la escala por completo, y utilizar un algoritmo completamente diferente. Por lo general tu opción será mejor que la predeterminada ya que sabes más acerca de los datos. 27.4.1 Marcas de los ejes y leyendas Hay dos argumentos principales que afectan la apariencia de las marcas, del inglés ticks, en los ejes y las leyendas:breaks y labels, del inglés quiebre y etiqueta respectivamente. Los breaks controlan la posición de las marcas en los ejes o los valores asociados con las leyendas. Las labels controlan la etiqueta de texto asociada con cada marca/leyenda. El uso más común de los breaks es redefinir la opción predeterminada: ggplot(millas, aes(motor, autopista)) + geom_point() + scale_y_continuous(breaks = seq(15, 40, by = 5)) Puedes usar labels de la misma manera (un vector de caracteres de la misma longitud que breaks), o puedes establecerlas como NULL del inglés nulo, para suprimir las etiquetas por completo. Esto es útil para mapas, o para publicar gráficos donde no puedes compartir los números absolutos. ggplot(millas, aes(motor, autopista)) + geom_point() + scale_x_continuous(labels = NULL) + scale_y_continuous(labels = NULL) También puedes usar breaks y labels para controlar la apariencia de las leyendas. En conjunto, los ejes y las leyendas se llaman guías. Los ejes se usan para la estética de x e y; las leyendas se usan para todo lo demás. Otro uso de los breaks es cuando tienes relativamente pocos puntos de datos y deseas resaltar exactamente dónde se producen las observaciones. Como ejemplo, el siguiente gráfico muestra cuándo comenzó y terminó su mandato cada presidente de los Estados Unidos. presidencial %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(inicio, id)) + geom_point() + geom_segment(aes(xend = fin, yend = id)) + scale_x_date(NULL, breaks = presidencial$inicio, date_labels = &quot;&#39;%y&quot;) Ten en cuenta que la especificación de breaks y labels para escalas en formato de fecha y fecha y hora es ligeramente diferente: date_labels toma en cuenta la especificación de formato, en la misma forma que parse_datetime(). date_breaks (no se muestra aquí), toma una cadena como “2 días” o “1 mes”. 27.4.2 Diseño de leyendas Con mayor frecuencia utilizarás breaks y labels para ajustar los ejes. Aunque ambos también funcionan con leyendas, hay algunas otras técnicas que podrías usar. Para controlar la posición general de la leyenda, debes usar una configuración de theme() del inglés tema. Volveremos a los temas al final del capítulo, pero en resumen, controlan las partes del gráfico que no son de datos. La configuración del tema legend.position del inglés posición de la leyenda, controla dónde se dibuja la leyenda: base &lt;- ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) base + theme(legend.position = &quot;left&quot;) base + theme(legend.position = &quot;top&quot;) base + theme(legend.position = &quot;bottom&quot;) base + theme(legend.position = &quot;right&quot;) # the default También puedes usar legend.position = &quot;none&quot; para suprimir por completo la visualización de la leyenda. Para controlar la visualización de leyendas individuales, usa guides() junto con guide_legend() o guide_colourbar(). El siguiente ejemplo muestra dos configuraciones importantes: controlar el número de filas que usa la leyenda con nrow, y redefinir una de las estéticas para agrandar los puntos. Esto es particularmente útil si has usado un valor de alfa bajo para mostrar muchos puntos en un diagrama. ggplot(millas, aes(motor, autopista)) + geom_point(aes(colour = clase)) + geom_smooth(se = FALSE) + theme(legend.position = &quot;bottom&quot;) + guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4))) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 27.4.3 Reemplazando una escala En lugar de simplemente modificar un poco los detalles, puedes reemplazar la escala por completo. Hay dos tipos de escalas que es probable que desees cambiar: escalas de posición continua y escalas de color. Afortunadamente, los mismos principios se aplican a todos los demás aspectos estéticos, por lo que una vez que hayas dominado la posición y el color serás capaz de realizar rápidamente otros reemplazos de escala. Esto es muy útil para graficar transformaciones de tu variable. A modo de ejemplo, como hemos visto en diamond prices, es más fácil ver la relación precisa entre quilate y precio si aplicamos una transformación logarítmica en base 10: ggplot(diamantes, aes(quilate, precio)) + geom_bin2d() ggplot(diamantes, aes(log10(quilate), log10(precio))) + geom_bin2d() Sin embargo, la desventaja de esta transformación es que los ejes ahora están etiquetados con los valores transformados, por lo que se vuelve difícil interpretar el gráfico. En lugar de hacer la transformación en el mapeo estético, podemos hacerlo con la escala. Esto es visualmente idéntico, excepto que los ejes están etiquetados en la escala original de los datos. ggplot(diamantes, aes(quilate, precio)) + geom_bin2d() + scale_x_log10() + scale_y_log10() Otra escala que se personaliza con frecuencia es el color. La escala categórica predeterminada selecciona los colores que están espaciados uniformemente alrededor del círculo cromático. Otras alternativas útiles son las escalas de ColorBrewer que han sido ajustadas manualmente para que funcionen mejor para personas con tipos comunes de daltonismo. Los dos gráficos de abajo se ven similares, sin embargo hay suficiente diferencia en los tonos rojo y verde tal que los puntos de la derecha pueden distinguirse incluso por personas con daltonismo rojo-verde. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion)) ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion)) + scale_colour_brewer(palette = &quot;Set1&quot;) No olvides las técnicas más simples. Si solo hay unos pocos colores, puedes agregar un mapeo de forma redundante. Esto también ayudará a asegurar que tu gráfico sea interpretable en blanco y negro. ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = traccion, shape = traccion)) + scale_colour_brewer(palette = &quot;Set1&quot;) Las escalas de ColorBrewer están documentadas en línea en http://colorbrewer2.org/ y están disponibles en R en el paquete RColorBrewer de Erich Neuwirth. La figura 27.2 muestra la lista completa de paletas de colores disponibles. Las paletas secuenciales (arriba) y divergentes (abajo) son particularmente útiles si sus valores categóricos están ordenados o tienen un “centro”. Esto a menudo ocurre si has utilizado cut() para convertir una variable continua en una variable categórica. Figure 27.2: All ColourBrewer scales. Cuando tengas un mapeo predefinido entre valores y colores, usa scale_colour_manual(). Por ejemplo, si mapeamos los partidos presidenciales de Estados Unidos en color, queremos usar el mapeo estándar de color rojo para republicanos y azul para demócratas: presidencial %&gt;% mutate(id = 33 + row_number()) %&gt;% ggplot(aes(inicio, id, colour = partido)) + geom_point() + geom_segment(aes(xend = fin, yend = id)) + scale_colour_manual(values = c(Republicano = &quot;red&quot;, Demócrata = &quot;blue&quot;)) Para generar una escala de color para variables continuas , puedes usar built in scale_colour_gradient() o scale_fill_gradient(). Si tienes una escala divergente, puedes usar scale_colour_gradient2(). Eso tepermite dar, por ejemplo, diferentes colores a valores positivos y negativos. Esto a veces también es útil si quieres distinguir puntos por encima o por debajo de la media. Otra opción es scale_colour_viridis() proporcionada por el paquete viridis. Es un análogo continuo de las escalas categóricas de ColorBrewer. Los diseñadores, Nathaniel Smith y Stéfan van der Walt, adaptaron cuidadosamente una paleta de color para variables continuas que tiene buenas propiedades perceptuales. Aquí hay un ejemplo de viridis: df &lt;- tibble( x = rnorm(10000), y = rnorm(10000) ) ggplot(df, aes(x, y)) + geom_hex() + coord_fixed() ggplot(df, aes(x, y)) + geom_hex() + viridis::scale_fill_viridis() + coord_fixed() Ten en cuenta que todas las escalas de color vienen en dos variedades: scale_colour_x() y scale_fill_x() para la estética color y fill, respectivamente (las escalas de color se expresan tanto en inglés americano como británico). 27.4.4 Ejercicios ¿Por qué el siguiente código no reemplaza la escala predeterminada? ggplot(df, aes(x, y)) + geom_hex() + scale_colour_gradient(low = &quot;white&quot;, high = &quot;red&quot;) + coord_fixed() ¿Cuál es el primer argumento para cada escala? ¿Cómo se compara con labs()? Cambia la visualización de los términos presidenciales de las siguientes maneras: Combinando las dos variantes que se muestran arriba. Mejorando la visualización del eje y. Etiquetando cada término con el nombre del presidente. Agregandoetiquetas informativas al gráfico. Poniendo intervalos de 4 años (¡esto es más complicado de lo que parece!). Utiliza override.aes para que la leyenda en el siguiente gráfico sea más fácil de ver: ggplot(diamantes, aes(quilate, precio)) + geom_point(aes(colour = corte, alpha = 1 / 20)) 27.5 Haciendo Zoom Hay tres formas de controlar los límites de un gráfico: Modificando los datos que se grafican Estableciendo los límites en cada escala Estableciendo xlim y ylim en coord_cartesian() Para ampliar una región del gráfico, generalmente es mejor usar coord_cartesian().Compara los siguientes dos gráficos: ggplot(millas, mapping = aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth() + coord_cartesian(xlim = c(5, 7), ylim = c(10, 30)) millas %&gt;% filter(motor &gt;= 5, motor &lt;= 7, autopista &gt;= 10, autopista &lt;= 30) %&gt;% ggplot(aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth() También puedes establecer limits del inglés límites en escalas individuales. La reducción de los límites del gráfico es equivalente a seleccionar un subconjunto de los datos. En general es más útil si deseas expandir los límites, por ejemplo, cuando quieres hacer coincidir escalas de diferentes gráficos. A modo de ejemplo, si extraemos dos clases de automóviles y los graficamos por separado, son difíciles de comparar ya que las tres escalas (el eje x, el eje y y la estética del color) tienen rangos diferentes. suv &lt;- millas %&gt;% filter(clase == &quot;suv&quot;) compacto &lt;- millas %&gt;% filter(clase == &quot;compact&quot;) ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() ggplot(compacto, aes(motor, autopista, colour = traccion)) + geom_point() Una forma de superar este problema es compartir la escala entre varios gráficos, estableciendo una escala única a partir de los límites del conjunto de datos completo. x_scale &lt;- scale_x_continuous(limits = range(millas$motor)) y_scale &lt;- scale_y_continuous(limits = range(millas$autopista)) col_scale &lt;- scale_colour_discrete(limits = unique(millas$traccion)) ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() + x_scale + y_scale + col_scale ggplot(suv, aes(motor, autopista, colour = traccion)) + geom_point() + x_scale + y_scale + col_scale En este caso particular podrías haber simplemente empleado la separación en facetas, pero esta técnica es más útil en general, por ejemplo, si deseas realizar gráficos en varias páginas de un informe. 27.6 Temas Finalmente, puedes personalizar los elementos de tu gráfico que no son datos aplicando un tema: ggplot(millas, aes(motor, autopista)) + geom_point(aes(color = clase)) + geom_smooth(se = FALSE) + theme_bw() ggplot2 incluye ocho temas por defecto, como se muestra en la Figura 27.3. Muchos otros están incluidos en paquetes adicionales como ggthemes (https://github.com/jrnold/ggthemes), de Jeffrey Arnold. Figure 27.3: The eight themes built-in to ggplot2. Muchas personas se preguntan por qué el tema predeterminado tiene un fondo gris. Esta fue una elección deliberada ya que el fondo gris pone los datos por delante mientras siguen siendo visibles las líneas de la cuadrícula. Las líneas blancas de la cuadrícula son visibles (lo cual es importante porque ayudan significativamente a evaluar la posición), pero tienen poco impacto visual y son fáciles de eliminar. El fondo gris le da a al gráfico un color tipográfico similar al del texto, asegurando que los gráficos encajen con el flujo de un documento sin saltar con un fondo blanco brillante. Finalmente, el fondo gris crea un campo continuo de color que asegura que el gráfico se perciba como una sola entidad visual. También es posible controlar componentes individuales de cada tema, como el tamaño y el color de la fuente utilizada para el eje y. Desafortunadamente, este nivel de detalle está fuera del alcance de este libro, por lo que deberás leer el libro ggplot2 book para obtener todos los detalles. También puedes crear tus propios temas, si estás tratando de hacer coincidir un estilo corporativo o de revista en particular. 27.7 Guardando tus gráficos Hay dos formas principales de obtener tus gráficos desde R: ggsave() y knitr. ggsave() guardarán el gráfico más reciente en el disco. ggplot(millas, aes(motor, autopista)) + geom_point() ggsave(&quot;my-plot.pdf&quot;) #&gt; Saving 6 x 3.7 in image Si no especificas width y height, del inglés el ancho y el alto, se usarán las dimensiones del dispositivo empleado para graficar. Para que el código sea reproducible, necesitarás especificarlos. En general, sin embargo, creo que deberías armar tus informes finales utilizando R Markdown, por lo que quiero centrarme en las opciones importantes para los bloques de código que debes conocer para graficar. Puedes obtener más información sobre ggsave() en la documentación. 27.7.1 Redimensionar una figura El mayor desafío de los gráficos en R Markdown es conseguir que tus figuras tengan el tamaño y la forma correctos. Hay cinco opciones principales que controlan el tamaño de la figura: fig.width, fig.height, fig.asp, out.width y out.height. El tamaño de la imagen es un desafío porque hay dos tamaños (el tamaño de la figura creada por R y el tamaño al que se inserta en el documento de salida) y varias formas de especificarlo (es decir, altura, ancho y relación de aspecto: elige dos de tres). Solo uso tres de las cinco opciones: Encuentro estéticamente más agradable que los gráficos tengan un ancho consistente. Para hacer cumplir esto, configuro fig.width = 6 (6 &quot;) y fig.asp = 0.618 (la proporción áurea) en los valores predeterminados. Luego, en bloques individuales, solo ajusto fig.asp. Controlo el tamaño de salida con out.width y lo configuro a un porcentaje del ancho de línea). De manera predeterminada, out.width = &quot;70%&quot; y fig.align = &quot;center&quot;. Eso le da a los gráficos cierto espacio para respirar, sin ocupar demasiado espacio. Para poner múltiples gráficos en una sola fila, establezco out.width en 50% para dos gráficos, 33% en 3 gráficos, o 25% en 4 gráficos, y setfig.align = &quot;default&quot;. Dependiendo de lo que intento ilustrar (por ejemplo, mostrar datos o variacionesdel gráfico), también modificaré fig.width cómo se explica a continuación. Si observas que tienes que entrecerrar los ojos para leer el texto de tu gráfico, debes ajustar fig.width. Si fig.width es mayor que el tamaño de la figura en el documento final, el texto será demasiado pequeño; si fig.width es más pequeño, el texto será demasiado grande. A menudo necesitarás experimentar un poco para calcular la proporción correcta entre fig.width y el ancho asociado en tu documento. Para ilustrar el principio, los siguientes tres gráficos tienen fig.width de 4, 6 y 8, respectivamente: Si deseas asegurarte que el tamaño de fuente es el mismo en todas tus figuras, al establecer out.width, también necesitarás ajustar fig.width para mantener la misma proporción en relación al out.width predeterminado. Por ejemplo, si tu valor predeterminado de fig.width es 6 y out.width es 0.7, cuando establezcas out.width = &quot;50%&quot; necesitarás establecer fig.width a 4.3 (6 * 0.5 / 0.7). 27.7.2 Otras opciones importantes Al mezclar código y texto, como hago en este libro, recomiendo configurar fig.show = &quot;hold&quot; para que los gráficos se muestren después del código. Esto tiene el agradable efecto secundario de obligarte a dividir grandes bloques de código con sus explicaciones. Para agregar un título al gráfico, usa fig.cap. En R Markdown esto cambiará la figura de “inline” a “floating”. Si estás produciendo resultados en formato PDF, el tipo de gráficos predeterminado es PDF. Esta es una buena configuración predeterminada porque los PDF son gráficos vectoriales de alta calidad. Sin embargo, pueden generar gráficos muy grandes y lentos si muestras miles de puntos. En ese caso, configura dev = &quot;png&quot; para forzar el uso de PNG. Son de calidad ligeramente inferior, pero serán mucho más compactos. Es una buena idea darles nombres a los bloques de código que producen figuras, incluso si no etiquetas rutinariamente otros bloques. Etiquetar el bloque se utiliza para generar el nombre de archivo del gráfico en el disco, por lo que darle un nombre a los bloques hace que sea mucho más fácil seleccionar gráficas y reutilizarlas en otras circunstancias (por ejemplo, si deseas colocar rápidamente un solo gráfico en un correo electrónico o un tweet). 27.8 Aprendiendo más El mejor lugar para aprender más es el libro de ggplot2: ggplot2: Elegant graphics for data analysis. Este explica con mucha más profundidad la teoría subyacente y tiene muchos más ejemplos de cómo combinar las piezas individuales para resolver problemas prácticos. Desafortunadamente, el libro no está disponible en línea de forma gratuita, aunque puede encontrar el código fuente en https://github.com/hadley/ggplot2-book. Otro gran recurso es la guía de extensiones ggplot2 http://www.ggplot2-exts.org/. Este sitio enumera muchos de los paquetes que amplían ggplot2 con nuevos geoms y escalas. Es un buen lugar para comenzar si tratas de hacer algo que parece difícil con ggplot2. "],
["r-markdown-formats.html", "28 R Markdown formats 28.1 Introduction 28.2 Output options 28.3 Documents 28.4 Notebooks 28.5 Presentations 28.6 Dashboards 28.7 Interactivity 28.8 Websites 28.9 Other formats 28.10 Learning more", " 28 R Markdown formats 28.1 Introduction So far you’ve seen R Markdown used to produce HTML documents. This chapter gives a brief overview of some of the many other types of output you can produce with R Markdown. There are two ways to set the output of a document: Permanently, by modifying the YAML header: title: &quot;Viridis Demo&quot; output: html_document Transiently, by calling rmarkdown::render() by hand: rmarkdown::render(&quot;diamond-sizes.Rmd&quot;, output_format = &quot;word_document&quot;) This is useful if you want to programmatically produce multiple types of output. RStudio’s knit button renders a file to the first format listed in its output field. You can render to additional formats by clicking the dropdown menu beside the knit button. 28.2 Output options Each output format is associated with an R function. You can either write foo or pkg::foo. If you omit pkg, the default is assumed to be rmarkdown. It’s important to know the name of the function that makes the output because that’s where you get help. For example, to figure out what parameters you can set with html_document, look at ?rmarkdown::html_document. To override the default parameter values, you need to use an expanded output field. For example, if you wanted to render an html_document with a floating table of contents, you’d use: output: html_document: toc: true toc_float: true You can even render to multiple outputs by supplying a list of formats: output: html_document: toc: true toc_float: true pdf_document: default Note the special syntax if you don’t want to override any of the default options. 28.3 Documents The previous chapter focused on the default html_document output. There are a number of basic variations on that theme, generating different types of documents: pdf_document makes a PDF with LaTeX (an open source document layout system), which you’ll need to install. RStudio will prompt you if you don’t already have it. word_document for Microsoft Word documents (.docx). odt_document for OpenDocument Text documents (.odt). rtf_document for Rich Text Format (.rtf) documents. md_document for a Markdown document. This isn’t typically useful by itself, but you might use it if, for example, your corporate CMS or lab wiki uses markdown. github_document: this is a tailored version of md_document designed for sharing on GitHub. Remember, when generating a document to share with decision makers, you can turn off the default display of code by setting global options in the setup chunk: knitr::opts_chunk$set(echo = FALSE) For html_documents another option is to make the code chunks hidden by default, but visible with a click: output: html_document: code_folding: hide 28.4 Notebooks A notebook, html_notebook, is a variation on a html_document. The rendered outputs are very similar, but the purpose is different. A html_document is focused on communicating with decision makers, while a notebook is focused on collaborating with other data scientists. These different purposes lead to using the HTML output in different ways. Both HTML outputs will contain the fully rendered output, but the notebook also contains the full source code. That means you can use the .nb.html generated by the notebook in two ways: You can view it in a web browser, and see the rendered output. Unlike html_document, this rendering always includes an embedded copy of the source code that generated it. You can edit it in RStudio. When you open an .nb.html file, RStudio will automatically recreate the .Rmd file that generated it. In the future, you will also be able to include supporting files (e.g. .csv data files), which will be automatically extracted when needed. Emailing .nb.html files is a simple way to share analyses with your colleagues. But things will get painful as soon as they want to make changes. If this starts to happen, it’s a good time to learn Git and GitHub. Learning Git and GitHub is definitely painful at first, but the collaboration payoff is huge. As mentioned earlier, Git and GitHub are outside the scope of the book, but there’s one tip that’s useful if you’re already using them: use both html_notebook and github_document outputs: output: html_notebook: default github_document: default html_notebook gives you a local preview, and a file that you can share via email. github_document creates a minimal md file that you can check into git. You can easily see how the results of your analysis (not just the code) change over time, and GitHub will render it for you nicely online. 28.5 Presentations You can also use R Markdown to produce presentations. You get less visual control than with a tool like Keynote or PowerPoint, but automatically inserting the results of your R code into a presentation can save a huge amount of time. Presentations work by dividing your content into slides, with a new slide beginning at each first (#) or second (##) level header. You can also insert a horizontal rule (***) to create a new slide without a header. R Markdown comes with three presentation formats built-in: ioslides_presentation - HTML presentation with ioslides slidy_presentation - HTML presentation with W3C Slidy beamer_presentation - PDF presentation with LaTeX Beamer. Two other popular formats are provided by packages: revealjs::revealjs_presentation - HTML presentation with reveal.js. Requires the revealjs package. rmdshower, https://github.com/MangoTheCat/rmdshower, provides a wrapper around the shower, https://github.com/shower/shower, presentation engine 28.6 Dashboards Dashboards are a useful way to communicate large amounts of information visually and quickly. Flexdashboard makes it particularly easy to create dashboards using R Markdown and a convention for how the headers affect the layout: Each level 1 header (#) begins a new page in the dashboard. Each level 2 header (##) begins a new column. Each level 3 header (###) begins a new row. For example, you can produce this dashboard: Using this code: --- title: &quot;Diamonds distribution dashboard&quot; output: flexdashboard::flex_dashboard --- ```{r setup, include = FALSE} library(ggplot2) library(dplyr) knitr::opts_chunk$set(fig.width = 5, fig.asp = 1 / 3) ``` ## Column 1 ### Carat ```{r} ggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.1) ``` ### Cut ```{r} ggplot(diamonds, aes(cut)) + geom_bar() ``` ### Colour ```{r} ggplot(diamonds, aes(color)) + geom_bar() ``` ## Column 2 ### The largest diamonds ```{r} diamonds %&gt;% arrange(desc(carat)) %&gt;% head(100) %&gt;% select(carat, cut, color, price) %&gt;% DT::datatable() ``` Flexdashboard also provides simple tools for creating sidebars, tabsets, value boxes, and gauges. To learn more about flexdashboard visit http://rmarkdown.rstudio.com/flexdashboard/. 28.7 Interactivity Any HTML format (document, notebook, presentation, or dashboard) can contain interactive components. 28.7.1 htmlwidgets HTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualisations. For example, take the leaflet map below. If you’re viewing this page on the web, you can drag the map around, zoom in and out, etc. You obviously can’t do that in a book, so rmarkdown automatically inserts a static screenshot for you. library(leaflet) leaflet() %&gt;% setView(174.764, -36.877, zoom = 16) %&gt;% addTiles() %&gt;% addMarkers(174.764, -36.877, popup = &quot;Maungawhau&quot;) The great thing about htmlwidgets is that you don’t need to know anything about HTML or JavaScript to use them. All the details are wrapped inside the package, so you don’t need to worry about it. There are many packages that provide htmlwidgets, including: dygraphs, http://rstudio.github.io/dygraphs/, for interactive time series visualisations. DT, http://rstudio.github.io/DT/, for interactive tables. threejs, https://github.com/bwlewis/rthreejs for interactive 3d plots. DiagrammeR, http://rich-iannone.github.io/DiagrammeR/ for diagrams (like flow charts and simple node-link diagrams). To learn more about htmlwidgets and see a more complete list of packages that provide them visit http://www.htmlwidgets.org/. 28.7.2 Shiny htmlwidgets provide client-side interactivity — all the interactivity happens in the browser, independently of R. On one hand, that’s great because you can distribute the HTML file without any connection to R. However, that fundamentally limits what you can do to things that have been implemented in HTML and JavaScript. An alternative approach is to use shiny, a package that allows you to create interactivity using R code, not JavaScript. To call Shiny code from an R Markdown document, add runtime: shiny to the header: title: &quot;Shiny Web App&quot; output: html_document runtime: shiny Then you can use the “input” functions to add interactive components to the document: library(shiny) textInput(&quot;name&quot;, &quot;What is your name?&quot;) numericInput(&quot;age&quot;, &quot;How old are you?&quot;, NA, min = 0, max = 150) You can then refer to the values with input$name and input$age, and the code that uses them will be automatically re-run whenever they change. I can’t show you a live shiny app here because shiny interactions occur on the server-side. This means that you can write interactive apps without knowing JavaScript, but you need a server to run them on. This introduces a logistical issue: Shiny apps need a Shiny server to be run online. When you run shiny apps on your own computer, shiny automatically sets up a shiny server for you, but you need a public facing shiny server if you want to publish this sort of interactivity online. That’s the fundamental trade-off of shiny: you can do anything in a shiny document that you can do in R, but it requires someone to be running R. Learn more about Shiny at http://shiny.rstudio.com/. 28.8 Websites With a little additional infrastructure you can use R Markdown to generate a complete website: Put your .Rmd files in a single directory. index.Rmd will become the home page. Add a YAML file named _site.yml provides the navigation for the site. For example: name: &quot;my-website&quot; navbar: title: &quot;My Website&quot; left: - text: &quot;Home&quot; href: index.html - text: &quot;Viridis Colors&quot; href: 1-example.html - text: &quot;Terrain Colors&quot; href: 3-inline.html Execute rmarkdown::render_site() to build _site, a directory of files ready to deploy as a standalone static website, or if you use an RStudio Project for your website directory. RStudio will add a Build tab to the IDE that you can use to build and preview your site. Read more at http://rmarkdown.rstudio.com/rmarkdown_websites.html. 28.9 Other formats Other packages provide even more output formats: The bookdown package, https://github.com/rstudio/bookdown, makes it easy to write books, like this one. To learn more, read Authoring Books with R Markdown, by Yihui Xie, which is, of course, written in bookdown. Visit http://www.bookdown.org to see other bookdown books written by the wider R community. The prettydoc package, https://github.com/yixuan/prettydoc/, provides lightweight document formats with a range of attractive themes. The rticles package, https://github.com/rstudio/rticles, compiles a selection of formats tailored for specific scientific journals. See http://rmarkdown.rstudio.com/formats.html for a list of even more formats. You can also create your own by following the instructions at http://rmarkdown.rstudio.com/developer_custom_formats.html. 28.10 Learning more To learn more about effective communication in these different formats I recommend the following resources: To improve your presentation skills, I recommend Presentation Patterns, by Neal Ford, Matthew McCollough, and Nathaniel Schutta. It provides a set of effective patterns (both low- and high-level) that you can apply to improve your presentations. If you give academic talks, I recommend reading the Leek group guide to giving talks. I haven’t taken it myself, but I’ve heard good things about Matt McGarrity’s online course on public speaking: https://www.coursera.org/learn/public-speaking. If you are creating a lot of dashboards, make sure to read Stephen Few’s Information Dashboard Design: The Effective Visual Communication of Data. It will help you create dashboards that are truly useful, not just pretty to look at. Effectively communicating your ideas often benefits from some knowledge of graphic design. The Non-Designer’s Design Book is a great place to start. "],
["r-markdown-workflow.html", "29 R Markdown workflow", " 29 R Markdown workflow Earlier, we discussed a basic workflow for capturing your R code where you work interactively in the console, then capture what works in the script editor. R Markdown brings together the console and the script editor, blurring the lines between interactive exploration and long-term code capture. You can rapidly iterate within a chunk, editing and re-executing with Cmd/Ctrl + Shift + Enter. When you’re happy, you move on and start a new chunk. R Markdown is also important because it so tightly integrates prose and code. This makes it a great analysis notebook because it lets you develop code and record your thoughts. An analysis notebook shares many of the same goals as a classic lab notebook in the physical sciences. It: Records what you did and why you did it. Regardless of how great your memory is, if you don’t record what you do, there will come a time when you have forgotten important details. Write them down so you don’t forget! Supports rigorous thinking. You are more likely to come up with a strong analysis if you record your thoughts as you go, and continue to reflect on them. This also saves you time when you eventually write up your analysis to share with others. Helps others understand your work. It is rare to do data analysis by yourself, and you’ll often be working as part of a team. A lab notebook helps you share not only what you’ve done, but why you did it with your colleagues or lab mates. Much of the good advice about using lab notebooks effectively can also be translated to analysis notebooks. I’ve drawn on my own experiences and Colin Purrington’s advice on lab notebooks (http://colinpurrington.com/tips/lab-notebooks) to come up with the following tips: Ensure each notebook has a descriptive title, an evocative filename, and a first paragraph that briefly describes the aims of the analysis. Use the YAML header date field to record the date you started working on the notebook: date: 2016-08-23 Use ISO8601 YYYY-MM-DD format so that’s there no ambiguity. Use it even if you don’t normally write dates that way! If you spend a lot of time on an analysis idea and it turns out to be a dead end, don’t delete it! Write up a brief note about why it failed and leave it in the notebook. That will help you avoid going down the same dead end when you come back to the analysis in the future. Generally, you’re better off doing data entry outside of R. But if you do need to record a small snippet of data, clearly lay it out using tibble::tribble(). If you discover an error in a data file, never modify it directly, but instead write code to correct the value. Explain why you made the fix. Before you finish for the day, make sure you can knit the notebook (if you’re using caching, make sure to clear the caches). That will let you fix any problems while the code is still fresh in your mind. If you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use packrat, http://rstudio.github.io/packrat/, which stores packages in your project directory, or checkpoint, https://github.com/RevolutionAnalytics/checkpoint, which will reinstall packages available on a specified date. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t let you easily recreate your packages as they are today, but at least you’ll know what they were. You are going to create many, many, many analysis notebooks over the course of your career. How are you going to organise them so you can find them again in the future? I recommend storing them in individual projects, and coming up with a good naming scheme. "]
]
